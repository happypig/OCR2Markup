{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b78b3a",
   "metadata": {},
   "source": [
    "# prompt optimization\n",
    "\n",
    "How to use Gemini 2.5 pro as the prompt optimization agent to finetune the prompt to be used in Gemma3:1b and Gemma3:4b to turn citation string into xml, also use Gemma3:1b and Gemma3:4b from local Ollama service for verification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bcf27",
   "metadata": {},
   "source": [
    "I'll help you create a prompt optimization system using Gemini 2.5 Pro to fine-tune prompts for your local Gemma models to convert citation strings to XML. Let's create a comprehensive notebook that leverages both the cloud Gemini API and your local Ollama service.\n",
    "\n",
    "## Step-by-step Solution:\n",
    "\n",
    "1. **Set up Gemini 2.5 Pro API connection** for prompt optimization\n",
    "2. **Create prompt optimization agent** using Gemini 2.5 Pro\n",
    "3. **Implement local Ollama verification** using Gemma3:1b and Gemma3:4b\n",
    "4. **Build iterative optimization loop** to improve prompt performance\n",
    "5. **Add evaluation metrics** for citation XML accuracy\n",
    "\n",
    "Create a comprehensive prompt optimization system using Gemini 2.5 Pro for fine-tuning and local Gemma models for verification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feeb6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import statistics\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['google-generativeai', 'requests', 'pandas', 'matplotlib', 'seaborn']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "GEMINI_API_KEY = \"your-gemini-api-key-here\"  # Replace with your actual API key\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_API_KEY = \"sk-your-api-key-here\"  # From your local setup\n",
    "\n",
    "# Load reference data\n",
    "with open('ref_markup_optV2.json', 'r', encoding='utf-8') as f:\n",
    "    reference_data = json.load(f)[0]\n",
    "\n",
    "class GeminiOptimizer:\n",
    "    \"\"\"Gemini 2.5 Pro prompt optimization agent\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "        # self.model = genai.GenerativeModel('gemini-2.0-flash-thinking-exp-1219')\n",
    "        \n",
    "    def analyze_prompt_performance(self, prompt: str, test_results: List[Dict]) -> str:\n",
    "        \"\"\"Analyze prompt performance and suggest improvements\"\"\"\n",
    "        \n",
    "        # Calculate success rate\n",
    "        total_tests = len(test_results)\n",
    "        successful_tests = sum(1 for result in test_results if result['success'])\n",
    "        success_rate = successful_tests / total_tests if total_tests > 0 else 0\n",
    "        \n",
    "        # Collect error patterns\n",
    "        errors = [result['error'] for result in test_results if not result['success']]\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "You are an expert prompt engineer specializing in XML markup tasks for Buddhist text citations. \n",
    "\n",
    "CURRENT PROMPT PERFORMANCE:\n",
    "- Success Rate: {success_rate:.2%} ({successful_tests}/{total_tests})\n",
    "- Failed Cases: {len(errors)}\n",
    "\n",
    "CURRENT PROMPT:\n",
    "{prompt}\n",
    "\n",
    "FAILED TEST CASES AND ERRORS:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, result in enumerate(test_results):\n",
    "            if not result['success']:\n",
    "                analysis_prompt += f\"\"\"\n",
    "Case {i+1}:\n",
    "Input: {result['input']}\n",
    "Expected: {result['expected']}\n",
    "Actual: {result['actual']}\n",
    "Error: {result['error']}\n",
    "---\n",
    "\"\"\"\n",
    "        \n",
    "        analysis_prompt += \"\"\"\n",
    "\n",
    "TASK:\n",
    "Analyze the current prompt and failed cases, then provide:\n",
    "\n",
    "1. **Error Pattern Analysis**: What are the common patterns in failures?\n",
    "2. **Prompt Issues**: What specific issues in the current prompt cause these failures?\n",
    "3. **Improvement Strategy**: How should the prompt be modified to address these issues?\n",
    "4. **Optimized Prompt**: Provide a complete rewritten prompt that addresses the identified issues.\n",
    "\n",
    "Focus on:\n",
    "- Clarity of XML tagging rules\n",
    "- Handling of OCR corrections\n",
    "- Proper handling of citation ranges\n",
    "- Consistent application of tagging rules\n",
    "- Edge case handling\n",
    "\n",
    "Provide your analysis and optimized prompt in a structured format.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(analysis_prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error in Gemini analysis: {e}\"\n",
    "    \n",
    "    def generate_optimized_prompt(self, base_prompt: str, error_analysis: str) -> str:\n",
    "        \"\"\"Generate an optimized prompt based on error analysis\"\"\"\n",
    "        \n",
    "        optimization_prompt = f\"\"\"\n",
    "Based on the error analysis, create an optimized version of this prompt for XML citation markup:\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "{base_prompt}\n",
    "\n",
    "ERROR ANALYSIS:\n",
    "{error_analysis}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Maintain the core structure and XML tags (<ref>, <canon>, <v>, <w>, <p>, <c>, <l>)\n",
    "2. Improve clarity on OCR correction rules\n",
    "3. Add specific guidance for handling citation ranges\n",
    "4. Include better examples for edge cases\n",
    "5. Ensure consistent tagging behavior\n",
    "\n",
    "Generate an improved prompt that addresses the identified issues while maintaining compatibility with the existing XML schema.\n",
    "\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(optimization_prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating optimized prompt: {e}\"\n",
    "\n",
    "class OllamaVerifier:\n",
    "    \"\"\"Local Ollama model verification system\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "        self.headers = {'Authorization': f'Bearer {api_key}'}\n",
    "    \n",
    "    def test_model_with_prompt(self, model: str, prompt: str, test_cases: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Test a model with given prompt on test cases\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for test_case in test_cases:\n",
    "            input_text = test_case['prompt']\n",
    "            expected_output = test_case['completion']\n",
    "            \n",
    "            # Construct full prompt\n",
    "            full_prompt = f\"{prompt}\\n\\nInput: {input_text}\\nOutput:\"\n",
    "            \n",
    "            try:\n",
    "                # Call Ollama API\n",
    "                response = requests.post(\n",
    "                    f\"{self.base_url}/ai/chat/completions\",\n",
    "                    headers=self.headers,\n",
    "                    json={\n",
    "                        \"model\": model,\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": full_prompt}\n",
    "                        ],\n",
    "                        \"max_tokens\": 500,\n",
    "                        \"temperature\": 0.1\n",
    "                    },\n",
    "                    timeout=120\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result_data = response.json()\n",
    "                    actual_output = result_data['choices'][0]['message']['content'].strip()\n",
    "                    \n",
    "                    # Evaluate result\n",
    "                    success = self.evaluate_xml_output(expected_output, actual_output)\n",
    "                    error = None if success else self.get_error_description(expected_output, actual_output)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'input': input_text,\n",
    "                        'expected': expected_output,\n",
    "                        'actual': actual_output,\n",
    "                        'success': success,\n",
    "                        'error': error,\n",
    "                        'model': model\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'input': input_text,\n",
    "                        'expected': expected_output,\n",
    "                        'actual': f\"API Error: {response.status_code}\",\n",
    "                        'success': False,\n",
    "                        'error': f\"HTTP {response.status_code}: {response.text}\",\n",
    "                        'model': model\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'input': input_text,\n",
    "                    'expected': expected_output,\n",
    "                    'actual': f\"Exception: {str(e)}\",\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'model': model\n",
    "                })\n",
    "                \n",
    "            # Small delay to avoid overwhelming the API\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_xml_output(self, expected: str, actual: str) -> bool:\n",
    "        \"\"\"Evaluate if XML output is correct\"\"\"\n",
    "        # Normalize whitespace\n",
    "        expected_clean = re.sub(r'\\s+', ' ', expected.strip())\n",
    "        actual_clean = re.sub(r'\\s+', ' ', actual.strip())\n",
    "        \n",
    "        # Simple exact match for now\n",
    "        # Could be enhanced with XML parsing for more sophisticated comparison\n",
    "        return expected_clean == actual_clean\n",
    "    \n",
    "    def get_error_description(self, expected: str, actual: str) -> str:\n",
    "        \"\"\"Generate error description for failed cases\"\"\"\n",
    "        if not actual.startswith('<ref>'):\n",
    "            return \"Missing <ref> wrapper\"\n",
    "        elif not actual.endswith('</ref>'):\n",
    "            return \"Missing </ref> closing tag\"\n",
    "        elif '<canon>' not in actual:\n",
    "            return \"Missing <canon> tag\"\n",
    "        else:\n",
    "            return \"XML structure or content mismatch\"\n",
    "\n",
    "class PromptOptimizationSystem:\n",
    "    \"\"\"Main prompt optimization system\"\"\"\n",
    "    \n",
    "    def __init__(self, gemini_api_key: str, ollama_base_url: str, ollama_api_key: str):\n",
    "        self.gemini_optimizer = GeminiOptimizer(gemini_api_key)\n",
    "        self.ollama_verifier = OllamaVerifier(ollama_base_url, ollama_api_key)\n",
    "        self.test_cases = reference_data['examples']\n",
    "        self.optimization_history = []\n",
    "    \n",
    "    def run_optimization_cycle(self, current_prompt: str, target_models: List[str], max_iterations: int = 3) -> Dict:\n",
    "        \"\"\"Run complete optimization cycle\"\"\"\n",
    "        \n",
    "        print(f\"🚀 Starting prompt optimization cycle\")\n",
    "        print(f\"📊 Target models: {', '.join(target_models)}\")\n",
    "        print(f\"🔄 Max iterations: {max_iterations}\")\n",
    "        print(f\"📝 Test cases: {len(self.test_cases)}\")\n",
    "        \n",
    "        best_prompt = current_prompt\n",
    "        best_performance = 0\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"🔄 Iteration {iteration + 1}/{max_iterations}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Test current prompt on all models\n",
    "            iteration_results = {}\n",
    "            overall_results = []\n",
    "            \n",
    "            for model in target_models:\n",
    "                print(f\"\\n🧪 Testing {model}...\")\n",
    "                model_results = self.ollama_verifier.test_model_with_prompt(\n",
    "                    model, best_prompt, self.test_cases\n",
    "                )\n",
    "                iteration_results[model] = model_results\n",
    "                overall_results.extend(model_results)\n",
    "                \n",
    "                # Calculate model-specific metrics\n",
    "                success_rate = sum(1 for r in model_results if r['success']) / len(model_results)\n",
    "                print(f\"   Success rate: {success_rate:.2%}\")\n",
    "            \n",
    "            # Calculate overall performance\n",
    "            overall_success_rate = sum(1 for r in overall_results if r['success']) / len(overall_results)\n",
    "            print(f\"\\n📊 Overall success rate: {overall_success_rate:.2%}\")\n",
    "            \n",
    "            # Store results\n",
    "            self.optimization_history.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'prompt': best_prompt,\n",
    "                'results': iteration_results,\n",
    "                'overall_success_rate': overall_success_rate,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            # If performance is good enough or last iteration, break\n",
    "            if overall_success_rate >= 0.9 or iteration == max_iterations - 1:\n",
    "                if overall_success_rate >= 0.9:\n",
    "                    print(f\"✅ Target performance achieved: {overall_success_rate:.2%}\")\n",
    "                break\n",
    "            \n",
    "            # Use Gemini to analyze and optimize\n",
    "            print(f\"\\n🤖 Analyzing with Gemini 2.5 Pro...\")\n",
    "            analysis = self.gemini_optimizer.analyze_prompt_performance(best_prompt, overall_results)\n",
    "            \n",
    "            print(f\"\\n📋 Analysis complete. Generating optimized prompt...\")\n",
    "            optimized_prompt = self.gemini_optimizer.generate_optimized_prompt(best_prompt, analysis)\n",
    "            \n",
    "            # Extract the actual prompt from Gemini's response (may include explanation)\n",
    "            best_prompt = self.extract_prompt_from_response(optimized_prompt)\n",
    "            \n",
    "            print(f\"✨ Prompt optimized for next iteration\")\n",
    "        \n",
    "        return {\n",
    "            'final_prompt': best_prompt,\n",
    "            'final_performance': overall_success_rate,\n",
    "            'optimization_history': self.optimization_history\n",
    "        }\n",
    "    \n",
    "    def extract_prompt_from_response(self, response: str) -> str:\n",
    "        \"\"\"Extract the actual prompt from Gemini's response\"\"\"\n",
    "        # Look for prompt markers or return the whole response if no clear markers\n",
    "        lines = response.split('\\n')\n",
    "        \n",
    "        # Try to find prompt between markers\n",
    "        in_prompt = False\n",
    "        prompt_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'OPTIMIZED PROMPT:' in line.upper() or 'IMPROVED PROMPT:' in line.upper():\n",
    "                in_prompt = True\n",
    "                continue\n",
    "            elif in_prompt and line.strip().startswith('---'):\n",
    "                break\n",
    "            elif in_prompt:\n",
    "                prompt_lines.append(line)\n",
    "        \n",
    "        if prompt_lines:\n",
    "            return '\\n'.join(prompt_lines).strip()\n",
    "        else:\n",
    "            # If no clear markers, return the whole response\n",
    "            return response.strip()\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate optimization report\"\"\"\n",
    "        if not self.optimization_history:\n",
    "            return \"No optimization history available\"\n",
    "        \n",
    "        report = \"# Prompt Optimization Report\\n\\n\"\n",
    "        \n",
    "        # Performance progression\n",
    "        report += \"## Performance Progression\\n\\n\"\n",
    "        for entry in self.optimization_history:\n",
    "            report += f\"**Iteration {entry['iteration']}**: {entry['overall_success_rate']:.2%} success rate\\n\"\n",
    "        \n",
    "        # Best performing prompt\n",
    "        best_entry = max(self.optimization_history, key=lambda x: x['overall_success_rate'])\n",
    "        report += f\"\\n## Best Performing Prompt\\n\\n\"\n",
    "        report += f\"**Success Rate**: {best_entry['overall_success_rate']:.2%}\\n\\n\"\n",
    "        report += f\"**Prompt**:\\n```\\n{best_entry['prompt']}\\n```\\n\\n\"\n",
    "        \n",
    "        # Model-specific performance\n",
    "        report += \"## Model-Specific Performance\\n\\n\"\n",
    "        for model in best_entry['results'].keys():\n",
    "            model_results = best_entry['results'][model]\n",
    "            success_rate = sum(1 for r in model_results if r['success']) / len(model_results)\n",
    "            report += f\"- **{model}**: {success_rate:.2%}\\n\"\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def visualize_performance(self):\n",
    "        \"\"\"Create performance visualization\"\"\"\n",
    "        if not self.optimization_history:\n",
    "            print(\"No data to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        iterations = [entry['iteration'] for entry in self.optimization_history]\n",
    "        overall_rates = [entry['overall_success_rate'] for entry in self.optimization_history]\n",
    "        \n",
    "        # Model-specific data\n",
    "        models = list(self.optimization_history[0]['results'].keys())\n",
    "        model_data = {model: [] for model in models}\n",
    "        \n",
    "        for entry in self.optimization_history:\n",
    "            for model in models:\n",
    "                model_results = entry['results'][model]\n",
    "                success_rate = sum(1 for r in model_results if r['success']) / len(model_results)\n",
    "                model_data[model].append(success_rate)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Overall performance\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(iterations, overall_rates, 'bo-', linewidth=2, markersize=8)\n",
    "        plt.title('Overall Performance Progress')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Success Rate')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Model-specific performance\n",
    "        plt.subplot(1, 2, 2)\n",
    "        for model in models:\n",
    "            plt.plot(iterations, model_data[model], 'o-', label=model, linewidth=2, markersize=6)\n",
    "        plt.title('Model-Specific Performance')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Success Rate')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the optimization system\n",
    "def setup_optimization_system():\n",
    "    \"\"\"Setup and configure the optimization system\"\"\"\n",
    "    print(\"🔧 Setting up Prompt Optimization System...\")\n",
    "    \n",
    "    # Check if API keys are configured\n",
    "    if GEMINI_API_KEY == \"your-gemini-api-key-here\":\n",
    "        print(\"⚠️  Please configure GEMINI_API_KEY\")\n",
    "        return None\n",
    "    \n",
    "    if OLLAMA_API_KEY == \"sk-your-api-key-here\":\n",
    "        print(\"⚠️  Please configure OLLAMA_API_KEY\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize system\n",
    "    optimizer = PromptOptimizationSystem(\n",
    "        gemini_api_key=GEMINI_API_KEY,\n",
    "        ollama_base_url=OLLAMA_BASE_URL,\n",
    "        ollama_api_key=OLLAMA_API_KEY\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Optimization system ready!\")\n",
    "    return optimizer\n",
    "\n",
    "# Example usage\n",
    "def run_optimization_example():\n",
    "    \"\"\"Run an example optimization cycle\"\"\"\n",
    "    \n",
    "    # Setup system\n",
    "    optimizer = setup_optimization_system()\n",
    "    if not optimizer:\n",
    "        return\n",
    "    \n",
    "    # Initial prompt from reference data\n",
    "    initial_prompt = reference_data['description']\n",
    "    \n",
    "    # Target models\n",
    "    target_models = ['gemma3:1b', 'gemma3:4b']\n",
    "    \n",
    "    print(f\"\\n🎯 Starting optimization with initial prompt:\")\n",
    "    print(f\"📝 Prompt length: {len(initial_prompt)} characters\")\n",
    "    print(f\"🔍 Target models: {', '.join(target_models)}\")\n",
    "    \n",
    "    # Run optimization\n",
    "    results = optimizer.run_optimization_cycle(\n",
    "        current_prompt=initial_prompt,\n",
    "        target_models=target_models,\n",
    "        max_iterations=3\n",
    "    )\n",
    "    \n",
    "    # Generate and display report\n",
    "    print(f\"\\n📊 OPTIMIZATION COMPLETE\")\n",
    "    print(f\"🎉 Final performance: {results['final_performance']:.2%}\")\n",
    "    \n",
    "    report = optimizer.generate_report()\n",
    "    print(f\"\\n{report}\")\n",
    "    \n",
    "    # Visualize results\n",
    "    optimizer.visualize_performance()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Quick test function\n",
    "def quick_test_model(model: str, prompt: str, test_input: str):\n",
    "    \"\"\"Quick test of a single model with given prompt and input\"\"\"\n",
    "    verifier = OllamaVerifier(OLLAMA_BASE_URL, OLLAMA_API_KEY)\n",
    "    \n",
    "    print(f\"🧪 Quick test: {model}\")\n",
    "    print(f\"📝 Input: {test_input}\")\n",
    "    \n",
    "    # Test single case\n",
    "    test_case = {'prompt': test_input, 'completion': 'N/A'}\n",
    "    results = verifier.test_model_with_prompt(model, prompt, [test_case])\n",
    "    \n",
    "    if results:\n",
    "        result = results[0]\n",
    "        print(f\"📤 Output: {result['actual']}\")\n",
    "        print(f\"✅ Success: {result['success']}\")\n",
    "        if result['error']:\n",
    "            print(f\"❌ Error: {result['error']}\")\n",
    "    \n",
    "    return results[0] if results else None\n",
    "\n",
    "# Display current reference data\n",
    "print(\"📚 Reference Data Loaded:\")\n",
    "print(f\"   Title: {reference_data['title']}\")\n",
    "print(f\"   Examples: {len(reference_data['examples'])}\")\n",
    "print(f\"   Description length: {len(reference_data['description'])} characters\")\n",
    "\n",
    "print(f\"\\n🔧 To start optimization, run: run_optimization_example()\")\n",
    "print(f\"🧪 For quick testing, run: quick_test_model('gemma3:1b', prompt, 'test input')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86fd27e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This comprehensive prompt optimization system provides:\n",
    "\n",
    "1\n",
    "\n",
    "Made changes.\n",
    "\n",
    ". **Gemini 2.5 Pro Integration**: Uses Gemini for intelligent prompt analysis and optimization\n",
    "2. **Local Ollama Verification**: Tests optimized prompts on your local Gemma models\n",
    "3. **Iterative Improvement**: Automatically runs optimization cycles to improve performance\n",
    "4. **Performance Metrics**: Tracks success rates and identifies error patterns\n",
    "5. **Visualization**: Creates charts showing optimization progress\n",
    "6. **Reporting**: Generates detailed reports on optimization results\n",
    "\n",
    "The system will:\n",
    "- Test your current prompt on both Gemma3:1b and Gemma3:4b\n",
    "- Use Gemini 2.5 Pro to analyze failures and suggest improvements\n",
    "- Iteratively refine the prompt based on performance data\n",
    "- Provide detailed reports and visualizations of the optimization process\n",
    "\n",
    "To use this system, you'll need to:\n",
    "1. Set up your Gemini API key\n",
    "2. Ensure your local Ollama service is running\n",
    "3. Configure the API keys in the notebook\n",
    "4. Run the optimization cycle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

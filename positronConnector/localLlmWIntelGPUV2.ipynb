{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb4c89d",
   "metadata": {},
   "source": [
    "# Local LLM with Intel GPU v2\n",
    "\n",
    "1. run Gemma model with ollama\n",
    "2. create RESTful API Wrapper with flask\n",
    "* create API key manager for API Wrapper \n",
    "* create wrapper for Oxygen Positron add-on by following:\n",
    "    * https://www.oxygenxml.com/doc/ug-addons/topics/ai-positron-custom-connector.html\n",
    "    * https://www.oxygenxml.com/doc/ug-addons/topics/ai_positron_enterprise.html\n",
    "    * https://github.com/oxygenxml/oxygen-ai-positron-custom-connector-addon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647a7d4",
   "metadata": {},
   "source": [
    "I'll help you create a complete solution for running Gemma model with Ollama and creating a RESTful API wrapper with Flask, including an API key manager and Oxygen Positron connector wrapper.\n",
    "\n",
    "## Step-by-step Solution:\n",
    "\n",
    "1. **Set up Ollama with Gemma model** - Install dependencies and pull the Gemma model\n",
    "2. **Create Flask API wrapper** - Build a RESTful API that interfaces with Ollama\n",
    "3. **Implement API key manager** - Add authentication and API key validation\n",
    "4. **Create Oxygen Positron connector** - Follow the specifications to create a compatible wrapper\n",
    "\n",
    "### [localLlmWIntelGPUV2.ipynb](file:///c%3A/Project/OCR/localLlmWIntelGPUV2.ipynb)\n",
    "\n",
    "Add implementation for running Gemma model with Ollama and creating Flask API wrapper with API key management and Oxygen Positron connector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f28f1",
   "metadata": {},
   "source": [
    "This implementation provides:\n",
    "\n",
    "1. **Ollama Integration**: Automatic installation and Gemma model management\n",
    "2. **Flask API Wrapper**: RESTful API with proper error handling\n",
    "3. **API Key Management**: SQLite-based key storage with usage tracking\n",
    "4. **Oxygen Positron Compatibility**: OpenAI-compatible endpoints that work with the Oxygen XML Author add-on\n",
    "5. **Security**: API key authentication for all endpoints\n",
    "6. **Monitoring**: Usage tracking and health checks\n",
    "\n",
    "The solution follows the Oxygen Positron custom connector specifications and provides a complete local LLM setup with Intel GPU support via Ollama.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cd749",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### 1. Setup and Installation\n",
    "- Run the notebook cell above to install dependencies and start the server\n",
    "- The initial API key will be displayed - save it securely\n",
    "- Ollama will be installed (if not present) and Gemma model will be pulled\n",
    "\n",
    "### 2. API Endpoints\n",
    "\n",
    "#### Standard Endpoints:\n",
    "- `GET /health` - Health check\n",
    "- `POST /api/generate` - Generate text with Ollama\n",
    "- `GET /api/models` - List available models\n",
    "- `POST /api/keys/generate` - Generate new API key\n",
    "\n",
    "#### Oxygen Positron Compatible Endpoints:\n",
    "- `POST /ai/chat/completions` - OpenAI-compatible chat completions\n",
    "- `GET /ai/models` - OpenAI-compatible models list\n",
    "\n",
    "### 3. Authentication\n",
    "All endpoints (except health and key generation) require API key in header:\n",
    "```\n",
    "Authorization: Bearer your-api-key-here\n",
    "```\n",
    "\n",
    "### 4. Oxygen Positron Configuration\n",
    "In Oxygen XML Author, configure the custom connector with:\n",
    "- **Base URL**: `http://localhost:5000/ai`\n",
    "- **API Key**: Your generated API key\n",
    "- **Model**: `gemma3:4b`\n",
    "\n",
    "### 5. Example Usage\n",
    "```python\n",
    "import requests\n",
    "\n",
    "headers = {'Authorization': 'Bearer your-api-key'}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n",
    "    ],\n",
    "    \"model\": \"gemma3:4b\"\n",
    "}\n",
    "\n",
    "response = requests.post('http://localhost:5000/ai/chat/completions', \n",
    "                        json=data, headers=headers)\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd293e",
   "metadata": {},
   "source": [
    "# Version 1.0 prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302efe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Initial API Key: sk-b521dae8eaab475ea5c80ae532ed8459\n",
      "Save this key - it won't be shown again!\n",
      "Starting Flask API server...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.136:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\OCR2Markup\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests', 'sqlite3']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "            else:\n",
    "                subprocess.run([\"curl\", \"-fsSL\", \"https://ollama.ai/install.sh\", \"|\", \"sh\"], shell=True)\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=\"gemma3:1b\"):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_response(self, prompt, model=\"gemma3:1b\", stream=False):\n",
    "        \"\"\"Generate response using Ollama API\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()})\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        return jsonify({'models': models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"id\": \"gemma3:4b\",\n",
    "                    \"object\": \"model\",\n",
    "                    \"created\": int(time.time()),\n",
    "                    \"owned_by\": \"local\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Initialize and run\n",
    "if __name__ == '__main__':\n",
    "    # Setup Ollama and Gemma\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    ollama_manager.install_ollama()\n",
    "    # ollama_manager.pull_gemma_model()\n",
    "    ollama_manager.list_models()\n",
    "    \n",
    "    # Generate initial API key\n",
    "    initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    print(f\"Initial API Key: {initial_key}\")\n",
    "    print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app\n",
    "    print(\"Starting Flask API server...\")\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab95b02",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemExit\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 320\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Start Flask app\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Flask API server...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\app.py:662\u001b[39m, in \u001b[36mFlask.run\u001b[39m\u001b[34m(self, host, port, debug, load_dotenv, **options)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwerkzeug\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_simple\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     \u001b[43mrun_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# reset the first request information if the development server\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# reset normally.  This makes it possible to restart the server\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# without reloader and that stuff from an interactive shell.\u001b[39;00m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28mself\u001b[39m._got_first_request = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\serving.py:1115\u001b[39m, in \u001b[36mrun_simple\u001b[39m\u001b[34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, exclude_patterns, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_reloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_with_reloader\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     \u001b[43mrun_with_reloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserve_forever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreloader_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreloader_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreloader_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1123\u001b[39m     srv.server_close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\_reloader.py:469\u001b[39m, in \u001b[36mrun_with_reloader\u001b[39m\u001b[34m(main_func, extra_files, exclude_patterns, interval, reloader_type)\u001b[39m\n\u001b[32m    467\u001b[39m             reloader.run()\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m         \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestart_with_reloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mSystemExit\u001b[39m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2de25",
   "metadata": {},
   "source": [
    "You can use the selected health check endpoint in several ways to monitor your Flask API:\n",
    "\n",
    "## 1. Test Health Check via HTTP Request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddbaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /health HTTP/1.1\" 200 143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Status: healthy\n",
      "Timestamp: 2025-06-03T16:28:15.946289\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test the health endpoint\n",
    "try:\n",
    "    response = requests.get('http://localhost:5000/health')\n",
    "    if response.status_code == 200:\n",
    "        health_data = response.json()\n",
    "        print(f\"Service Status: {health_data['status']}\")\n",
    "        print(f\"Timestamp: {health_data['timestamp']}\")\n",
    "    else:\n",
    "        print(f\"Health check failed with status: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Service is unreachable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe7ad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Browser-based Health Check\n",
    "\n",
    "Simply open your browser and navigate to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00346c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You should see a JSON response like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"status\": \"healthy\",\n",
    "  \"timestamp\": \"2025-05-31T14:30:45.123456\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d8295",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Enhance the Health Check Endpoint\n",
    "\n",
    "You can improve the health check to include more diagnostic information:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb7857",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Automated Health Monitoring Script\n",
    "\n",
    "Create a monitoring script that regularly checks the health:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /health HTTP/1.1\" 200 143\n",
      "INFO:root:✅ Service healthy - 2025-06-03T16:28:40.078615\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /health HTTP/1.1\" 200 143\n",
      "INFO:root:✅ Service healthy - 2025-06-03T16:29:12.201205\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def monitor_health(url=\"http://localhost:5000/health\", interval=30):\n",
    "    \"\"\"Monitor health endpoint every 30 seconds\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                logging.info(f\"✅ Service healthy - {data['timestamp']}\")\n",
    "            else:\n",
    "                logging.warning(f\"❌ Health check returned {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"🔥 Health check failed: {e}\")\n",
    "        \n",
    "        time.sleep(interval)\n",
    "\n",
    "# Run the monitor\n",
    "monitor_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495aff8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Command Line Health Check\n",
    "\n",
    "Use curl or PowerShell to check health from command line:\n",
    "\n",
    "**Using curl:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da934f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76160906",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Using PowerShell:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed3dc",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "Invoke-RestMethod -Uri \"http://localhost:5000/health\" -Method GET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeecc6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Integration with Load Balancers\n",
    "\n",
    "The health endpoint is designed to work with load balancers and container orchestrators. Configure your load balancer to:\n",
    "- **Health Check URL**: `http://localhost:5000/health`\n",
    "- **Expected Status Code**: `200`\n",
    "- **Check Interval**: `30 seconds`\n",
    "- **Timeout**: `5 seconds`\n",
    "\n",
    "This health check endpoint helps ensure your local LLM API service is running properly and can respond to requests from the Oxygen Positron connector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca73ed",
   "metadata": {},
   "source": [
    "# Version 2.0 without GPU\n",
    "\n",
    "## Plan\n",
    "\n",
    "The code has a SystemExit error which is likely caused by running Flask in a Jupyter notebook. Flask's `app.run()` is designed for standalone scripts, not interactive environments. I need to:\n",
    "\n",
    "1. Modify the initialization section to work in Jupyter\n",
    "2. Add proper error handling for Ollama installation\n",
    "3. Make the Flask app run in a way that's compatible with Jupyter notebooks\n",
    "4. Add better model list parsing for the oxygen_list_models endpoint\n",
    "5. Ensure proper cleanup and threading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Available models:\n",
      "NAME          ID              SIZE      MODIFIED    \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    4 weeks ago    \n",
      "gemma3:1b     8648f39daa8f    815 MB    4 weeks ago    \n",
      "gemma3:12b    f4031aab637d    8.1 GB    4 weeks ago    \n",
      "\n",
      "Flask server started on http://localhost:5000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.136:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "ollama_default_model = \"gemma3:1b\"\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    # for local instsallation\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    # servcie from DILA\n",
    "    # def __init__(self, base_url=\"http://172.27.2.129:11434\"):\n",
    "    #     self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True, text=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "                print(\"After installation, restart this notebook and run 'ollama serve' in a terminal\")\n",
    "                return False\n",
    "            else:\n",
    "                try:\n",
    "                    # Download installer script\n",
    "                    import urllib.request\n",
    "                    urllib.request.urlretrieve(\"https://ollama.ai/install.sh\", \"ollama_install.sh\")\n",
    "                    subprocess.run([\"chmod\", \"+x\", \"ollama_install.sh\"], check=True)\n",
    "                    subprocess.run([\"./ollama_install.sh\"], check=True)\n",
    "                    print(\"Ollama installed successfully\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to install Ollama: {e}\")\n",
    "                    return False\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=ollama_default_model):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            print(f\"Pulling {model_name} model...\")\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            print(f\"Make sure Ollama is running with 'ollama serve'\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_models(self, models_output):\n",
    "        \"\"\"Parse ollama list output into structured format\"\"\"\n",
    "        if not models_output:\n",
    "            return []\n",
    "        \n",
    "        models = []\n",
    "        lines = models_output.strip().split('\\n')[1:]  # Skip header\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    model_name = parts[0]\n",
    "                    models.append({\n",
    "                        \"id\": model_name,\n",
    "                        \"object\": \"model\",\n",
    "                        \"created\": int(time.time()),\n",
    "                        \"owned_by\": \"local\"\n",
    "                    })\n",
    "        return models\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_default_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API with extended timeout\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": {\n",
    "                \"num_predict\": 3000,  # Limit tokens to speed up\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Use longer timeout for qwen models\n",
    "        if \"qwen\" in model.lower():\n",
    "            timeout = 480 \n",
    "        elif \"deepseek\" in model.lower() or \"llama\" in model.lower() or \"mistral\" in model.lower():\n",
    "            timeout = 180 \n",
    "        elif \"gemma3:12b\" in model.lower():\n",
    "            timeout = 360 \n",
    "        else:\n",
    "            timeout = 100\n",
    "        # timeout = 480 if \"qwen\" in model.lower() or \"deepseek\" in model.lower() else 60\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating with {model} (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            # print(json.dumps(response.json(), indent=2))\n",
    "            result = response.json()\n",
    "            if 'response' in result:\n",
    "                print(result['response'])\n",
    "            else:\n",
    "                print(json.dumps(result, indent=2))\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout after {timeout} seconds with model {model}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:1b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        return jsonify({'models': parsed_models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format`\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    print(f\"Received data: {data}\")\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', ollama_default_model)\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        if not parsed_models:\n",
    "            # Fallback if no models found\n",
    "            parsed_models = [{\n",
    "                \"id\": ollama_default_model,\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"local\"\n",
    "            }]\n",
    "        \n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": parsed_models\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Global variable to track server thread\n",
    "server_thread = None\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask app in a separate thread\"\"\"\n",
    "    # Enable Flask debug logging\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    app.logger.setLevel(logging.DEBUG)\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread is None or not server_thread.is_alive():\n",
    "        server_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "        server_thread.start()\n",
    "        print(\"Flask server started on http://localhost:5000\")\n",
    "    else:\n",
    "        print(\"Server is already running\")\n",
    "\n",
    "def setup_and_start():\n",
    "    \"\"\"Setup Ollama and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually and run 'ollama serve' before continuing\")\n",
    "        return None\n",
    "    \n",
    "    # Try to list models (this will also test if Ollama is running)\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"Available models:\")\n",
    "        print(models)\n",
    "    else:\n",
    "        print(\"⚠️  No models found. You may need to pull a model first.\")\n",
    "        print(f\"Run: ollama pull {ollama_default_model}\")\n",
    "    \n",
    "    # Generate initial API key\n",
    "    # initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    # if initial_key:\n",
    "    #     print(f\"\\n🔑 Initial API Key: {initial_key}\")\n",
    "    #     print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app in background thread\n",
    "    start_server()\n",
    "    \n",
    "    # return initial_key\n",
    "\n",
    "# Initialize setup\n",
    "if __name__ == '__main__':\n",
    "    setup_and_start()\n",
    "else:\n",
    "    # When imported in Jupyter, just run setup\n",
    "    initial_key = setup_and_start()\n",
    "    if initial_key:\n",
    "        print(f\"\\n✅ Server setup complete!\")\n",
    "        print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "        print(f\"📚 API documentation available in the notebook cells above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af86eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models output:\n",
      "NAME          ID              SIZE      MODIFIED     \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    7 hours ago     \n",
      "gemma3:1b     8648f39daa8f    815 MB    7 hours ago     \n",
      "gemma3:12b    f4031aab637d    8.1 GB    39 hours ago    \n",
      "\n",
      "Parsed models:\n",
      "- gemma3:4b\n",
      "- gemma3:1b\n",
      "- gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "models_output = ollama_manager.list_models()\n",
    "print(\"Models output:\")\n",
    "print(models_output)\n",
    "\n",
    "# Parse models from the output\n",
    "parsed_models = ollama_manager.parse_models(models_output)\n",
    "print(\"Parsed models:\")\n",
    "for model in parsed_models:\n",
    "\tprint(f\"- {model['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c641a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/models HTTP/1.1\" 200 260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"created\": 1748968236,\n",
      "      \"id\": \"llama3.1:8b\",\n",
      "      \"object\": \"model\",\n",
      "      \"owned_by\": \"local\"\n",
      "    },\n",
      "    {\n",
      "      \"created\": 1748968236,\n",
      "      \"id\": \"gemma3:12b\",\n",
      "      \"object\": \"model\",\n",
      "      \"owned_by\": \"local\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"sk-2c76be0b113d4e54aa129a3289eea8b5\"\n",
    "headers = {'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "response = requests.get('http://localhost:5000/api/models', headers=headers)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8fa20",
   "metadata": {},
   "source": [
    "## setting\n",
    "\n",
    "1. Plugins / Oxygen AI Positron Assistant / AI Service Configuration\n",
    "* Address: `https://aipositron.oxygenxml.com`\n",
    "* Model: `by selection from GPT-4o, GPT-4o mini....`\n",
    "\n",
    "2. Plugins / Oxygen AI Positron Assistant `Enterprise` / AI Service Configuration\n",
    "* AI connector: `Custom AI service`\n",
    "* Address: `http://127.0.0.1:5000/ai/`\n",
    "* API key: `from flask API key manager`\n",
    "* Model: `by selection from gemma3:1b, gemma3:4b, deepseek-r1:1.5b(T), qwen3:4b(T)`\n",
    "* Enable text moderation: `unchecked`\n",
    "* Enable streaming: `checked`\n",
    "\n",
    "\n",
    "## result\n",
    "\n",
    "| AI Positron | model | Action\\xml | p.67c4 | t.No.366,p.348b9 |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| AI Positron |GPT-4o|ref markup opt| ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |GPT-4o mini(8b) | | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| Enterprise<br>+CustomAI |gemma3:12b<br>⚠️slower| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:4b| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<v>366</v>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |llama3.1:8b<br>⚠️slow| | ⚠️❌```<p.<p>67</p><c>c</c><l>4</l></ref>``` | ⚠️✅```<ref><canon>T</canon>.No.<w>366</w>, p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| AI Positron |GPT-4o|ref markup++| ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |GPT-4o mini(8b) | | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| Enterprise<br>+CustomAI |gemma3:12b<br>⚠️slower| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>no.366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:4b| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>no.366</w> p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:1b| | ❌```<ref><canon>p.<67</p><c>4</c>``` | ❌```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |llama3.1:8b<br>⚠️slow| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |mistral:7b<br>⚠️slow| | ❌``` <ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌``` <ref><canon>T</canon>.<v>No.</v>366,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |qwen3:8b(T)<br>⚠️very slow| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>366</w> p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |qwen3:4b(T)<br>⚠️very slow| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon> No.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |deepseek-r1:1.5b(T)<br>⚠️unstable| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>t.No.366,p.348b9</canon></ref><v>366</v><p>.348</p><c>b</c><l>9</l>``` |\n",
    "| AI Positron |GPT-4o|ref markup+| ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.No.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |GPT-4o mini(8b)| | ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.No.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| Enterprise<br>+CustomAI |gemma3:12b<br>⚠️slower| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>no.366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:4b| | ❌```<ref><canon>T</canon>.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:1b| | ❌```<ref><canon>p</canon>.<p>67</p><c>4</c>``` | ❌```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |llama3.1:8b<br>⚠️slow| | ⚠️✅``` <ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ⚠️✅```<ref><canon>T</canon>.No.<w>366</w>, p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |mistral:7b<br>⚠️slow| | ❌``` <ref><canon>T</canon>.<v>...</v>,<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<v>unknown</v>,<p>67</p><c>c</c><l>4</l></ref>``` |\n",
    "|  |qwen3:8b(T)<br>⚠️very slow| | ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>No.366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |qwen3:4b(T)<br>⚠️very slow| | ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |deepseek-r1:1.5b(T)<br>⚠️unstable| | ❌```<ref><canon>p.67</canon><v>67</v><p>67</p><c>4</c><l>1</l>-<l>296</l></ref>``` | ❌```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c><l>0</l>-<l>1</l></ref>``` |\n",
    "\n",
    "* extra comment from llama3.1:8b w ref markup+:\n",
    "    * ```It seems like you are entering a single page reference, I will just markup it with <p>, <c> and <l> elements.```\n",
    "    * ```Would you like to add the <ref> element around it as well?```\n",
    "\n",
    "* llama3.1:8b\n",
    "    * ```<p.67<c>c</c><l>4</l></ref>```\n",
    "    * ```<p.<<p>>67</p><c>c</c><l>4</l></ref>```\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243d5a1",
   "metadata": {},
   "source": [
    "❌ gemma3:12b failed with status: 500\n",
    "\n",
    "* Error: {\"error\":\"model requires more system memory (11.3 GiB) than is available (8.4 GiB)\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15674f1f",
   "metadata": {},
   "source": [
    "## pull and rm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling deepseek-r1:1.5b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-25 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled deepseek-r1:1.5b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899faca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling qwen3:4b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled qwen3:4b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"qwen3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d08706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma3:1b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-37 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:1b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma3:4b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-39 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:4b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cddfeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma3:12b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:12b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"gemma3:12b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5759b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling qwen3:8b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-101 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled qwen3:8b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"qwen3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a9da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling llama3.1:8b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29328c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling mistral:7b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled mistral:7b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"mistral:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14515933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing model: llama3.1:8b\n",
      "✅ Model llama3.1:8b removed successfully!\n",
      "deleted 'llama3.1:8b'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def remove_ollama_model(model_name):\n",
    "    \"\"\"Remove a specific model from Ollama\"\"\"\n",
    "    try:\n",
    "        print(f\"Removing model: {model_name}\")\n",
    "        result = subprocess.run(['ollama', 'rm', model_name], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Model {model_name} removed successfully!\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Failed to remove model {model_name}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error removing model: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# # Example: Remove gemma3:1b model\n",
    "# remove_ollama_model('gemma3:12b')\n",
    "# remove_ollama_model('qwen3:4b')\n",
    "# remove_ollama_model('gemma3:4b')\n",
    "# remove_ollama_model('deepseek-r1:1.5b')\n",
    "# remove_ollama_model('qwen3:8b')\n",
    "# remove_ollama_model('mistral:7b')\n",
    "remove_ollama_model('llama3.1:8b')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c5091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          ID              SIZE      MODIFIED               \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    Less than a second ago    \n",
      "gemma3:1b     8648f39daa8f    815 MB    3 minutes ago             \n",
      "gemma3:12b    f4031aab637d    8.1 GB    32 hours ago              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = ollama_manager.list_models()\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83523002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      MODIFIED      \n",
      "llama3.1:8b    46e0c10c039e    4.9 GB    7 minutes ago    \n",
      "gemma3:12b     f4031aab637d    8.1 GB    3 hours ago      \n",
      "\n",
      "Generating with llama3.1:8b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 913\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program, so I don't have feelings in the same way humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How about you? Is there something on your mind that you'd like to chat about or ask for assistance with?\n",
      "I'm just a computer program, so I don't have feelings in the same way humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How about you? Is there something on your mind that you'd like to chat about or ask for assistance with?\n",
      "Generating with llama3.1:8b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 423\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "Generating with gemma3:12b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 826\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but my systems are running smoothly and I'm ready to chat. 😊 \n",
      "\n",
      "How are *you* doing today?\n",
      "Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but my systems are running smoothly and I'm ready to chat. 😊 \n",
      "\n",
      "How are *you* doing today?\n",
      "Generating with gemma3:12b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n",
      "\n",
      "\n",
      "\n",
      "It's also its most populous city!\n",
      "The capital of France is **Paris**.\n",
      "\n",
      "\n",
      "\n",
      "It's also its most populous city!\n"
     ]
    }
   ],
   "source": [
    "model_list = ollama_manager.list_models()\n",
    "print(model_list)\n",
    "\n",
    "# response = ollama_manager.generate_response(\"Hello, how are you?\", model=\"qwen3:4b\")\n",
    "# # print(json.dumps(response, indent=2))\n",
    "# print(response.json()['response'])\n",
    "\n",
    "# response = ollama_manager.generate_response(\"What is the capital of France?\", model=\"qwen3:4b\")\n",
    "# print(response.json()['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"Hello, how are you?\", model=\"llama3.1:8b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"What is the capital of France?\", model=\"llama3.1:8b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"Hello, how are you?\", model=\"gemma3:12b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"What is the capital of France?\", model=\"gemma3:12b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0dbad",
   "metadata": {},
   "source": [
    "Collecting workspace informationBased on the error message and your notebook output, the issue with `qwen3:4b` getting HTTP 500 errors while `gemma3:4b` works fine is likely due to several factors:\n",
    "\n",
    "## 1. **Model Size and Resource Requirements**\n",
    "\n",
    "The `qwen3:4b` model may have different resource requirements than `gemma3:4b`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c474fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "gemma3:12b model info:\n",
      "  Model\n",
      "    architecture        gemma3    \n",
      "    parameters          12.2B     \n",
      "    context length      131072    \n",
      "    embedding length    3840      \n",
      "    quantization        Q4_K_M    \n",
      "\n",
      "  Capabilities\n",
      "    completion    \n",
      "    vision        \n",
      "\n",
      "  Parameters\n",
      "    stop           \"<end_of_turn>\"    \n",
      "    temperature    1                  \n",
      "    top_k          64                 \n",
      "    top_p          0.95               \n",
      "\n",
      "  License\n",
      "    Gemma Terms of Use                  \n",
      "    Last modified: February 21, 2024    \n",
      "    ...                                 \n",
      "\n",
      "\n",
      "Error getting model info: Command '['ollama', 'show', 'llama3.1:8b']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "# Check model sizes and resource usage\n",
    "def check_model_info():\n",
    "    \"\"\"Check detailed model information\"\"\"\n",
    "    try:\n",
    "        # Get detailed model info\n",
    "        model_list = [\n",
    "            \"gemma3:12b\",\n",
    "            # \"qwen3:4b\",\n",
    "            # \"gemma3:4b\",\n",
    "            # \"deepseek-r1:1.5b\",\n",
    "            # \"qwen3:8b\",\n",
    "            # \"mistral:7b\",\n",
    "            \"llama3.1:8b\",\n",
    "                      ]\n",
    "        for model in model_list:\n",
    "            result = subprocess.run([\"ollama\", \"show\", model], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"\\n{model} model info:\")\n",
    "            print(result.stdout)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error getting model info: {e}\")\n",
    "\n",
    "check_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee29922",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Timeout Issues**\n",
    "\n",
    "The error shows a read timeout after 60 seconds. `qwen3:4b` might be slower to respond:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_longer_timeout(self, prompt, model=ollama_default_model, stream=False):\n",
    "    \"\"\"Generate response using Ollama API with extended timeout\"\"\"\n",
    "    url = f\"{self.base_url}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream,\n",
    "        \"options\": {\n",
    "            \"num_predict\": 100,  # Limit tokens to speed up\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Use longer timeout for qwen models\n",
    "    timeout = 180 if \"qwen\" in model.lower() else 60\n",
    "    \n",
    "    try:\n",
    "        print(f\"Generating with {model} (timeout: {timeout}s)...\")\n",
    "        response = requests.post(url, json=payload, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout after {timeout} seconds with model {model}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Update your OllamaManager class\n",
    "ollama_manager.generate_response = generate_response_with_longer_timeout.__get__(ollama_manager, OllamaManager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b21591",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Model Loading State**\n",
    "\n",
    "`qwen3:4b` might not be fully loaded into memory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2347ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n",
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"GET /api/ps HTTP/1.1\" 200 362\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently running models:\n",
      "{\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"name\": \"gemma3:1b\",\n",
      "      \"model\": \"gemma3:1b\",\n",
      "      \"size\": 2126066688,\n",
      "      \"digest\": \"8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc\",\n",
      "      \"details\": {\n",
      "        \"parent_model\": \"\",\n",
      "        \"format\": \"gguf\",\n",
      "        \"family\": \"gemma3\",\n",
      "        \"families\": [\n",
      "          \"gemma3\"\n",
      "        ],\n",
      "        \"parameter_size\": \"999.89M\",\n",
      "        \"quantization_level\": \"Q4_K_M\"\n",
      "      },\n",
      "      \"expires_at\": \"2025-06-05T13:51:22.1910956+08:00\",\n",
      "      \"size_vram\": 1706823936\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Testing gemma3:12b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 500 62\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ gemma3:12b failed with status: 500\n",
      "Error: {\"error\":\"llama runner process has terminated: exit status 2\"}\n",
      "\n",
      "Testing gemma3:4b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 408\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ gemma3:4b responded: Hello there! How can I help you today?...\n",
      "\n",
      "Testing gemma3:1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ gemma3:1b responded: Hello there! How’s your day going so...\n"
     ]
    }
   ],
   "source": [
    "def check_model_status():\n",
    "    \"\"\"Check if models are loaded and ready\"\"\"\n",
    "    try:\n",
    "        # Check running models/processes\n",
    "        response = requests.get('http://localhost:11434/api/ps')\n",
    "        if response.status_code == 200:\n",
    "            running_models = response.json()\n",
    "            print(\"Currently running models:\")\n",
    "            print(json.dumps(running_models, indent=2))\n",
    "        \n",
    "        # Test both models with simple prompts\n",
    "        models_to_test = [\n",
    "                            \"gemma3:12b\",\n",
    "                            # \"qwen3:4b\",\n",
    "                            \"gemma3:4b\",\n",
    "                            \"gemma3:1b\",\n",
    "                            # \"deepseek-r1:1.5b\",\n",
    "                            # \"qwen3:8b\",\n",
    "                            # \"mistral:7b\",\n",
    "                            # \"llama3.1:8b\",\n",
    "                ]\n",
    "        \n",
    "        for model in models_to_test:\n",
    "            print(f\"\\nTesting {model}...\")\n",
    "            try:\n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json={\n",
    "                                           \"model\": model,\n",
    "                                           \"prompt\": \"Hello\",\n",
    "                                           \"stream\": False,\n",
    "                                           \"options\": {\"num_predict\": 10}\n",
    "                                       }, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    print(f\"✅ {model} responded: {result.get('response', '')}...\")\n",
    "                    # print(f\"✅ {model} responded: {result.get('response', '')[:50]}...\")\n",
    "                else:\n",
    "                    print(f\"❌ {model} failed with status: {response.status_code}\")\n",
    "                    print(f\"Error: {response.text}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {model} error: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking model status: {e}\")\n",
    "\n",
    "check_model_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5cdcf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Memory and Performance Issues**\n",
    "\n",
    "Your system might struggle with `qwen3:4b`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0a3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 15.8 GB\n",
      "Available RAM: 4.0 GB\n",
      "Used RAM: 11.8 GB (74.7%)\n",
      "Ollama process: PID 1704, Memory: 70.2 MB\n",
      "Ollama process: PID 2736, Memory: 11.2 MB\n",
      "Ollama process: PID 83428, Memory: 1069.5 MB\n"
     ]
    }
   ],
   "source": [
    "def check_system_resources():\n",
    "    \"\"\"Check system resources and Ollama memory usage\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        \n",
    "        # Check memory usage\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "        print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "        print(f\"Used RAM: {memory.used / (1024**3):.1f} GB ({memory.percent}%)\")\n",
    "        \n",
    "        # Check for Ollama processes\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'memory_info']):\n",
    "            if 'ollama' in proc.info['name'].lower():\n",
    "                memory_mb = proc.info['memory_info'].rss / (1024 * 1024)\n",
    "                print(f\"Ollama process: PID {proc.info['pid']}, Memory: {memory_mb:.1f} MB\")\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"Install psutil for detailed system monitoring: pip install psutil\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking resources: {e}\")\n",
    "\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6feb80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Fix: Update Flask API with Better Error Handling**\n",
    "\n",
    "Update your Flask API to handle these model-specific issues:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d8f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@app\u001b[39m\u001b[43m.\u001b[49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;129m@require_api_key\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapi_generate\u001b[39m():\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate text using Ollama with model-specific handling\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     data = request.get_json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\scaffold.py:46\u001b[39m, in \u001b[36msetupmethod.<locals>.wrapper_func\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_func\u001b[39m(\u001b[38;5;28mself\u001b[39m: Scaffold, *args: t.Any, **kwargs: t.Any) -> t.Any:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_setup_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\app.py:415\u001b[39m, in \u001b[36mApp._check_setup_finished\u001b[39m\u001b[34m(self, f_name)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_setup_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m, f_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._got_first_request:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    416\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe setup method \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m can no longer be called\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m on the application. It has already handled its first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m request, any changes will not be applied\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m consistently.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    420\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure all imports, decorators, functions, etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m needed to set up the application are done before\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m running it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         )\n",
      "\u001b[31mAssertionError\u001b[39m: The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it."
     ]
    }
   ],
   "source": [
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama with model-specific handling\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', ollama_default_model)\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    # Model-specific timeout and options\n",
    "    if \"qwen\" in model.lower():\n",
    "        timeout = 180\n",
    "        options = {\"num_predict\": 100, \"temperature\": 0.7}\n",
    "    else:\n",
    "        timeout = 60\n",
    "        options = {\"num_predict\": 150, \"temperature\": 0.7}\n",
    "    \n",
    "    try:\n",
    "        # Direct Ollama API call with custom timeout\n",
    "        response = requests.post(f'http://localhost:11434/api/generate', \n",
    "                               json={\n",
    "                                   \"model\": model,\n",
    "                                   \"prompt\": prompt,\n",
    "                                   \"stream\": stream,\n",
    "                                   \"options\": options\n",
    "                               }, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return jsonify(response.json())\n",
    "        else:\n",
    "            return jsonify({\n",
    "                'error': f'Ollama API error: {response.status_code}',\n",
    "                'details': response.text\n",
    "            }), 500\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        return jsonify({\n",
    "            'error': f'Request timeout after {timeout} seconds with model {model}',\n",
    "            'suggestion': 'Try using a smaller model or reduce the prompt length'\n",
    "        }), 504\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'Failed to generate response: {str(e)}',\n",
    "            'model': model\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53452f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. **Recommended Solutions**\n",
    "\n",
    "1. **Use a smaller Qwen model**:\n",
    "   ```python\n",
    "   ollama_manager.pull_gemma_model(\"qwen2:1.5b\")  # Smaller, faster\n",
    "   ```\n",
    "\n",
    "2. **Increase timeouts in your Oxygen Positron configuration**\n",
    "\n",
    "3. **Pre-load the model** by running a simple query first:\n",
    "   ```python\n",
    "   # Warm up the model\n",
    "   ollama_manager.generate_response(\"Hi\", model=\"qwen3:4b\")\n",
    "   ```\n",
    "\n",
    "4. **Monitor resource usage** and consider using `gemma3:1b` or `qwen2:1.5b` for better performance on your system.\n",
    "\n",
    "The issue is likely that `qwen3:4b` requires more computational resources and time to respond, causing timeouts in your Flask API and Oxygen Positron integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea3d44",
   "metadata": {},
   "source": [
    "Looking at your code, you have an Intel GPU setup but are asking about NVIDIA MX250. The NVIDIA MX250 is a low-end discrete GPU that can potentially accelerate Ollama inference. Here's how to set it up:\n",
    "\n",
    "## 1. Check NVIDIA GPU Support\n",
    "\n",
    "First, verify your MX250 is detected and CUDA-capable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff61826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU detected:\n",
      "Wed Jun  4 21:51:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.52                 Driver Version: 576.52         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce MX250         WDDM  |   00000000:2B:00.0 Off |                  N/A |\n",
      "| N/A   60C    P0            N/A  / 5001W |       0MiB /   2048MiB |      2%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "CUDA Toolkit detected:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Wed_Apr__9_19:29:17_Pacific_Daylight_Time_2025\n",
      "Cuda compilation tools, release 12.9, V12.9.41\n",
      "Build cuda_12.9.r12.9/compiler.35813241_0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def check_nvidia_gpu():\n",
    "    \"\"\"Check NVIDIA GPU status and CUDA support\"\"\"\n",
    "    try:\n",
    "        # Check nvidia-smi\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"NVIDIA GPU detected:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"NVIDIA GPU not detected or drivers not installed\")\n",
    "            return False\n",
    "            \n",
    "        # Check CUDA version\n",
    "        try:\n",
    "            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"\\nCUDA Toolkit detected:\")\n",
    "                print(result.stdout)\n",
    "            else:\n",
    "                print(\"CUDA Toolkit not found\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CUDA Toolkit not installed\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"NVIDIA drivers not installed\")\n",
    "        return False\n",
    "\n",
    "check_nvidia_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2caa8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Install CUDA Toolkit (if needed)\n",
    "\n",
    "If CUDA isn't installed:\n",
    "\n",
    "1. Download CUDA Toolkit from [NVIDIA's website](https://developer.nvidia.com/cuda-toolkit)\n",
    "2. Install the version compatible with your MX250 (CUDA 10.x or 11.x should work)\n",
    "3. Restart your system\n",
    "\n",
    "## 3. Configure Ollama for NVIDIA GPU\n",
    "\n",
    "Update your OllamaManager to utilize GPU acceleration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9112ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.setup_gpu_environment()\n",
    "        \n",
    "    def setup_gpu_environment(self):\n",
    "        \"\"\"Setup environment variables for GPU acceleration\"\"\"\n",
    "        import os\n",
    "        \n",
    "        # Force NVIDIA GPU usage\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "        os.environ['OLLAMA_GPU_LAYERS'] = '35'    # Number of layers to offload to GPU\n",
    "        \n",
    "        # For MX250 (limited VRAM), be conservative\n",
    "        os.environ['OLLAMA_GPU_MEMORY'] = '1GB'   # Limit GPU memory usage\n",
    "        \n",
    "        print(\"GPU environment configured for NVIDIA MX250\")\n",
    "    \n",
    "    def check_gpu_acceleration(self):\n",
    "        \"\"\"Check if Ollama is using GPU acceleration\"\"\"\n",
    "        try:\n",
    "            # Check Ollama GPU status\n",
    "            response = requests.get(f\"{self.base_url}/api/ps\")\n",
    "            if response.status_code == 200:\n",
    "                processes = response.json()\n",
    "                print(\"Ollama processes:\")\n",
    "                print(json.dumps(processes, indent=2))\n",
    "                \n",
    "            # Test GPU usage with a model\n",
    "            test_response = self.generate_response(\"Hello\", model=\"gemma3:1b\")\n",
    "            if test_response:\n",
    "                print(\"✅ GPU acceleration test successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ GPU acceleration test failed\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking GPU acceleration: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_default_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API with GPU optimization\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        # Optimize for MX250's limited VRAM\n",
    "        gpu_options = {\n",
    "            \"num_predict\": 1000,      # Reasonable token limit\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_ctx\": 2048,          # Context window size\n",
    "            \"num_gpu\": 35,            # GPU layers for MX250\n",
    "            \"num_thread\": 4,          # CPU threads\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": gpu_options\n",
    "        }\n",
    "        \n",
    "        # Adjust timeout based on model and GPU usage\n",
    "        if \"qwen\" in model.lower():\n",
    "            timeout = 120  # Reduced with GPU\n",
    "        elif \"gemma3:12b\" in model.lower():\n",
    "            timeout = 180  # Still large model\n",
    "        else:\n",
    "            timeout = 45   # Faster with GPU\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating with {model} on GPU (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            if 'response' in result:\n",
    "                print(f\"GPU generation completed: {len(result['response'])} chars\")\n",
    "                # Don't print full response to avoid clutter\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"GPU timeout after {timeout} seconds with model {model}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"GPU generation error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b10ef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Start Ollama with GPU Support\n",
    "\n",
    "Modify your setup function to enable GPU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95308d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start():\n",
    "    \"\"\"Setup Ollama with GPU acceleration and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama with NVIDIA MX250 GPU acceleration...\")\n",
    "    \n",
    "    # Setup GPU environment\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['OLLAMA_GPU_LAYERS'] = '35'\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually\")\n",
    "        print(\"Then run: ollama serve\")\n",
    "        return None\n",
    "    \n",
    "    # Check GPU acceleration\n",
    "    print(\"\\n🔍 Checking GPU acceleration...\")\n",
    "    gpu_working = ollama_manager.check_gpu_acceleration()\n",
    "    \n",
    "    if gpu_working:\n",
    "        print(\"✅ NVIDIA MX250 GPU acceleration is working!\")\n",
    "    else:\n",
    "        print(\"⚠️  GPU acceleration may not be working. Check CUDA installation.\")\n",
    "    \n",
    "    # Pull optimized models for MX250\n",
    "    recommended_models = [\"gemma3:1b\", \"gemma3:4b\"]  # Start with smaller models\n",
    "    for model in recommended_models:\n",
    "        print(f\"\\n📥 Pulling {model} (optimized for MX250)...\")\n",
    "        ollama_manager.pull_gemma_model(model)\n",
    "    \n",
    "    # List available models\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"\\n📋 Available models:\")\n",
    "        print(models)\n",
    "    \n",
    "    # Start Flask server\n",
    "    start_server()\n",
    "    \n",
    "    print(f\"\\n✅ Server setup complete with GPU acceleration!\")\n",
    "    print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "    print(f\"🎮 GPU Status: {'Enabled' if gpu_working else 'Check CUDA installation'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8ae65",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Monitor GPU Usage\n",
    "\n",
    "Add GPU monitoring to your health check:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5eff91",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@app\u001b[39m\u001b[43m.\u001b[49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/health\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhealth_check\u001b[39m():\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Enhanced health check with GPU monitoring\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# Check Ollama connectivity\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\scaffold.py:46\u001b[39m, in \u001b[36msetupmethod.<locals>.wrapper_func\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_func\u001b[39m(\u001b[38;5;28mself\u001b[39m: Scaffold, *args: t.Any, **kwargs: t.Any) -> t.Any:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_setup_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\app.py:415\u001b[39m, in \u001b[36mApp._check_setup_finished\u001b[39m\u001b[34m(self, f_name)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_setup_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m, f_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._got_first_request:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    416\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe setup method \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m can no longer be called\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m on the application. It has already handled its first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m request, any changes will not be applied\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m consistently.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    420\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure all imports, decorators, functions, etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m needed to set up the application are done before\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m running it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         )\n",
      "\u001b[31mAssertionError\u001b[39m: The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it."
     ]
    }
   ],
   "source": [
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check with GPU monitoring\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        gpu_status = \"unknown\"\n",
    "        \n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "            \n",
    "            # Check GPU usage\n",
    "            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                       '--format=csv,noheader,nounits'], \n",
    "                                      capture_output=True, text=True)\n",
    "            if gpu_result.returncode == 0:\n",
    "                gpu_info = gpu_result.stdout.strip().split(', ')\n",
    "                gpu_status = f\"GPU: {gpu_info[0]}% util, {gpu_info[1]}MB/{gpu_info[2]}MB memory\"\n",
    "            else:\n",
    "                gpu_status = \"GPU not available\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            gpu_status = f\"GPU check failed: {e}\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status,\n",
    "                'gpu': gpu_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85190bbb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Test GPU Acceleration\n",
    "\n",
    "Add a test function to verify GPU is working:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7decd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing llama3.1:8b performance...\n",
      "Generating with llama3.1:8b on GPU (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 404 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU generation error: 404 Client Error: Not Found for url: http://localhost:11434/api/generate\n",
      "\n",
      "⏱️  GPU Generation Time: 2.06 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0555624961853027"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_gpu_performance():\n",
    "    \"\"\"Test GPU vs CPU performance\"\"\"\n",
    "    import time\n",
    "    \n",
    "    test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "    # model = \"gemma3:1b\"  # Use small model for testing\n",
    "    model = \"llama3.1:8b\"  # Use small model for testing\n",
    "    \n",
    "    print(f\"Testing {model} performance...\")\n",
    "    \n",
    "    # Test with GPU\n",
    "    start_time = time.time()\n",
    "    gpu_response = ollama_manager.generate_response(test_prompt, model)\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n⏱️  GPU Generation Time: {gpu_time:.2f} seconds\")\n",
    "    if gpu_response and 'response' in gpu_response:\n",
    "        print(f\"📝 Response length: {len(gpu_response['response'])} characters\")\n",
    "    \n",
    "    return gpu_time\n",
    "\n",
    "# Run the test\n",
    "test_gpu_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da52399",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Important Notes for MX250:\n",
    "\n",
    "1. **Limited VRAM**: MX250 has only 2GB VRAM, so use smaller models (1B-4B parameters)\n",
    "2. **Power Efficiency**: MX250 is designed for laptops, so it may throttle under heavy load\n",
    "3. **Model Recommendations**: Start with `gemma3:1b` and `gemma3:4b` before trying larger models\n",
    "4. **Memory Management**: The GPU memory limit settings are crucial to avoid crashes\n",
    "\n",
    "The MX250 should provide modest acceleration compared to CPU-only inference, especially for smaller models. Monitor GPU utilization to ensure it's being used effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ee5fa",
   "metadata": {},
   "source": [
    "# Version 3.0 with GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97036f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU environment configured for NVIDIA MX250\n",
      "Setting up Ollama with NVIDIA MX250 GPU acceleration...\n",
      "Ollama is already installed\n",
      "\n",
      "🔍 Checking GPU acceleration...\n",
      "Ollama processes:\n",
      "{\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"name\": \"gemma3:4b\",\n",
      "      \"model\": \"gemma3:4b\",\n",
      "      \"size\": 6263581684,\n",
      "      \"digest\": \"a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a\",\n",
      "      \"details\": {\n",
      "        \"parent_model\": \"\",\n",
      "        \"format\": \"gguf\",\n",
      "        \"family\": \"gemma3\",\n",
      "        \"families\": [\n",
      "          \"gemma3\"\n",
      "        ],\n",
      "        \"parameter_size\": \"4.3B\",\n",
      "        \"quantization_level\": \"Q4_K_M\"\n",
      "      },\n",
      "      \"expires_at\": \"2025-06-04T23:21:51.1735896+08:00\",\n",
      "      \"size_vram\": 1705525300\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Generating with gemma3:1b on GPU (timeout: 120s)...\n",
      "GPU generation completed: 107 chars\n",
      "✅ GPU acceleration test successful\n",
      "✅ NVIDIA MX250 GPU acceleration is working!\n",
      "\n",
      "📥 Pulling gemma3:1b (optimized for MX250)...\n",
      "Pulling gemma3:1b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:1b\n",
      "\n",
      "📥 Pulling gemma3:4b (optimized for MX250)...\n",
      "Pulling gemma3:4b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:4b\n",
      "Available models:\n",
      "NAME          ID              SIZE      MODIFIED               \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    Less than a second ago    \n",
      "gemma3:1b     8648f39daa8f    815 MB    Less than a second ago    \n",
      "gemma3:12b    f4031aab637d    8.1 GB    2 days ago                \n",
      "\n",
      "Flask server started on http://localhost:5000\n",
      " * Serving Flask app '__main__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.50.43:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received data: {'model': 'gemma3:4b', 'messages': [{'role': 'system', 'content': \"${contextInfo} ${contextInfo} You are an experienced Buddhist Tripitaka expert and XML markup editor. The selected string is an in-text citation from a paper in a Japanese Buddhist Research periodical, referencing the Buddhist Tripitaka. You must identify the key semantic elements of the citation and then mark them up with custom TEI P5 XML tags by following these procedures: 1) Enclose the entire string within `<ref>` tags for the citation; 2) Markup the key semantic elements using the following tags, as demonstrated in the examples below: 2a) `<canon>` for abbreviations of the Buddhist Tripitaka (Tripitaka code only, e.g., T, ChT, 大正, 大正藏); 2b) `<v>` for volume(number only, representing the volumn number or between '第' and '卷'); 2c) `<w>` for work (number only, typically the sutra serial number after 'No.'), `<p>` for page (number only,  preceded by 'p', 'pp' or '頁'), `<c>` for column (column code only, e.g., a, b, c, or 上段, 中段, 下段 in Chinese), and `<l>` for line number within a column; 3-1) Sometimes, only the volume or work may be present, which is acceptable; 3-2) Combinations of `<p>`, `<c>`, and `<l>` are used to represent range of citation; 3-3) For some citations, `<canon>`, `<v>`, and `<w>` may be skipped for 'ibid' (indicating the same source); 4) You are only allowed to markup the key semantic elements of the string with the specified custom XML tags. Do not add extra characters or punctuation to the original string. Leave all other field indicators(`No.`, `no.`, `第`, `卷`, `P`, `PP`,  `頁`) or punctuation(before or after for separator) untagged and between tags in their original order; 5) There are three exceptions replacements caused by OCR: Replace the first character with 'T' if it is 't'. Replace the first three characters with 'ChT' if they are 'cht' or 'chT'. Replace 'no' with 'No' when it precedes a work number. /n/nValidation: The length of the final marked-up string in XML, excluding the XML tag pairs themselves, must be equal to the original string.\"}, {'role': 'user', 'content': '『大正藏』第四卷359a1-b7'}, {'role': 'assistant', 'content': '<ref>『<canon>大正藏</canon>』第<v>四</v>卷<p>359</p><c>a </c><l>1</l>-<c>b </c><l>7</l></ref>'}, {'role': 'user', 'content': '卍続一-三四、五七a-b'}, {'role': 'assistant', 'content': '<ref><canon>卍続</canon>一-三四、<p>五七</p><c>a</c>-<c>b</c></ref>'}, {'role': 'user', 'content': '大正二八、二七三上、七行'}, {'role': 'assistant', 'content': '<ref><canon>大正</canon><v>二八</v>、<p>二七三</p><c>上</c>、<l>七</l>行</ref>'}, {'role': 'user', 'content': '大正:no.310, 373, 上段 4-5'}, {'role': 'assistant', 'content': '<ref><canon>大正</canon>:No.<w>310</w>, <p>373</p>, <c>上段</c> <l>4</l>-<l>5</l></ref>'}, {'role': 'user', 'content': '大正18 no.865 p.216.b.4-12'}, {'role': 'assistant', 'content': '<ref><canon>大正</canon><v>18</v> No.<w>865</w> p.<p>216</p>.<c>b</c>.<l>4</l>-<l>12</l></ref>'}, {'role': 'user', 'content': 'cht.2,p.69c5-9'}, {'role': 'assistant', 'content': '<ref><canon>ChT</canon>.<v>2</v>,p.<p>69</p><c>c</c><l>5</l>-<l>9</l></ref>'}, {'role': 'user', 'content': 't.31,757a15-17;924c10-12'}, {'role': 'assistant', 'content': '<ref><canon>T</canon>.<v>31</v>,<p>757</p><c>a</c><l>15</l>-<l>17</l>;<p>924</p><c>c</c><l>10</l>-<l>12</l></ref>'}, {'role': 'user', 'content': 't.43,295c1-296a19'}, {'role': 'assistant', 'content': '<ref><canon>T</canon>.<v>43</v>,<p>295</p><c>c</c><l>1</l>-<p>296</p><<c>a</c><l>19</l></ref>'}, {'role': 'user', 'content': 't.23,926b29-c12'}, {'role': 'assistant', 'content': '<ref><canon>T</canon>.<v>23</v>,<p>926</p><c>b</c><l>29</l>-<c>c</c><l>12</l></ref>'}, {'role': 'user', 'content': 't.22,761a21-29'}, {'role': 'assistant', 'content': '<ref><canon>T</canon>.<v>22</v>,<p>761</p><c>a</c><l>21</l>-<l>29</l></ref>'}, {'role': 'user', 'content': 't.no.366,p.348b9'}], 'temperature': 0.0, 'stream': True}\n",
      "Generating with gemma3:4b on GPU (timeout: 120s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffw\\AppData\\Local\\Temp\\ipykernel_43808\\3855877087.py:342: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'timestamp': datetime.utcnow().isoformat(),\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 23:18:52] \"GET /GPUhealth HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [04/Jun/2025 23:20:29] \"\u001b[35m\u001b[1mPOST /ai/chat/completions HTTP/1.1\u001b[0m\" 500 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU timeout after 120 seconds with model gemma3:4b\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "ollama_default_model = \"gemma3:1b\"\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.setup_gpu_environment()\n",
    "        \n",
    "    def setup_gpu_environment(self):\n",
    "        \"\"\"Setup environment variables for GPU acceleration\"\"\"\n",
    "        import os\n",
    "        \n",
    "        # Force NVIDIA GPU usage\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "        os.environ['OLLAMA_GPU_LAYERS'] = '35'    # Number of layers to offload to GPU\n",
    "        \n",
    "        # For MX250 (limited VRAM), be conservative\n",
    "        os.environ['OLLAMA_GPU_MEMORY'] = '1GB'   # Limit GPU memory usage\n",
    "        \n",
    "        print(\"GPU environment configured for NVIDIA MX250\")\n",
    "\n",
    "    def check_gpu_acceleration(self):\n",
    "        \"\"\"Check if Ollama is using GPU acceleration\"\"\"\n",
    "        try:\n",
    "            # Check Ollama GPU status\n",
    "            response = requests.get(f\"{self.base_url}/api/ps\")\n",
    "            if response.status_code == 200:\n",
    "                processes = response.json()\n",
    "                print(\"Ollama processes:\")\n",
    "                print(json.dumps(processes, indent=2))\n",
    "                \n",
    "            # Test GPU usage with a model\n",
    "            test_response = self.generate_response(\"Hello\", model=\"gemma3:1b\")\n",
    "            if test_response:\n",
    "                print(\"✅ GPU acceleration test successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ GPU acceleration test failed\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking GPU acceleration: {e}\")\n",
    "            return False\n",
    "\n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True, text=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "                print(\"After installation, restart this notebook and run 'ollama serve' in a terminal\")\n",
    "                return False\n",
    "            else:\n",
    "                try:\n",
    "                    # Download installer script\n",
    "                    import urllib.request\n",
    "                    urllib.request.urlretrieve(\"https://ollama.ai/install.sh\", \"ollama_install.sh\")\n",
    "                    subprocess.run([\"chmod\", \"+x\", \"ollama_install.sh\"], check=True)\n",
    "                    subprocess.run([\"./ollama_install.sh\"], check=True)\n",
    "                    print(\"Ollama installed successfully\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to install Ollama: {e}\")\n",
    "                    return False\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=ollama_default_model):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            print(f\"Pulling {model_name} model...\")\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            print(f\"Make sure Ollama is running with 'ollama serve'\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_models(self, models_output):\n",
    "        \"\"\"Parse ollama list output into structured format\"\"\"\n",
    "        if not models_output:\n",
    "            return []\n",
    "        \n",
    "        models = []\n",
    "        lines = models_output.strip().split('\\n')[1:]  # Skip header\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    model_name = parts[0]\n",
    "                    models.append({\n",
    "                        \"id\": model_name,\n",
    "                        \"object\": \"model\",\n",
    "                        \"created\": int(time.time()),\n",
    "                        \"owned_by\": \"local\"\n",
    "                    })\n",
    "        return models\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_default_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API with extended timeout\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "\n",
    "        # Optimize for MX250's limited VRAM\n",
    "        gpu_options = {\n",
    "            \"num_predict\": 3000,      # Reasonable token limit\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_ctx\": 2048,          # Context window size\n",
    "            \"num_gpu\": 35,            # GPU layers for MX250\n",
    "            \"num_thread\": 4,          # CPU threads\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            # \"options\": {\n",
    "            #     \"num_predict\": 3000,  # Limit tokens to speed up\n",
    "            #     \"temperature\": 0.7\n",
    "            # }\n",
    "            \"options\": gpu_options\n",
    "        }\n",
    "        \n",
    "        # Use longer timeout for qwen models\n",
    "        if \"qwen\" in model.lower():\n",
    "            timeout = 480 \n",
    "        elif \"deepseek\" in model.lower() or \"llama\" in model.lower() or \"mistral\" in model.lower():\n",
    "            timeout = 180 \n",
    "        elif \"gemma3:12b\" in model.lower():\n",
    "            timeout = 360 \n",
    "        else:\n",
    "            timeout = 120\n",
    "        # timeout = 480 if \"qwen\" in model.lower() or \"deepseek\" in model.lower() else 60\n",
    "        \n",
    "        # try:\n",
    "        #     print(f\"Generating with {model} (timeout: {timeout}s)...\")\n",
    "        #     response = requests.post(url, json=payload, timeout=timeout)\n",
    "        #     response.raise_for_status()\n",
    "        #     # print(json.dumps(response.json(), indent=2))\n",
    "        #     result = response.json()\n",
    "        #     if 'response' in result:\n",
    "        #         print(result['response'])\n",
    "        #     else:\n",
    "        #         print(json.dumps(result, indent=2))\n",
    "        #     return response.json()\n",
    "        # except requests.exceptions.Timeout:\n",
    "        #     print(f\"Timeout after {timeout} seconds with model {model}\")\n",
    "        #     return None\n",
    "        # except requests.exceptions.RequestException as e:\n",
    "        #     print(f\"Error generating response: {e}\")\n",
    "        #     return None\n",
    "\n",
    "        try:\n",
    "            print(f\"Generating with {model} on GPU (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            if 'response' in result:\n",
    "                print(f\"GPU generation completed: {len(result['response'])} chars\")\n",
    "                # Don't print full response to avoid clutter\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"GPU timeout after {timeout} seconds with model {model}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"GPU generation error: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/GPUhealth', methods=['GET'])\n",
    "def GPU_health_check():\n",
    "    \"\"\"Enhanced health check with GPU monitoring\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        gpu_status = \"unknown\"\n",
    "        \n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "            \n",
    "            # Check GPU usage\n",
    "            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                       '--format=csv,noheader,nounits'], \n",
    "                                      capture_output=True, text=True)\n",
    "            if gpu_result.returncode == 0:\n",
    "                gpu_info = gpu_result.stdout.strip().split(', ')\n",
    "                gpu_status = f\"GPU: {gpu_info[0]}% util, {gpu_info[1]}MB/{gpu_info[2]}MB memory\"\n",
    "            else:\n",
    "                gpu_status = \"GPU not available\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            gpu_status = f\"GPU check failed: {e}\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status,\n",
    "                'gpu': gpu_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        return jsonify({'models': parsed_models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format`\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    print(f\"Received data: {data}\")\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', ollama_default_model)\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        if not parsed_models:\n",
    "            # Fallback if no models found\n",
    "            parsed_models = [{\n",
    "                \"id\": ollama_default_model,\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"local\"\n",
    "            }]\n",
    "        \n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": parsed_models\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Global variable to track server thread\n",
    "server_thread = None\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask app in a separate thread\"\"\"\n",
    "    # Enable Flask debug logging\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    app.logger.setLevel(logging.DEBUG)\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread is None or not server_thread.is_alive():\n",
    "        server_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "        server_thread.start()\n",
    "        print(\"Flask server started on http://localhost:5000\")\n",
    "    else:\n",
    "        print(\"Server is already running\")\n",
    "\n",
    "def setup_and_start():\n",
    "    # \"\"\"Setup Ollama and start the Flask server\"\"\"\n",
    "    # print(\"Setting up Ollama and Gemma model...\")\n",
    "    \n",
    "    \"\"\"Setup Ollama with GPU acceleration and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama with NVIDIA MX250 GPU acceleration...\")\n",
    "\n",
    "    # Setup GPU environment\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['OLLAMA_GPU_LAYERS'] = '35'\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually and run 'ollama serve' before continuing\")\n",
    "        return None\n",
    "    \n",
    "    # Check GPU acceleration\n",
    "    print(\"\\n🔍 Checking GPU acceleration...\")\n",
    "    gpu_working = ollama_manager.check_gpu_acceleration()\n",
    "    \n",
    "    if gpu_working:\n",
    "        print(\"✅ NVIDIA MX250 GPU acceleration is working!\")\n",
    "    else:\n",
    "        print(\"⚠️  GPU acceleration may not be working. Check CUDA installation.\")\n",
    "    \n",
    "    # Pull optimized models for MX250\n",
    "    recommended_models = [\"gemma3:1b\", \"gemma3:4b\"]  # Start with smaller models\n",
    "    for model in recommended_models:\n",
    "        print(f\"\\n📥 Pulling {model} (optimized for MX250)...\")\n",
    "        ollama_manager.pull_gemma_model(model)\n",
    "\n",
    "    # Try to list models (this will also test if Ollama is running)\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"Available models:\")\n",
    "        print(models)\n",
    "    else:\n",
    "        print(\"⚠️  No models found. You may need to pull a model first.\")\n",
    "        print(f\"Run: ollama pull {ollama_default_model}\")\n",
    "    \n",
    "    # Generate initial API key\n",
    "    # initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    # if initial_key:\n",
    "    #     print(f\"\\n🔑 Initial API Key: {initial_key}\")\n",
    "    #     print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app in background thread\n",
    "    start_server()\n",
    "    \n",
    "    # return initial_key\n",
    "\n",
    "# Initialize setup\n",
    "if __name__ == '__main__':\n",
    "    setup_and_start()\n",
    "else:\n",
    "    # When imported in Jupyter, just run setup\n",
    "    initial_key = setup_and_start()\n",
    "    if initial_key:\n",
    "        print(f\"\\n✅ Server setup complete!\")\n",
    "        print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "        print(f\"📚 API documentation available in the notebook cells above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1290c2",
   "metadata": {},
   "source": [
    "Great question! Yes, **theoretically you can utilize the 8GB shared memory for loading larger models** like `gemma3:4b` with the MX250, but there are important caveats to understand.\n",
    "\n",
    "## How MX250 Memory Works\n",
    "\n",
    "The MX250 has:\n",
    "- **2GB dedicated VRAM** (fast GPU memory)\n",
    "- **Access to up to 8GB shared system RAM** (slower, shared with CPU)\n",
    "\n",
    "## Update Your GPU Performance Test\n",
    "\n",
    "Let's modify your test function to monitor memory usage with larger models:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fd1495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing gemma3:1b performance...\n",
      "==================================================\n",
      "📊 System RAM before: 10.5GB used / 15.8GB total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 GPU memory before: 0MB / 2048MB\n",
      "Generating with gemma3:1b on GPU (timeout: 60s)...\n",
      "GPU timeout after 60 seconds with model gemma3:1b\n",
      "\n",
      "⏱️  Generation Time: 62.06 seconds\n",
      "📊 System RAM after: 11.1GB used (diff: +0.6GB)\n",
      "🎮 GPU memory after: 883MB / 2048MB\n",
      "✅ gemma3:1b test completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing gemma3:4b performance...\n",
      "==================================================\n",
      "📊 System RAM before: 11.1GB used / 15.8GB total\n",
      "🎮 GPU memory before: 883MB / 2048MB\n",
      "Generating with gemma3:4b on GPU (timeout: 60s)...\n",
      "GPU timeout after 60 seconds with model gemma3:4b\n",
      "\n",
      "⏱️  Generation Time: 62.06 seconds\n",
      "📊 System RAM after: 13.9GB used (diff: +2.8GB)\n",
      "🎮 GPU memory after: 1959MB / 2048MB\n",
      "✅ gemma3:4b test completed\n"
     ]
    }
   ],
   "source": [
    "def test_gpu_performance_with_memory_monitoring():\n",
    "    \"\"\"Test GPU performance with memory monitoring for larger models\"\"\"\n",
    "    import time\n",
    "    import psutil\n",
    "    \n",
    "    test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "    models_to_test = [\n",
    "        \"gemma3:1b\",   # ~1.3GB - fits in VRAM\n",
    "        \"gemma3:4b\",   # ~3.3GB - needs shared memory\n",
    "    ]\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {model} performance...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Check memory before\n",
    "        memory_before = psutil.virtual_memory()\n",
    "        print(f\"📊 System RAM before: {memory_before.used / (1024**3):.1f}GB used / {memory_before.total / (1024**3):.1f}GB total\")\n",
    "        \n",
    "        # Check GPU memory before (if nvidia-smi available)\n",
    "        try:\n",
    "            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', \n",
    "                                       '--format=csv,noheader,nounits'], \n",
    "                                      capture_output=True, text=True)\n",
    "            if gpu_result.returncode == 0:\n",
    "                gpu_mem = gpu_result.stdout.strip().split(', ')\n",
    "                print(f\"🎮 GPU memory before: {gpu_mem[0]}MB / {gpu_mem[1]}MB\")\n",
    "        except:\n",
    "            print(\"🎮 GPU memory check not available\")\n",
    "        \n",
    "        # Test generation\n",
    "        start_time = time.time()\n",
    "        response = ollama_manager.generate_response(test_prompt, model)\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n⏱️  Generation Time: {generation_time:.2f} seconds\")\n",
    "        if response and 'response' in response:\n",
    "            print(f\"📝 Response length: {len(response['response'])} characters\")\n",
    "            \n",
    "        # Check memory after\n",
    "        memory_after = psutil.virtual_memory()\n",
    "        memory_diff = (memory_after.used - memory_before.used) / (1024**3)\n",
    "        print(f\"📊 System RAM after: {memory_after.used / (1024**3):.1f}GB used (diff: {memory_diff:+.1f}GB)\")\n",
    "        \n",
    "        # Check GPU memory after\n",
    "        try:\n",
    "            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', \n",
    "                                       '--format=csv,noheader,nounits'], \n",
    "                                      capture_output=True, text=True)\n",
    "            if gpu_result.returncode == 0:\n",
    "                gpu_mem_after = gpu_result.stdout.strip().split(', ')\n",
    "                print(f\"🎮 GPU memory after: {gpu_mem_after[0]}MB / {gpu_mem_after[1]}MB\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(f\"✅ {model} test completed\")\n",
    "        \n",
    "        # Small delay between tests\n",
    "        time.sleep(2)\n",
    "\n",
    "# Run the enhanced test\n",
    "test_gpu_performance_with_memory_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1cb2e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Configure Ollama for Shared Memory Usage\n",
    "\n",
    "Update your OllamaManager to better utilize shared memory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29390d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.setup_mx250_environment()\n",
    "        \n",
    "    def setup_mx250_environment(self):\n",
    "        \"\"\"Setup environment optimized for MX250 with shared memory\"\"\"\n",
    "        import os\n",
    "        \n",
    "        # MX250 with shared memory configuration\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "        \n",
    "        # Allow Ollama to use both VRAM and shared memory\n",
    "        os.environ['OLLAMA_GPU_LAYERS'] = '20'        # Conservative layer count\n",
    "        os.environ['OLLAMA_GPU_MEMORY'] = '6GB'       # Use up to 6GB (2GB VRAM + 4GB shared)\n",
    "        \n",
    "        # Performance optimization for shared memory\n",
    "        os.environ['OLLAMA_NUM_PARALLEL'] = '1'       # Avoid memory fragmentation\n",
    "        os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'  # Only load one model at a time\n",
    "        \n",
    "        print(\"🎮 MX250 environment configured for shared memory usage\")\n",
    "        print(\"📝 Configuration:\")\n",
    "        print(f\"   - GPU Layers: {os.environ.get('OLLAMA_GPU_LAYERS')}\")\n",
    "        print(f\"   - GPU Memory: {os.environ.get('OLLAMA_GPU_MEMORY')}\")\n",
    "        print(f\"   - Max Models: {os.environ.get('OLLAMA_MAX_LOADED_MODELS')}\")\n",
    "\n",
    "    def generate_response_with_memory_optimization(self, prompt, model=\"gemma3:4b\", stream=False):\n",
    "        \"\"\"Generate response with MX250 memory optimization\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        # Model-specific optimization\n",
    "        if \"gemma3:4b\" in model.lower():\n",
    "            # For 3.3GB model on MX250\n",
    "            gpu_options = {\n",
    "                \"num_predict\": 2000,      # Reasonable token limit\n",
    "                \"temperature\": 0.7,\n",
    "                \"num_ctx\": 4096,          # Context window\n",
    "                \"num_gpu\": 20,            # Conservative GPU layers for shared memory\n",
    "                \"num_thread\": 2,          # Fewer threads to avoid memory pressure\n",
    "                \"numa\": False,            # Disable NUMA for consistent memory access\n",
    "            }\n",
    "            timeout = 120\n",
    "        elif \"gemma3:1b\" in model.lower():\n",
    "            # For 1.3GB model (fits in VRAM)\n",
    "            gpu_options = {\n",
    "                \"num_predict\": 2000,\n",
    "                \"temperature\": 0.7,\n",
    "                \"num_ctx\": 4096,\n",
    "                \"num_gpu\": 35,            # More layers can fit in VRAM\n",
    "                \"num_thread\": 4,\n",
    "            }\n",
    "            timeout = 60\n",
    "        else:\n",
    "            # Default settings\n",
    "            gpu_options = {\n",
    "                \"num_predict\": 1500,\n",
    "                \"temperature\": 0.7,\n",
    "                \"num_ctx\": 2048,\n",
    "                \"num_gpu\": 15,\n",
    "                \"num_thread\": 2,\n",
    "            }\n",
    "            timeout = 90\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": gpu_options\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            print(f\"🚀 Generating with {model} using shared memory (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            if 'response' in result:\n",
    "                print(f\"✅ Generation completed: {len(result['response'])} chars\")\n",
    "                print(f\"📱 Model loaded in: {'VRAM+Shared' if 'gemma3:4b' in model else 'VRAM'} memory\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"⏰ Timeout after {timeout} seconds - model may be too large for available memory\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Generation error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Update the manager with new method\n",
    "ollama_manager.generate_response = ollama_manager.generate_response_with_memory_optimization.__get__(ollama_manager, OllamaManager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de4ef4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Check Model Loading in Shared Memory\n",
    "\n",
    "Add a function to verify how models are loaded:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec88288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_memory_usage():\n",
    "    \"\"\"Check how models are using memory\"\"\"\n",
    "    try:\n",
    "        # Check which models are currently loaded\n",
    "        response = requests.get('http://localhost:11434/api/ps')\n",
    "        if response.status_code == 200:\n",
    "            loaded_models = response.json()\n",
    "            print(\"📋 Currently loaded models:\")\n",
    "            print(json.dumps(loaded_models, indent=2))\n",
    "            \n",
    "            # Analyze memory distribution\n",
    "            if loaded_models.get('models'):\n",
    "                for model_info in loaded_models['models']:\n",
    "                    model_name = model_info.get('name', 'unknown')\n",
    "                    size = model_info.get('size', 0)\n",
    "                    print(f\"\\n📊 Model: {model_name}\")\n",
    "                    print(f\"   Size: {size / (1024**3):.1f}GB\")\n",
    "                    \n",
    "                    # Estimate memory distribution\n",
    "                    if size > 2 * 1024**3:  # > 2GB\n",
    "                        print(f\"   🎮 Memory: VRAM (2GB) + Shared RAM ({(size - 2*1024**3) / 1024**3:.1f}GB)\")\n",
    "                    else:\n",
    "                        print(f\"   🎮 Memory: VRAM ({size / 1024**3:.1f}GB)\")\n",
    "        else:\n",
    "            print(\"❌ Could not check loaded models\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking memory usage: {e}\")\n",
    "\n",
    "# Test loading gemma3:4b\n",
    "print(\"Testing gemma3:4b model loading...\")\n",
    "response = ollama_manager.generate_response(\"Hello\", model=\"gemma3:4b\")\n",
    "check_model_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edeaf7c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Performance Expectations\n",
    "\n",
    "Here's what to expect with `gemma3:4b` on MX250:\n",
    "\n",
    "| Memory Type | Speed | Capacity | Usage for gemma3:4b |\n",
    "|-------------|--------|----------|-------------------|\n",
    "| **VRAM** | Fast | 2GB | First 2GB of model |\n",
    "| **Shared RAM** | Slower | 6GB available | Remaining 1.3GB of model |\n",
    "| **Total** | Mixed | 8GB | Full 3.3GB model fits |\n",
    "\n",
    "## Key Considerations\n",
    "\n",
    "1. **Performance Impact**: Shared memory is **significantly slower** than VRAM\n",
    "2. **Generation Speed**: Expect 2-3x slower inference compared to models that fit entirely in VRAM\n",
    "3. **Memory Pressure**: May affect system performance when other applications need RAM\n",
    "4. **Stability**: Shared memory usage can be less stable than dedicated VRAM\n",
    "\n",
    "## Optimized Model Recommendations for MX250\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c80bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mx250_optimized_models():\n",
    "    \"\"\"Get model recommendations optimized for MX250\"\"\"\n",
    "    recommendations = {\n",
    "        \"fits_in_vram\": {\n",
    "            \"models\": [\"gemma3:1b\", \"qwen2:1.5b\"],\n",
    "            \"performance\": \"Fast\",\n",
    "            \"memory\": \"2GB VRAM only\",\n",
    "            \"recommended\": True\n",
    "        },\n",
    "        \"uses_shared_memory\": {\n",
    "            \"models\": [\"gemma3:4b\", \"llama3.1:3b\"],\n",
    "            \"performance\": \"Moderate (2-3x slower)\",\n",
    "            \"memory\": \"2GB VRAM + 2-4GB Shared\",\n",
    "            \"recommended\": \"For better quality when speed isn't critical\"\n",
    "        },\n",
    "        \"too_large\": {\n",
    "            \"models\": [\"gemma3:12b\", \"llama3.1:8b\"],\n",
    "            \"performance\": \"Slow/Unstable\",\n",
    "            \"memory\": \"> 8GB total\",\n",
    "            \"recommended\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, info in recommendations.items():\n",
    "        print(f\"\\n{category.upper().replace('_', ' ')}:\")\n",
    "        print(f\"  Models: {', '.join(info['models'])}\")\n",
    "        print(f\"  Performance: {info['performance']}\")\n",
    "        print(f\"  Memory: {info['memory']}\")\n",
    "        print(f\"  Recommended: {info['recommended']}\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "get_mx250_optimized_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479d814",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Yes, you can load `gemma3:4b` (3.3GB) using MX250's shared memory access**, but expect:\n",
    "\n",
    "- ✅ **Works**: Model will load and run\n",
    "- ⚠️ **Slower**: 2-3x slower than VRAM-only models\n",
    "- ⚠️ **System Impact**: Uses significant system RAM\n",
    "- ✅ **Better Quality**: Better responses than smaller models\n",
    "\n",
    "**Best Practice**: Use `gemma3:1b` for speed, `gemma3:4b` for quality when you can tolerate slower responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

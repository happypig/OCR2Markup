{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895924e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests flask flask_cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb02a74",
   "metadata": {},
   "source": [
    "Yes, it's possible to install and run Ollama with the Gemma model from a Jupyter notebook and provide a RESTful API. Here's how to set it up:\n",
    "\n",
    "## 1. Install Ollama\n",
    "\n",
    "First, install Ollama on your Windows machine. You can download it from the official website or use the notebook to install it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Download and install Ollama (run this once)\n",
    "def install_ollama():\n",
    "    \"\"\"Download and install Ollama on Windows\"\"\"\n",
    "    print(\"Please download Ollama from: https://ollama.ai/download\")\n",
    "    print(\"Or run: winget install Ollama.Ollama\")\n",
    "    \n",
    "# Uncomment to install\n",
    "install_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Download and install Ollama (run this once)\n",
    "def install_ollama():\n",
    "    \"\"\"Download and install Ollama on Windows using winget\"\"\"\n",
    "    try:\n",
    "        print(\"Installing Ollama using winget...\")\n",
    "        result = subprocess.run(['winget', 'install', 'Ollama.Ollama'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"Ollama installed successfully!\")\n",
    "            print(\"You may need to restart your terminal or add Ollama to PATH\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Installation failed: {result.stderr}\")\n",
    "            print(\"Alternative: Download from https://ollama.ai/download\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error installing Ollama: {e}\")\n",
    "        print(\"Manual installation required:\")\n",
    "        print(\"1. Download from: https://ollama.ai/download\")\n",
    "        print(\"2. Or run in PowerShell/CMD: winget install Ollama.Ollama\")\n",
    "        return False\n",
    "\n",
    "# Install Ollama\n",
    "install_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0894ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def install_ollama_with_monitoring():\n",
    "    \"\"\"Install Ollama with detailed monitoring\"\"\"\n",
    "    try:\n",
    "        print(\"Starting Ollama installation with winget...\")\n",
    "        print(\"This may take several minutes...\\n\")\n",
    "        \n",
    "        # Run installation with real-time output\n",
    "        process = subprocess.Popen(\n",
    "            ['winget', 'install', 'Ollama.Ollama', '--accept-package-agreements', '--accept-source-agreements'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            shell=True\n",
    "        )\n",
    "        \n",
    "        # Monitor output in real-time\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "        \n",
    "        return_code = process.poll()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"\\n✅ Installation completed successfully!\")\n",
    "            \n",
    "            # Verify installation\n",
    "            print(\"Verifying installation...\")\n",
    "            verify_result = subprocess.run(['ollama', '--version'], \n",
    "                                         capture_output=True, text=True, shell=True)\n",
    "            if verify_result.returncode == 0:\n",
    "                print(f\"✅ Verification successful: {verify_result.stdout.strip()}\")\n",
    "            else:\n",
    "                print(\"⚠️ Installation completed but Ollama not found in PATH\")\n",
    "                print(\"You may need to restart your terminal or VS Code\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\n❌ Installation failed with return code: {return_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during installation: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama is installed and running\"\"\"\n",
    "    print(\"Checking Ollama status...\")\n",
    "    \n",
    "    # Check if Ollama is installed\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Ollama installed: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"❌ Ollama not found\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking Ollama: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if Ollama service is running\n",
    "    try:\n",
    "        service_check = subprocess.run(['sc', 'query', 'Ollama'], \n",
    "                                     capture_output=True, text=True, shell=True)\n",
    "        if 'RUNNING' in service_check.stdout:\n",
    "            print(\"✅ Ollama service is running\")\n",
    "        else:\n",
    "            print(\"⚠️ Ollama service is not running\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not check service status: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Check current status first\n",
    "if not check_ollama_status():\n",
    "    print(\"\\nProceeding with installation...\")\n",
    "    install_ollama_with_monitoring()\n",
    "    print(\"\\nRechecking status after installation...\")\n",
    "    check_ollama_status()\n",
    "else:\n",
    "    print(\"Ollama is already installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b83fae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Pull and Run Gemma Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4fc3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started on http://localhost:11434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Pull the Gemma model (this will take some time)\n",
    "def setup_gemma():\n",
    "    \"\"\"Pull the Gemma model using Ollama\"\"\"\n",
    "    try:\n",
    "        # Pull gemma model\n",
    "        # result = subprocess.run(['ollama', 'pull', 'gemma3:4b'], \n",
    "        #                       capture_output=True, text=True, shell=True)\n",
    "        result = subprocess.run(['ollama', 'pull', 'gemma3:1b'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        print(\"Gemma model pulled successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run Ollama server in background\n",
    "def start_ollama_server():\n",
    "    \"\"\"Start Ollama server\"\"\"\n",
    "    try:\n",
    "        subprocess.Popen(['ollama', 'serve'], shell=True)\n",
    "        time.sleep(5)  # Wait for server to start\n",
    "        print(\"Ollama server started on http://localhost:11434\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting server: {e}\")\n",
    "        return False\n",
    "\n",
    "# Setup the environment\n",
    "# setup_gemma()\n",
    "start_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b8c87",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Create RESTful API Wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API server started on http://localhost:5000\n",
      "Available endpoints:\n",
      "- POST /api/generate - Generate text\n",
      "- GET /api/models - List available models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.50.43:5000\n",
      "\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.50.43:5000\n",
      "Press CTRL+C to quitPress CTRL+C to quit\n",
      "\n",
      "127.0.0.1 - - [30/May/2025 23:14:01] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/May/2025 23:14:01] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/May/2025 23:14:28] \"GET /api/models HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/May/2025 23:14:28] \"GET /api/models HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/May/2025 23:14:54] \"127.0.0.1 - - [30/May/2025 23:14:54] \"GET /api/health HTTP/1.1GET /api/health HTTP/1.1\" 404 -\n",
      "\" 404 -\n",
      "127.0.0.1 - - [30/May/2025 23:15:27] \"POST /api/generate HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [30/May/2025 23:15:27] \"POST /api/generate HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "\n",
    "class OllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)  # Enable CORS for Oxygen XML Editor\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST'])\n",
    "        def generate():\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                prompt = data.get('prompt', '')\n",
    "                # model = data.get('model', 'gemma3:4b')\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                \n",
    "                # Call Ollama API\n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json={\n",
    "                                           'model': model,\n",
    "                                           'prompt': prompt,\n",
    "                                           'stream': False\n",
    "                                       })\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return jsonify({\n",
    "                        'success': True,\n",
    "                        'response': result.get('response', ''),\n",
    "                        'model': model\n",
    "                    })\n",
    "                else:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'Failed to generate response'\n",
    "                    }), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags')\n",
    "                if response.status_code == 200:\n",
    "                    return jsonify(response.json())\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    def run(self, host='localhost', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False)\n",
    "\n",
    "# Create and start the API server\n",
    "api = OllamaAPI()\n",
    "\n",
    "# Run in a separate thread to avoid blocking the notebook\n",
    "def start_api_server():\n",
    "    api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "server_thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"API server started on http://localhost:5000\")\n",
    "print(\"Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text\")\n",
    "print(\"- GET /api/models - List available models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbce15a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Test the API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbfc1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Test Successful!\n",
      "Response: Okay, let's break down machine learning in simple terms.\n",
      "\n",
      "**Imagine you're teaching a dog a new trick.** You don't tell the dog *exactly* how to do it step-by-step. Instead, you show it examples, reward good behavior, and correct mistakes. Eventually, the dog learns to do the trick on its own. \n",
      "\n",
      "**Machine learning is kind of like that, but for computers.** \n",
      "\n",
      "Here’s a simplified explanation:\n",
      "\n",
      "1. **You give the computer a lot of data.** This data could be anything – pictures of cats, customer reviews, sales figures, etc. \n",
      "\n",
      "2. **The computer analyzes this data.** It looks for patterns and trends. It tries to find relationships between the data.\n",
      "\n",
      "3. **The computer learns from these patterns.**  It builds a \"model\" – a simplified representation of the data – that helps it make predictions or decisions about new data.\n",
      "\n",
      "4. **You give the computer a new input.**  Now, it uses its model to make a guess or prediction about what will happen next.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "* **Data:** Like a collection of examples.\n",
      "* **Model:**  Like a set of rules or a recipe.\n",
      "* **Learning:**  Like the computer adjusting the recipe based on the examples it sees.\n",
      "\n",
      "**Common Examples:**\n",
      "\n",
      "* **Spam Filters:** Machine learning analyzes emails to identify spam.\n",
      "* **Recommendation Systems (Netflix, Amazon):** It learns what you like based on your past choices.\n",
      "* **Voice Assistants (Siri, Alexa):** It learns to understand your voice and respond to your commands.\n",
      "* **Self-Driving Cars:** It learns to recognize traffic lights, pedestrians, and other cars.\n",
      "\n",
      "**In essence, machine learning is about teaching computers to learn from data without being explicitly programmed for every single scenario.** \n",
      "\n",
      "**It's not about creating a robot that *always* does exactly what we want.** It's about creating systems that can *improve* over time as they see more data.\n",
      "\n",
      "---\n",
      "\n",
      "**Do you want me to:**\n",
      "\n",
      "*   Give you a specific example?\n",
      "*   Explain a particular type of machine learning (like supervised vs. unsupervised)?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test the API\n",
    "def test_api():\n",
    "    \"\"\"Test the local API\"\"\"\n",
    "    test_data = {\n",
    "        'prompt': 'Explain what machine learning is in simple terms.',\n",
    "        # 'model': 'gemma3:4b'\n",
    "        'model': 'gemma3:1b'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post('http://127.0.0.1:5000/api/generate', \n",
    "                               json=test_data)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"API Test Successful!\")\n",
    "            print(f\"Response: {result['response']}\")\n",
    "        else:\n",
    "            print(f\"API Test Failed: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API Test Error: {e}\")\n",
    "\n",
    "# Wait a moment for server to start, then test\n",
    "time.sleep(3)\n",
    "test_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7742356",
   "metadata": {},
   "source": [
    "When you get \"API Test Failed: 500\", it indicates a server-side error. Here are the most likely causes and how to fix them:\n",
    "\n",
    "## Common Issues Causing HTTP 500:\n",
    "\n",
    "### 1. **Ollama Server Not Running**\n",
    "The most common cause - your Flask API is running but can't connect to Ollama:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c287e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Checking if Ollama server is running...\n",
      "✅ Ollama server is running\n",
      "Available models: ['gemma3:1b', 'gemma:2b', 'gemma3:4b']\n",
      "\n",
      "2. Checking Flask API...\n",
      "❌ Flask API error: HTTPConnectionPool(host='localhost', port=5000): Max retries exceeded with url: /api/models (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000217ED451D30>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n",
      "\n",
      "3. Testing direct Ollama API...\n",
      "Direct Ollama test: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def diagnose_api_error():\n",
    "    \"\"\"Diagnose API 500 errors\"\"\"\n",
    "    \n",
    "    # Check if Ollama server is running\n",
    "    print(\"1. Checking if Ollama server is running...\")\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Ollama server is running\")\n",
    "            models = response.json().get('models', [])\n",
    "            print(f\"Available models: {[m['name'] for m in models]}\")\n",
    "        else:\n",
    "            print(\"❌ Ollama server returned error:\", response.status_code)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Cannot connect to Ollama server on localhost:11434\")\n",
    "        print(\"Starting Ollama server...\")\n",
    "        subprocess.Popen(['ollama', 'serve'], shell=True)\n",
    "        time.sleep(10)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking Ollama: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if Flask API is running\n",
    "    print(\"\\n2. Checking Flask API...\")\n",
    "    try:\n",
    "        response = requests.get('http://localhost:5000/api/models', timeout=5)\n",
    "        print(f\"Flask API status: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Flask API error: {e}\")\n",
    "    \n",
    "    # Test with a simple model request\n",
    "    print(\"\\n3. Testing direct Ollama API...\")\n",
    "    try:\n",
    "        test_request = {\n",
    "            'model': 'gemma3:1b',\n",
    "            'prompt': 'Hello',\n",
    "            'stream': False\n",
    "        }\n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json=test_request, timeout=30)\n",
    "        print(f\"Direct Ollama test: {response.status_code}\")\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Direct Ollama test failed: {e}\")\n",
    "\n",
    "# Run diagnosis\n",
    "diagnose_api_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0757ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. **Model Not Downloaded**\n",
    "Gemma3:4b model might not be pulled yet:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb13deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if gemma3:1b model is available...\n",
      "✅ gemma3:1b model is available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensure_model_available():\n",
    "    \"\"\"Ensure the Gemma model is available\"\"\"\n",
    "    print(\"Checking if gemma3:1b model is available...\")\n",
    "    \n",
    "    try:\n",
    "        # List available models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if 'gemma3:1b' in result.stdout:\n",
    "            print(\"✅ gemma3:1b model is available\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ gemma3:4b model not found\")\n",
    "            print(\"Available models:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            print(\"Pulling gemma3:4b model (this may take several minutes)...\")\n",
    "            pull_result = subprocess.run(['ollama', 'pull', 'gemma3:4b'], \n",
    "                                       capture_output=True, text=True, shell=True)\n",
    "            \n",
    "            if pull_result.returncode == 0:\n",
    "                print(\"✅ Model pulled successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ Failed to pull model\")\n",
    "                print(pull_result.stderr)\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking models: {e}\")\n",
    "        return False\n",
    "\n",
    "ensure_model_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac66150",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. **Enhanced Error Handling in Flask API**\n",
    "Add better error handling to see exactly what's failing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "672f65e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced API server started on http://localhost:5000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.50.43:5000\n",
      "Press CTRL+C to quit\n",
      "Exception in thread Thread-8 (start_api_server):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Temp\\ipykernel_13520\\3509073467.py\", line 98, in start_api_server\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Temp\\ipykernel_13520\\3509073467.py\", line 92, in run\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\app.py\", line 662, in run\n",
      "    run_simple(t.cast(str, host), port, self, **options)\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\serving.py\", line 1115, in run_simple\n",
      "    run_with_reloader(\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\_reloader.py\", line 452, in run_with_reloader\n",
      "    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: signal only works in main thread of the main interpreter\n",
      "\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.50.43:5000\n",
      "Press CTRL+C to quit\n",
      "Exception in thread Thread-8 (start_api_server):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Temp\\ipykernel_13520\\3509073467.py\", line 98, in start_api_server\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Temp\\ipykernel_13520\\3509073467.py\", line 92, in run\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\app.py\", line 662, in run\n",
      "    run_simple(t.cast(str, host), port, self, **options)\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\serving.py\", line 1115, in run_simple\n",
      "    run_with_reloader(\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\_reloader.py\", line 452, in run_with_reloader\n",
      "    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\signal.py\", line 58, in signal\n",
      "    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: signal only works in main thread of the main interpreter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Health check failed: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "class OllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST'])\n",
    "        def generate():\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'success': False, 'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                prompt = data.get('prompt', '')\n",
    "                model = data.get('model', 'gemma3:4b')\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'success': False, 'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"Generating response for prompt: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection first\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'success': False, \n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'success': False, \n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API\n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json={\n",
    "                                           'model': model,\n",
    "                                           'prompt': prompt,\n",
    "                                           'stream': False\n",
    "                                       }, timeout=60)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return jsonify({\n",
    "                        'success': True,\n",
    "                        'response': result.get('response', ''),\n",
    "                        'model': model\n",
    "                    })\n",
    "                else:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    }), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'traceback': error_traceback\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            try:\n",
    "                # Check Ollama connection\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                return jsonify({\n",
    "                    'flask_api': 'running',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'ollama_status_code': ollama_response.status_code\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'flask_api': 'running',\n",
    "                    'ollama_server': 'not_reachable',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    def run(self, host='localhost', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=True)\n",
    "\n",
    "# Create and start the enhanced API server\n",
    "api = OllamaAPI()\n",
    "\n",
    "def start_api_server():\n",
    "    api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "server_thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"Enhanced API server started on http://localhost:5000\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Test health endpoint\n",
    "try:\n",
    "    health_response = requests.get('http://localhost:5000/api/health')\n",
    "    print(f\"Health check: {health_response.json()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc109e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. **Updated Test Function**\n",
    "Use a more robust test function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a737a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API with enhanced error reporting...\n",
      "Sending request to Flask API...\n",
      "Response status code: 200\n",
      "✅ API Test Successful!\n",
      "Response: 2 + 2 = 4\n",
      "\n",
      "Response status code: 200\n",
      "✅ API Test Successful!\n",
      "Response: 2 + 2 = 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_api_enhanced():\n",
    "    \"\"\"Enhanced API test with better error reporting\"\"\"\n",
    "    print(\"Testing API with enhanced error reporting...\")\n",
    "    \n",
    "    test_data = {\n",
    "        'prompt': 'What is 2+2?',\n",
    "        'model': 'gemma3:1b'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Sending request to Flask API...\")\n",
    "        response = requests.post('http://127.0.0.1:5000/api/generate', \n",
    "                               json=test_data, timeout=60)\n",
    "        \n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"✅ API Test Successful!\")\n",
    "            print(f\"Response: {result.get('response', 'No response')}\")\n",
    "        else:\n",
    "            print(f\"❌ API Test Failed: {response.status_code}\")\n",
    "            try:\n",
    "                error_detail = response.json()\n",
    "                print(f\"Error details: {error_detail}\")\n",
    "            except:\n",
    "                print(f\"Raw response: {response.text}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"❌ Request timed out - model might be loading\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ API Test Error: {e}\")\n",
    "\n",
    "# Wait for server to start, then test\n",
    "time.sleep(5)\n",
    "test_api_enhanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5782c22",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run these diagnostic functions to identify the exact cause of the 500 error. The most common fix is ensuring Ollama server is running and the model is downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c793751",
   "metadata": {},
   "source": [
    "Collecting workspace informationYou can check if the Gemma model is successfully pulled from Ollama using several methods. Here are the most effective ways:\n",
    "\n",
    "## 1. **Check Available Models with `ollama list`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7787e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully retrieved model list:\n",
      "NAME         ID              SIZE      MODIFIED    \n",
      "gemma3:1b    8648f39daa8f    815 MB    5 hours ago    \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    9 hours ago    \n",
      "\n",
      "✅ gemma3:1b model is available!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_pulled_models():\n",
    "    \"\"\"Check all models pulled by Ollama\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Successfully retrieved model list:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Check specifically for gemma3:1b\n",
    "            if 'gemma3:1b' in result.stdout:\n",
    "                print(\"✅ gemma3:1b model is available!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ gemma3:1b model not found in the list\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"❌ Error running ollama list: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking models: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check models\n",
    "check_pulled_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb5b7b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Use Ollama API to List Models**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effa57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Available models via API:\n",
      "  - gemma3:1b (Size: 815,319,791 bytes, Modified: 2025-05-31T01:51:33.6340204+08:00)\n",
      "  - gemma3:4b (Size: 3,338,801,804 bytes, Modified: 2025-05-30T22:20:59.6846172+08:00)\n",
      "✅ gemma3:4b model found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_models_via_api():\n",
    "    \"\"\"Check models using Ollama's REST API\"\"\"\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            models_data = response.json()\n",
    "            models = models_data.get('models', [])\n",
    "            \n",
    "            print(\"✅ Available models via API:\")\n",
    "            for model in models:\n",
    "                name = model.get('name', 'Unknown')\n",
    "                size = model.get('size', 0)\n",
    "                modified = model.get('modified_at', 'Unknown')\n",
    "                print(f\"  - {name} (Size: {size:,} bytes, Modified: {modified})\")\n",
    "            \n",
    "            # Check for gemma3:4b specifically\n",
    "            gemma_models = [m for m in models if 'gemma3:4b' in m.get('name', '')]\n",
    "            if gemma_models:\n",
    "                print(\"✅ gemma3:4b model found!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ gemma3:4b model not found\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"❌ API request failed: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"❌ Cannot connect to Ollama server. Make sure it's running.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error checking models via API: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check via API\n",
    "check_models_via_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d64e0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Test Model with Simple Generation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a18cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model generation...\n",
      "✅ Model generation test successful!\n",
      "Prompt: Hello, say hi back to me.\n",
      "Response: Hello to you too! 😊 How can I help you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_generation():\n",
    "    \"\"\"Test if the model can actually generate text\"\"\"\n",
    "    try:\n",
    "        test_request = {\n",
    "            # 'model': 'gemma3:4b',\n",
    "            'model': 'gemma3:1b',\n",
    "            'prompt': 'Hello, say hi back to me.',\n",
    "            'stream': False\n",
    "        }\n",
    "        \n",
    "        print(\"Testing model generation...\")\n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json=test_request, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            generated_text = result.get('response', '')\n",
    "            print(\"✅ Model generation test successful!\")\n",
    "            print(f\"Prompt: {test_request['prompt']}\")\n",
    "            print(f\"Response: {generated_text}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Model generation failed: {response.status_code}\")\n",
    "            print(f\"Error: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error testing model generation: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test generation\n",
    "test_model_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213053d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Complete Model Status Check Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62034c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "COMPLETE GEMMA3:4B MODEL STATUS CHECK\n",
      "==================================================\n",
      "\n",
      "1. Checking Ollama server...\n",
      "✅ Ollama server is running\n",
      "\n",
      "2. Checking models via command line...\n",
      "✅ Successfully retrieved model list:\n",
      "NAME         ID              SIZE      MODIFIED    \n",
      "gemma3:1b    8648f39daa8f    815 MB    5 hours ago    \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    9 hours ago    \n",
      "\n",
      "✅ gemma3:1b model is available!\n",
      "\n",
      "3. Checking models via API...\n",
      "✅ Available models via API:\n",
      "  - gemma3:1b (Size: 815,319,791 bytes, Modified: 2025-05-31T01:51:33.6340204+08:00)\n",
      "  - gemma3:4b (Size: 3,338,801,804 bytes, Modified: 2025-05-30T22:20:59.6846172+08:00)\n",
      "✅ gemma3:4b model found!\n",
      "\n",
      "4. Testing model generation...\n",
      "Testing model generation...\n",
      "✅ Model generation test successful!\n",
      "Prompt: Hello, say hi back to me.\n",
      "Response: Hello there! 👋 How can I help you today?\n",
      "\n",
      "==================================================\n",
      "SUMMARY:\n",
      "Command line check: ✅ PASS\n",
      "API check: ✅ PASS\n",
      "Generation test: ✅ PASS\n",
      "🎉 gemma3:1b model is fully functional!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complete_model_check():\n",
    "    \"\"\"Complete check for gemma3:4b model availability and functionality\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"COMPLETE GEMMA3:4B MODEL STATUS CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Check if Ollama is running\n",
    "    print(\"\\n1. Checking Ollama server...\")\n",
    "    try:\n",
    "        health = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        if health.status_code == 200:\n",
    "            print(\"✅ Ollama server is running\")\n",
    "        else:\n",
    "            print(\"❌ Ollama server error\")\n",
    "            return False\n",
    "    except:\n",
    "        print(\"❌ Ollama server not reachable\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Check model list via command\n",
    "    print(\"\\n2. Checking models via command line...\")\n",
    "    cmd_result = check_pulled_models()\n",
    "    \n",
    "    # Step 3: Check model list via API\n",
    "    print(\"\\n3. Checking models via API...\")\n",
    "    api_result = check_models_via_api()\n",
    "    \n",
    "    # Step 4: Test actual generation\n",
    "    print(\"\\n4. Testing model generation...\")\n",
    "    gen_result = test_model_generation()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"Command line check: {'✅ PASS' if cmd_result else '❌ FAIL'}\")\n",
    "    print(f\"API check: {'✅ PASS' if api_result else '❌ FAIL'}\")\n",
    "    print(f\"Generation test: {'✅ PASS' if gen_result else '❌ FAIL'}\")\n",
    "    \n",
    "    if all([cmd_result, api_result, gen_result]):\n",
    "        print(\"🎉 gemma3:1b model is fully functional!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️ There are issues with the gemma3:1b model\")\n",
    "        return False\n",
    "\n",
    "# Run complete check\n",
    "complete_model_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9270a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **If Model is Not Found, Pull It**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_gemma_model():\n",
    "    \"\"\"Ensure gemma3:4b model is available, pull if necessary\"\"\"\n",
    "    if not complete_model_check():\n",
    "        print(\"\\n🔄 Model not found or not working. Attempting to pull...\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Pulling gemma3:4b model (this may take several minutes)...\")\n",
    "            pull_process = subprocess.Popen(\n",
    "                ['ollama', 'pull', 'gemma3:4b'],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                shell=True\n",
    "            )\n",
    "            \n",
    "            # Show progress\n",
    "            while True:\n",
    "                output = pull_process.stdout.readline()\n",
    "                if output == '' and pull_process.poll() is not None:\n",
    "                    break\n",
    "                if output:\n",
    "                    print(output.strip())\n",
    "            \n",
    "            if pull_process.returncode == 0:\n",
    "                print(\"✅ Model pulled successfully!\")\n",
    "                # Check again\n",
    "                return complete_model_check()\n",
    "            else:\n",
    "                print(\"❌ Failed to pull model\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error pulling model: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"✅ Model is already available and working!\")\n",
    "        return True\n",
    "\n",
    "# Ensure model is available\n",
    "ensure_gemma_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf15ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run these functions in your localLlmWIntelGPU.ipynb notebook to verify that the Gemma model is successfully pulled and working. The complete check will tell you exactly what's working and what needs to be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ddf5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Oxygen XML Editor Integration\n",
    "\n",
    "For Oxygen XML Editor, you can create a connector using the REST API:\n",
    "\n",
    "**Connector Configuration:**\n",
    "- **URL**: `http://localhost:5000/api/generate`\n",
    "- **Method**: POST\n",
    "- **Content-Type**: application/json\n",
    "- **Request Body**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa264b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"prompt\": \"${prompt}\",\n",
    "    \"model\": \"gemma:2b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84544a1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Usage in Oxygen XML Editor:**\n",
    "1. Go to **Options** > **Preferences** > **External Tools**\n",
    "2. Create a new external tool with the API endpoint\n",
    "3. Configure parameters to pass selected text as prompt\n",
    "4. Set up response handling to insert generated content\n",
    "\n",
    "## 6. Enhanced API with Intel GPU Support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26941f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Intel GPU optimization, ensure you have Intel Extension for PyTorch\n",
    "def setup_intel_gpu():\n",
    "    \"\"\"Setup Intel GPU acceleration for better performance\"\"\"\n",
    "    try:\n",
    "        import intel_extension_for_pytorch as ipex\n",
    "        print(\"Intel Extension for PyTorch available\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Installing Intel Extension for PyTorch...\")\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'intel_extension_for_pytorch'])\n",
    "        return False\n",
    "\n",
    "setup_intel_gpu()\n",
    "\n",
    "# Enhanced API endpoint with performance monitoring\n",
    "@api.app.route('/api/generate/enhanced', methods=['POST'])\n",
    "def generate_enhanced():\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        prompt = data.get('prompt', '')\n",
    "        model = data.get('model', 'gemma:2b')\n",
    "        max_tokens = data.get('max_tokens', 100)\n",
    "        \n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json={\n",
    "                                   'model': model,\n",
    "                                   'prompt': prompt,\n",
    "                                   'stream': False,\n",
    "                                   'options': {\n",
    "                                       'num_predict': max_tokens\n",
    "                                   }\n",
    "                               })\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return jsonify({\n",
    "                'success': True,\n",
    "                'response': result.get('response', ''),\n",
    "                'model': model,\n",
    "                'processing_time': processing_time,\n",
    "                'tokens_generated': len(result.get('response', '').split())\n",
    "            })\n",
    "        else:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'error': 'Failed to generate response',\n",
    "                'processing_time': processing_time\n",
    "            }), 500\n",
    "            \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }), 500\n",
    "\n",
    "print(\"Enhanced API endpoint added: POST /api/generate/enhanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaadf8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This setup provides a complete solution for running Ollama with Gemma model locally and exposing it via REST API for Oxygen XML Editor integration. The API runs on port 5000 and provides endpoints for text generation and model management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680eb107",
   "metadata": {},
   "source": [
    "You can remove models from Ollama using several methods. Here are the most effective ways:\n",
    "\n",
    "## 1. **Using Ollama Command Line**\n",
    "\n",
    "The simplest method is using the `ollama rm` command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6722ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing model: gemma:2b\n",
      "✅ Model gemma:2b removed successfully!\n",
      "deleted 'gemma:2b'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def remove_ollama_model(model_name):\n",
    "    \"\"\"Remove a specific model from Ollama\"\"\"\n",
    "    try:\n",
    "        print(f\"Removing model: {model_name}\")\n",
    "        result = subprocess.run(['ollama', 'rm', model_name], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Model {model_name} removed successfully!\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Failed to remove model {model_name}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error removing model: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example: Remove gemma3:4b model\n",
    "remove_ollama_model('gemma:2b')\n",
    "\n",
    "# # Example: Remove gemma3:1b model\n",
    "# remove_ollama_model('gemma3:1b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03073e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **List and Remove Models Interactively**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_and_remove_models():\n",
    "    \"\"\"List available models and allow interactive removal\"\"\"\n",
    "    try:\n",
    "        # First, list all available models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"Available models:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Parse model names from output\n",
    "            lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n",
    "            models = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    model_name = line.split()[0]\n",
    "                    models.append(model_name)\n",
    "            \n",
    "            if not models:\n",
    "                print(\"No models found to remove.\")\n",
    "                return\n",
    "            \n",
    "            print(\"\\nModels available for removal:\")\n",
    "            for i, model in enumerate(models, 1):\n",
    "                print(f\"{i}. {model}\")\n",
    "            \n",
    "            # You can modify this to remove specific models\n",
    "            # For example, remove all gemma models:\n",
    "            gemma_models = [m for m in models if 'gemma' in m.lower()]\n",
    "            \n",
    "            if gemma_models:\n",
    "                print(f\"\\nFound Gemma models: {gemma_models}\")\n",
    "                for model in gemma_models:\n",
    "                    remove_ollama_model(model)\n",
    "            else:\n",
    "                print(\"No Gemma models found.\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"❌ Error listing models: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Run the interactive removal\n",
    "list_and_remove_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44687b8e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Remove All Models (Clean Slate)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_models():\n",
    "    \"\"\"Remove all models from Ollama\"\"\"\n",
    "    try:\n",
    "        # Get list of all models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n",
    "            models = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    model_name = line.split()[0]\n",
    "                    models.append(model_name)\n",
    "            \n",
    "            if not models:\n",
    "                print(\"No models found to remove.\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(models)} models to remove:\")\n",
    "            for model in models:\n",
    "                print(f\"  - {model}\")\n",
    "            \n",
    "            # Remove each model\n",
    "            for model in models:\n",
    "                print(f\"\\nRemoving {model}...\")\n",
    "                remove_result = subprocess.run(['ollama', 'rm', model], \n",
    "                                             capture_output=True, text=True, shell=True)\n",
    "                \n",
    "                if remove_result.returncode == 0:\n",
    "                    print(f\"✅ {model} removed successfully\")\n",
    "                else:\n",
    "                    print(f\"❌ Failed to remove {model}: {remove_result.stderr}\")\n",
    "            \n",
    "            print(\"\\n✅ All models removal process completed!\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ Error listing models: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Uncomment to remove all models (use with caution!)\n",
    "# remove_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6a6ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Check Storage Space After Removal**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67090f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Ollama storage usage:\n",
      "📁 C:\\Users\\jeffw/.ollama: 3.87 GB\n",
      "📁 C:\\Users\\jeffw/AppData/Local/ollama: 0.00 GB\n",
      "📁 C:\\Users\\jeffw/AppData/Roaming/ollama: Not found\n",
      "📁 C:\\Users\\jeffw\\AppData\\Local\\ollama: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def check_ollama_storage():\n",
    "    \"\"\"Check storage used by Ollama after removal\"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    \n",
    "    # Common Ollama data directories\n",
    "    possible_paths = [\n",
    "        os.path.expanduser(\"~/.ollama\"),\n",
    "        os.path.expanduser(\"~/AppData/Local/ollama\"),\n",
    "        os.path.expanduser(\"~/AppData/Roaming/ollama\"),\n",
    "        \"C:\\\\Users\\\\{}\\\\AppData\\\\Local\\\\ollama\".format(os.getenv('USERNAME'))\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking Ollama storage usage:\")\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                total_size = 0\n",
    "                for dirpath, dirnames, filenames in os.walk(path):\n",
    "                    for filename in filenames:\n",
    "                        filepath = os.path.join(dirpath, filename)\n",
    "                        total_size += os.path.getsize(filepath)\n",
    "                \n",
    "                size_gb = total_size / (1024**3)\n",
    "                print(f\"📁 {path}: {size_gb:.2f} GB\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error checking {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"📁 {path}: Not found\")\n",
    "\n",
    "# Check storage usage\n",
    "check_ollama_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc1ec8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Complete Cleanup Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_ollama_cleanup():\n",
    "    \"\"\"Complete cleanup of Ollama models and verification\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"COMPLETE OLLAMA CLEANUP\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: List current models\n",
    "    print(\"\\n1. Current models:\")\n",
    "    list_result = subprocess.run(['ollama', 'list'], \n",
    "                                capture_output=True, text=True, shell=True)\n",
    "    if list_result.returncode == 0:\n",
    "        print(list_result.stdout)\n",
    "    else:\n",
    "        print(\"No models or error listing models\")\n",
    "    \n",
    "    # Step 2: Remove all models\n",
    "    print(\"\\n2. Removing all models...\")\n",
    "    remove_all_models()\n",
    "    \n",
    "    # Step 3: Verify removal\n",
    "    print(\"\\n3. Verifying removal...\")\n",
    "    verify_result = subprocess.run(['ollama', 'list'], \n",
    "                                  capture_output=True, text=True, shell=True)\n",
    "    if verify_result.returncode == 0:\n",
    "        lines = verify_result.stdout.strip().split('\\n')[1:]\n",
    "        remaining_models = [line for line in lines if line.strip()]\n",
    "        \n",
    "        if not remaining_models:\n",
    "            print(\"✅ All models successfully removed!\")\n",
    "        else:\n",
    "            print(f\"⚠️ {len(remaining_models)} models still present:\")\n",
    "            print(verify_result.stdout)\n",
    "    \n",
    "    # Step 4: Check storage\n",
    "    print(\"\\n4. Storage check after cleanup:\")\n",
    "    check_ollama_storage()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CLEANUP COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Run complete cleanup (uncomment to use)\n",
    "# complete_ollama_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62be7d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Quick Commands Summary:\n",
    "\n",
    "**Remove specific model:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e9575",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama rm gemma3:4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8557b31",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Remove multiple models:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama rm gemma3:4b gemma3:1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22923d3c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**List models before removal:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aef46a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use these functions to safely remove models from Ollama. The `complete_ollama_cleanup()` function will remove all models if you want to start fresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853d0e1",
   "metadata": {},
   "source": [
    "Based on your Jupyter notebook and the Oxygen AI Positron documentation, here are the necessary steps to integrate your Ollama + Gemma3:1b RESTful service with Oxygen XML Editor:\n",
    "\n",
    "## 1. **Ensure Your Ollama RESTful Service is Running**\n",
    "\n",
    "First, make sure your Flask API is running from your notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your API is running\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def verify_ollama_service():\n",
    "    \"\"\"Verify both Ollama and Flask API are running\"\"\"\n",
    "    try:\n",
    "        # Check Ollama server\n",
    "        ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        print(f\"✅ Ollama server: {ollama_response.status_code}\")\n",
    "        \n",
    "        # Check Flask API\n",
    "        flask_response = requests.get('http://localhost:5000/api/health', timeout=5)\n",
    "        print(f\"✅ Flask API: {flask_response.status_code}\")\n",
    "        \n",
    "        # Test generation with gemma3:1b\n",
    "        test_data = {\n",
    "            'prompt': 'Hello, this is a test.',\n",
    "            'model': 'gemma3:1b'\n",
    "        }\n",
    "        \n",
    "        gen_response = requests.post('http://localhost:5000/api/generate', \n",
    "                                   json=test_data, timeout=30)\n",
    "        \n",
    "        if gen_response.status_code == 200:\n",
    "            result = gen_response.json()\n",
    "            print(\"✅ Generation test successful!\")\n",
    "            print(f\"Response: {result.get('response', '')[:]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Generation test failed: {gen_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Service verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "verify_ollama_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561d949",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Create Oxygen AI Positron Custom Connector Configuration**\n",
    "\n",
    "Create a JSON configuration file for your custom connector:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6deb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"Local Ollama Gemma3:1b\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model for text generation\",\n",
    "  \"endpoint\": \"http://localhost:5000/api/generate\",\n",
    "  \"method\": \"POST\",\n",
    "  \"headers\": {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\"\n",
    "  },\n",
    "  \"requestBody\": {\n",
    "    \"prompt\": \"${input}\",\n",
    "    \"model\": \"gemma3:1b\",\n",
    "    \"max_tokens\": 1000\n",
    "  },\n",
    "  \"responseMapping\": {\n",
    "    \"textPath\": \"$.response\",\n",
    "    \"errorPath\": \"$.error\"\n",
    "  },\n",
    "  \"timeout\": 60000,\n",
    "  \"streaming\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834438fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Install and Configure Oxygen AI Positron Add-on**\n",
    "\n",
    "### Step 3.1: Install the Add-on\n",
    "1. Open Oxygen XML Editor\n",
    "2. Go to **Help** > **Install new add-ons**\n",
    "3. Add the update site: `https://www.oxygenxml.com/InstData/Addons/default/updateSite.xml`\n",
    "4. Search for \"AI Positron\" and install it\n",
    "5. Restart Oxygen XML Editor\n",
    "\n",
    "### Step 3.2: Configure the Custom Connector\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Click **Add Custom Connector**\n",
    "3. Provide the configuration details:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d407ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"connectorName\": \"Local Ollama Gemma3:1b\",\n",
    "  \"baseURL\": \"http://localhost:5000\",\n",
    "  \"apiKey\": \"\",\n",
    "  \"requestConfig\": {\n",
    "    \"endpoint\": \"/api/generate\",\n",
    "    \"method\": \"POST\",\n",
    "    \"headers\": {\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    \"bodyTemplate\": {\n",
    "      \"prompt\": \"${prompt}\",\n",
    "      \"model\": \"gemma3:1b\"\n",
    "    }\n",
    "  },\n",
    "  \"responseConfig\": {\n",
    "    \"textFieldPath\": \"response\",\n",
    "    \"errorFieldPath\": \"error\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181868d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Enhanced Flask API for Oxygen Integration**\n",
    "\n",
    "Update your Flask API to be more compatible with Oxygen AI Positron:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f036cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "class OxygenOllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)  # Enable CORS for Oxygen XML Editor\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST', 'OPTIONS'])\n",
    "        def generate():\n",
    "            # Handle preflight requests\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "            \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Handle both 'prompt' and 'input' fields for compatibility\n",
    "                prompt = data.get('prompt') or data.get('input', '')\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                max_tokens = data.get('max_tokens', 500)\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"🤖 Processing request: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': max_tokens,\n",
    "                        'temperature': 0.7\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # Format response for Oxygen AI Positron\n",
    "                    oxygen_response = {\n",
    "                        'response': generated_text,\n",
    "                        'model': model,\n",
    "                        'success': True,\n",
    "                        'metadata': {\n",
    "                            'tokens_generated': len(generated_text.split()),\n",
    "                            'model_used': model\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"✅ Generated {len(generated_text)} characters\")\n",
    "                    return jsonify(oxygen_response)\n",
    "                else:\n",
    "                    error_msg = f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    print(f\"❌ {error_msg}\")\n",
    "                    return jsonify({'error': error_msg}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"❌ Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'success': False\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint for Oxygen\"\"\"\n",
    "            try:\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                models = ollama_response.json().get('models', []) if ollama_response.status_code == 200 else []\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'healthy',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'available_models': [m.get('name', '') for m in models],\n",
    "                    'flask_api': 'running'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            \"\"\"List available models for Oxygen configuration\"\"\"\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    models_data = response.json()\n",
    "                    models = []\n",
    "                    for model in models_data.get('models', []):\n",
    "                        models.append({\n",
    "                            'name': model.get('name', ''),\n",
    "                            'size': model.get('size', 0),\n",
    "                            'modified': model.get('modified_at', '')\n",
    "                        })\n",
    "                    return jsonify({'models': models})\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False, threaded=True)\n",
    "\n",
    "# Create and start the enhanced API server for Oxygen\n",
    "oxygen_api = OxygenOllamaAPI()\n",
    "\n",
    "def start_oxygen_api_server():\n",
    "    oxygen_api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Stop previous server if running\n",
    "try:\n",
    "    requests.get('http://localhost:5000/api/health', timeout=1)\n",
    "    print(\"⚠️ Previous server detected. Starting new server...\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "server_thread = threading.Thread(target=start_oxygen_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 Oxygen-compatible API server started on http://localhost:5000\")\n",
    "print(\"Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text (Oxygen AI Positron endpoint)\")\n",
    "print(\"- GET /api/health - Health check\")\n",
    "print(\"- GET /api/models - List available models\")\n",
    "\n",
    "# Test the enhanced API\n",
    "time.sleep(3)\n",
    "test_data = {\n",
    "    'input': 'Write a brief summary about XML editing.',\n",
    "    'model': 'gemma3:1b'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post('http://localhost:5000/api/generate', json=test_data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\n✅ Test successful!\")\n",
    "        print(f\"Response: {result.get('response', '')[:200]}...\")\n",
    "    else:\n",
    "        print(f\"❌ Test failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666f791",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Configure Oxygen AI Positron Settings**\n",
    "\n",
    "In Oxygen XML Editor:\n",
    "\n",
    "### Step 5.1: Basic Configuration\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Set the following:\n",
    "   - **Provider**: Custom\n",
    "   - **Base URL**: `http://localhost:5000`\n",
    "   - **API Key**: Leave empty (not needed for local server)\n",
    "\n",
    "### Step 5.2: Advanced Configuration\n",
    "Create a connector configuration file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"local-ollama-gemma\",\n",
    "  \"name\": \"Local Ollama Gemma3:1b\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model\",\n",
    "  \"type\": \"custom\",\n",
    "  \"config\": {\n",
    "    \"baseUrl\": \"http://localhost:5000\",\n",
    "    \"endpoints\": {\n",
    "      \"generate\": {\n",
    "        \"path\": \"/api/generate\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\n",
    "          \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        \"requestBodyTemplate\": {\n",
    "          \"input\": \"${prompt}\",\n",
    "          \"model\": \"gemma3:1b\",\n",
    "          \"max_tokens\": 1000\n",
    "        },\n",
    "        \"responseMapping\": {\n",
    "          \"textPath\": \"$.response\",\n",
    "          \"errorPath\": \"$.error\"\n",
    "        }\n",
    "      },\n",
    "      \"health\": {\n",
    "        \"path\": \"/api/health\",\n",
    "        \"method\": \"GET\"\n",
    "      }\n",
    "    },\n",
    "    \"timeout\": 60000,\n",
    "    \"retryAttempts\": 2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec0bbc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. **Create Custom Actions in Oxygen**\n",
    "\n",
    "### Step 6.1: Document Actions\n",
    "1. Go to **Options** > **Menu Shortcut Keys**\n",
    "2. Create new actions for common AI tasks:\n",
    "\n",
    "#### Action 1: Summarize Selected Text\n",
    "- **Name**: \"AI Summarize\"\n",
    "- **Shortcut**: `Ctrl+Alt+S`\n",
    "- **Operation**: Call AI Positron with prompt: \"Summarize the following text: ${selection}\"\n",
    "\n",
    "#### Action 2: Improve Writing\n",
    "- **Name**: \"AI Improve Writing\"\n",
    "- **Shortcut**: `Ctrl+Alt+I`  \n",
    "- **Operation**: Call AI Positron with prompt: \"Improve the writing and clarity of: ${selection}\"\n",
    "\n",
    "#### Action 3: Generate Documentation\n",
    "- **Name**: \"AI Generate Docs\"\n",
    "- **Shortcut**: `Ctrl+Alt+D`\n",
    "- **Operation**: Call AI Positron with prompt: \"Generate documentation for: ${selection}\"\n",
    "\n",
    "### Step 6.2: Create Custom Framework\n",
    "Create a custom framework file `oxygen-ai-actions.framework`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dac8b",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<serialized version=\"24.1\" xml:space=\"preserve\">\n",
    "    <serializableOrderedMap>\n",
    "        <entry>\n",
    "            <String>document.types</String>\n",
    "            <documentTypeDescriptor-array>\n",
    "                <documentTypeDescriptor>\n",
    "                    <field name=\"name\">\n",
    "                        <String>AI Enhanced XML</String>\n",
    "                    </field>\n",
    "                    <field name=\"description\">\n",
    "                        <String>XML editing with AI assistance</String>\n",
    "                    </field>\n",
    "                    <field name=\"priority\">\n",
    "                        <Integer>3</Integer>\n",
    "                    </field>\n",
    "                    <field name=\"authorActions\">\n",
    "                        <authorAction-array>\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.summarize</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Summarize</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Summarize selected text using AI</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ai.positron.generate</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Summarize this text: ${selection}</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                        </authorAction-array>\n",
    "                    </field>\n",
    "                </documentTypeDescriptor>\n",
    "            </documentTypeDescriptor-array>\n",
    "        </entry>\n",
    "    </serializableOrderedMap>\n",
    "</serialized>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee6124",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. **Test the Integration**\n",
    "\n",
    "### Step 7.1: Basic Test\n",
    "1. Open an XML document in Oxygen\n",
    "2. Select some text\n",
    "3. Use **AI Positron** > **Generate** or your custom action\n",
    "4. Verify the response is inserted correctly\n",
    "\n",
    "### Step 7.2: Advanced Test Script\n",
    "Create a test validation script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bafe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_oxygen_integration():\n",
    "    \"\"\"Test the integration with various prompts\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple generation\",\n",
    "            \"prompt\": \"What is XML?\",\n",
    "            \"expected_length\": 50\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Code explanation\",\n",
    "            \"prompt\": \"Explain this XML: <book><title>Example</title></book>\",\n",
    "            \"expected_length\": 100\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Documentation generation\",\n",
    "            \"prompt\": \"Generate documentation for an XML schema\",\n",
    "            \"expected_length\": 200\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 Testing Oxygen AI Positron Integration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test['name']}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.post('http://localhost:5000/api/generate', \n",
    "                                   json={\n",
    "                                       'input': test['prompt'],\n",
    "                                       'model': 'gemma3:1b'\n",
    "                                   }, \n",
    "                                   timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                generated_text = result.get('response', '')\n",
    "                \n",
    "                if len(generated_text) >= test['expected_length']:\n",
    "                    print(f\"✅ PASS - Generated {len(generated_text)} characters\")\n",
    "                    print(f\"Preview: {generated_text[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"⚠️ PARTIAL - Short response: {len(generated_text)} chars\")\n",
    "            else:\n",
    "                print(f\"❌ FAIL - HTTP {response.status_code}\")\n",
    "                print(response.text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"✅ Integration test completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_oxygen_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5afd37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. **Troubleshooting Common Issues**\n",
    "\n",
    "### Issue 1: CORS Errors\n",
    "If you get CORS errors in Oxygen:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced CORS configuration\n",
    "from flask_cors import CORS\n",
    "\n",
    "# Add this to your Flask app initialization\n",
    "CORS(oxygen_api.app, \n",
    "     origins=['*'],\n",
    "     methods=['GET', 'POST', 'OPTIONS'],\n",
    "     allow_headers=['Content-Type', 'Authorization', 'X-Requested-With'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fad26e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Issue 2: Timeout Issues\n",
    "Increase timeout values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"timeout\": 120000,\n",
    "  \"retryAttempts\": 3,\n",
    "  \"retryDelay\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4581f45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Issue 3: Model Not Found\n",
    "Verify your model is available:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available models\n",
    "def check_models():\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, shell=True)\n",
    "    print(\"Available models:\")\n",
    "    print(result.stdout)\n",
    "\n",
    "check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eef506",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This setup provides a complete integration between your local Ollama Gemma3:1b service and Oxygen XML Editor via the AI Positron add-on, allowing you to use AI assistance directly within your XML editing workflow.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbab512",
   "metadata": {},
   "source": [
    "Based on your Jupyter notebook and the Oxygen AI Positron 5.0 documentation with the sample plugin, here's a comprehensive guide to integrate your Ollama + Gemma3:1b RESTful service with Oxygen XML Editor:\n",
    "\n",
    "## 1. **Prepare Your Ollama RESTful Service for Oxygen Integration**\n",
    "\n",
    "First, enhance your existing Flask API to be fully compatible with Oxygen AI Positron 5.0:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "class OxygenOllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        # Enhanced CORS configuration for Oxygen XML Editor\n",
    "        CORS(self.app, \n",
    "             origins=['*'],\n",
    "             methods=['GET', 'POST', 'OPTIONS', 'PUT', 'DELETE'],\n",
    "             allow_headers=['Content-Type', 'Authorization', 'X-Requested-With', 'Accept'],\n",
    "             expose_headers=['Content-Type', 'Authorization'],\n",
    "             supports_credentials=False)\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST', 'OPTIONS'])\n",
    "        def generate():\n",
    "            # Handle CORS preflight requests\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-Requested-With')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "            \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Handle multiple input field formats for Oxygen compatibility\n",
    "                prompt = (data.get('prompt') or \n",
    "                         data.get('input') or \n",
    "                         data.get('text') or \n",
    "                         data.get('message', ''))\n",
    "                \n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                max_tokens = data.get('max_tokens', 1000)\n",
    "                temperature = data.get('temperature', 0.7)\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"🤖 Processing Oxygen request: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API with proper options\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': max_tokens,\n",
    "                        'temperature': temperature,\n",
    "                        'top_k': 40,\n",
    "                        'top_p': 0.9\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # Format response for Oxygen AI Positron compatibility\n",
    "                    # Following the sample plugin structure\n",
    "                    oxygen_response = {\n",
    "                        'text': generated_text,  # Primary response field for Oxygen\n",
    "                        'response': generated_text,  # Alternative field\n",
    "                        'content': generated_text,  # Another alternative\n",
    "                        'choices': [{  # OpenAI-compatible format\n",
    "                            'text': generated_text,\n",
    "                            'message': {\n",
    "                                'content': generated_text,\n",
    "                                'role': 'assistant'\n",
    "                            }\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'success': True,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'metadata': {\n",
    "                            'tokens_generated': len(generated_text.split()),\n",
    "                            'model_used': model,\n",
    "                            'processing_time': result.get('total_duration', 0)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"✅ Generated {len(generated_text)} characters for Oxygen\")\n",
    "                    return jsonify(oxygen_response)\n",
    "                else:\n",
    "                    error_msg = f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    print(f\"❌ {error_msg}\")\n",
    "                    return jsonify({'error': error_msg}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"❌ Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'success': False\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/chat/completions', methods=['POST', 'OPTIONS'])\n",
    "        def chat_completions():\n",
    "            \"\"\"OpenAI-compatible endpoint for Oxygen AI Positron\"\"\"\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "                \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                messages = data.get('messages', [])\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                \n",
    "                # Extract prompt from messages\n",
    "                if messages:\n",
    "                    prompt = messages[-1].get('content', '')\n",
    "                else:\n",
    "                    prompt = data.get('prompt', '')\n",
    "                \n",
    "                # Use the same generation logic\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': data.get('max_tokens', 1000),\n",
    "                        'temperature': data.get('temperature', 0.7)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # OpenAI-compatible response format for Oxygen\n",
    "                    openai_response = {\n",
    "                        'choices': [{\n",
    "                            'message': {\n",
    "                                'role': 'assistant',\n",
    "                                'content': generated_text\n",
    "                            },\n",
    "                            'finish_reason': 'stop',\n",
    "                            'index': 0\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split()),\n",
    "                            'prompt_tokens': len(prompt.split()),\n",
    "                            'completion_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'object': 'chat.completion'\n",
    "                    }\n",
    "                    \n",
    "                    return jsonify(openai_response)\n",
    "                else:\n",
    "                    return jsonify({'error': 'Generation failed'}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint for Oxygen monitoring\"\"\"\n",
    "            try:\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                models = ollama_response.json().get('models', []) if ollama_response.status_code == 200 else []\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'healthy',\n",
    "                    'service': 'ollama-gemma-api',\n",
    "                    'version': '1.0.0',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'available_models': [m.get('name', '') for m in models],\n",
    "                    'flask_api': 'running',\n",
    "                    'endpoints': [\n",
    "                        '/api/generate',\n",
    "                        '/api/chat/completions',\n",
    "                        '/api/health',\n",
    "                        '/api/models'\n",
    "                    ]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            \"\"\"List available models in OpenAI-compatible format\"\"\"\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    models_data = response.json()\n",
    "                    models = []\n",
    "                    for model in models_data.get('models', []):\n",
    "                        models.append({\n",
    "                            'id': model.get('name', ''),\n",
    "                            'object': 'model',\n",
    "                            'created': 0,\n",
    "                            'owned_by': 'ollama',\n",
    "                            'name': model.get('name', ''),\n",
    "                            'size': model.get('size', 0),\n",
    "                            'modified': model.get('modified_at', '')\n",
    "                        })\n",
    "                    return jsonify({'data': models, 'object': 'list'})\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False, threaded=True)\n",
    "\n",
    "# Create and start the enhanced API server\n",
    "oxygen_api = OxygenOllamaAPI()\n",
    "\n",
    "def start_oxygen_api_server():\n",
    "    oxygen_api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_oxygen_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 Oxygen AI Positron 5.0 compatible API server started on http://localhost:5000\")\n",
    "print(\"Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text (primary endpoint)\")\n",
    "print(\"- POST /api/chat/completions - OpenAI-compatible endpoint\")\n",
    "print(\"- GET /api/health - Health check\")\n",
    "print(\"- GET /api/models - List available models\")\n",
    "\n",
    "# Test the API\n",
    "time.sleep(3)\n",
    "test_data = {\n",
    "    'input': 'Write a brief summary about XML editing.',\n",
    "    'model': 'gemma3:1b'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post('http://localhost:5000/api/generate', json=test_data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\n✅ API Test successful!\")\n",
    "        print(f\"Response: {result.get('text', result.get('response', ''))[:200]}...\")\n",
    "    else:\n",
    "        print(f\"❌ API Test failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"❌ API Test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982677d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Install Oxygen AI Positron 5.0 Add-on**\n",
    "\n",
    "### Step 2.1: Install the Add-on\n",
    "1. Open Oxygen XML Editor\n",
    "2. Go to **Help** > **Install new add-ons**\n",
    "3. Add the update site: `https://www.oxygenxml.com/InstData/Addons/default/updateSite.xml`\n",
    "4. Search for \"AI Positron\" and install version 5.0+\n",
    "5. Restart Oxygen XML Editor\n",
    "\n",
    "### Step 2.2: Verify Installation\n",
    "1. Check **Options** > **Preferences** > **Plugins** for \"AI Positron\"\n",
    "2. Verify the add-on is enabled and version 5.0+\n",
    "\n",
    "## 3. **Create Custom Connector Following Sample Plugin Structure**\n",
    "\n",
    "Based on the GitHub sample plugin, create a custom connector configuration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"local-ollama-gemma3-1b\",\n",
    "  \"name\": \"Local Ollama Gemma3:1b\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model for text generation\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"type\": \"custom\",\n",
    "  \"provider\": \"ollama\",\n",
    "  \"baseUrl\": \"http://localhost:5000\",\n",
    "  \"authentication\": {\n",
    "    \"type\": \"none\",\n",
    "    \"required\": false\n",
    "  },\n",
    "  \"capabilities\": {\n",
    "    \"textGeneration\": true,\n",
    "    \"chatCompletion\": true,\n",
    "    \"streaming\": false\n",
    "  },\n",
    "  \"endpoints\": {\n",
    "    \"textGeneration\": {\n",
    "      \"path\": \"/api/generate\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"input\": \"${prompt}\",\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.text\",\n",
    "        \"alternativeTextPaths\": [\n",
    "          \"$.response\", \n",
    "          \"$.content\", \n",
    "          \"$.choices[0].text\",\n",
    "          \"$.choices[0].message.content\"\n",
    "        ],\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"chatCompletion\": {\n",
    "      \"path\": \"/api/chat/completions\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"${prompt}\"\n",
    "          }\n",
    "        ],\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.choices[0].message.content\",\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"models\": {\n",
    "      \"path\": \"/api/models\",\n",
    "      \"method\": \"GET\",\n",
    "      \"responseMapping\": {\n",
    "        \"modelsPath\": \"$.data\",\n",
    "        \"modelIdPath\": \"$.id\",\n",
    "        \"modelNamePath\": \"$.name\"\n",
    "      }\n",
    "    },\n",
    "    \"health\": {\n",
    "      \"path\": \"/api/health\",\n",
    "      \"method\": \"GET\"\n",
    "    }\n",
    "  },\n",
    "  \"defaultParameters\": {\n",
    "    \"model\": \"gemma3:1b\",\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0.7\n",
    "  },\n",
    "  \"timeout\": 120000,\n",
    "  \"retryAttempts\": 2,\n",
    "  \"retryDelay\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d68691",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Configure Oxygen AI Positron Settings**\n",
    "\n",
    "### Step 4.1: Basic Configuration\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Click **Add Custom Connector**\n",
    "3. Import the JSON configuration from Step 3\n",
    "4. Set the following:\n",
    "   - **Name**: `Local Ollama Gemma3:1b`\n",
    "   - **Base URL**: `http://localhost:5000`\n",
    "   - **API Key**: Leave empty\n",
    "   - **Default Model**: `gemma3:1b`\n",
    "\n",
    "### Step 4.2: Test Connection\n",
    "1. In the Custom Connector configuration dialog\n",
    "2. Click **Test Connection**\n",
    "3. Verify successful connection to your Ollama service\n",
    "\n",
    "## 5. **Create Custom Actions and Framework**\n",
    "\n",
    "### Step 5.1: Create Enhanced Framework File\n",
    "\n",
    "Create `oxygen-ai-gemma-framework.framework`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dba9d3",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<serialized version=\"25.1\" xml:space=\"preserve\">\n",
    "    <serializableOrderedMap>\n",
    "        <entry>\n",
    "            <String>document.types</String>\n",
    "            <documentTypeDescriptor-array>\n",
    "                <documentTypeDescriptor>\n",
    "                    <field name=\"name\">\n",
    "                        <String>AI Enhanced XML with Gemma3</String>\n",
    "                    </field>\n",
    "                    <field name=\"description\">\n",
    "                        <String>XML editing with AI assistance using local Gemma3:1b model via Oxygen AI Positron 5.0</String>\n",
    "                    </field>\n",
    "                    <field name=\"priority\">\n",
    "                        <Integer>3</Integer>\n",
    "                    </field>\n",
    "                    <field name=\"authorActions\">\n",
    "                        <authorAction-array>\n",
    "                            <!-- AI Summarize Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.summarize</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Summarize with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Summarize selected text using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please provide a concise and clear summary of the following text. Focus on the main points and key information:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>REPLACE</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>500</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt S</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Improve Writing Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.improve</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Improve Writing with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Improve writing style and clarity using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please improve the writing, clarity, and flow of the following text while maintaining its original meaning and intent. Make it more professional and readable:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>REPLACE</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>800</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt I</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Generate Documentation Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.document</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Generate Documentation with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Generate comprehensive documentation using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please generate comprehensive technical documentation for the following code or content. Include purpose, parameters, usage examples, and relevant notes:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>1200</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt D</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Explain XML Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.explain.xml</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Explain XML with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Explain XML structure and content using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please explain the following XML structure in detail. Describe what it represents, its elements, attributes, and how it should be used:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>800</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt E</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Generate XML Schema Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.schema</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Generate XML Schema with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Generate XML Schema (XSD) using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Based on the following XML structure, please generate a corresponding XML Schema (XSD) with appropriate element declarations, types, constraints, and documentation:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>1500</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt X</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Translate Text Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.translate</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Translate with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Translate selected text using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please translate the following text to English (if it's in another language) or provide a translation to Spanish (if it's in English):\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>600</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt T</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                        </authorAction-array>\n",
    "                    </field>\n",
    "                </documentTypeDescriptor>\n",
    "            </documentTypeDescriptor-array>\n",
    "        </entry>\n",
    "    </serializableOrderedMap>\n",
    "</serialized>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed0d89",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 5.2: Install the Framework\n",
    "1. Save the framework file to your Oxygen frameworks directory\n",
    "2. Go to **Options** > **Preferences** > **Document Type Association**\n",
    "3. Import the framework or create a new document type based on it\n",
    "\n",
    "## 6. **Create Sample Plugin (Following GitHub Sample)**\n",
    "\n",
    "Based on the sample plugin structure, create a custom plugin:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8213cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create directory: c:\\Project\\OCR\\OxygenGemmaPlugin\\src\\main\\java\\com\\example\\oxygen\\gemma\\\n",
    "\n",
    "// File: OxygenGemmaPlugin.java\n",
    "package com.example.oxygen.gemma;\n",
    "\n",
    "import ro.sync.exml.plugin.Plugin;\n",
    "import ro.sync.exml.plugin.PluginDescriptor;\n",
    "import ro.sync.exml.workspace.api.standalone.StandalonePluginWorkspace;\n",
    "\n",
    "public class OxygenGemmaPlugin extends Plugin {\n",
    "    \n",
    "    public static final String PLUGIN_ID = \"oxygen.gemma.ai.plugin\";\n",
    "    \n",
    "    @Override\n",
    "    public void applicationStarted(StandalonePluginWorkspace workspace) {\n",
    "        // Add menu customizer for Gemma AI actions\n",
    "        workspace.addMenuBarCustomizer(new GemmaMenuCustomizer());\n",
    "        \n",
    "        // Add toolbar customizer\n",
    "        workspace.addToolbarComponentsCustomizer(new GemmaToolbarCustomizer());\n",
    "        \n",
    "        // Register custom AI connector\n",
    "        workspace.getUtilAccess().addExtension(new GemmaAIConnectorExtension());\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public PluginDescriptor getDescriptor() {\n",
    "        return new PluginDescriptor() {\n",
    "            @Override\n",
    "            public String getID() {\n",
    "                return PLUGIN_ID;\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getDescription() {\n",
    "                return \"Integration plugin for local Ollama Gemma3:1b model with Oxygen AI Positron 5.0\";\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getName() {\n",
    "                return \"Oxygen Gemma AI Plugin\";\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getVendor() {\n",
    "                return \"Custom Development\";\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getVersion() {\n",
    "                return \"1.0.0\";\n",
    "            }\n",
    "        };\n",
    "    }\n",
    "}\n",
    "\n",
    "// File: GemmaMenuCustomizer.java\n",
    "package com.example.oxygen.gemma;\n",
    "\n",
    "import ro.sync.exml.workspace.api.standalone.MenuBarCustomizer;\n",
    "import ro.sync.exml.workspace.api.standalone.StandalonePluginWorkspace;\n",
    "import javax.swing.*;\n",
    "\n",
    "public class GemmaMenuCustomizer implements MenuBarCustomizer {\n",
    "    \n",
    "    @Override\n",
    "    public void customizeMainMenu(JMenuBar menuBar, StandalonePluginWorkspace workspace) {\n",
    "        // Create Gemma AI menu\n",
    "        JMenu gemmaMenu = new JMenu(\"Gemma AI\");\n",
    "        \n",
    "        // Add actions\n",
    "        gemmaMenu.add(createSummarizeAction(workspace));\n",
    "        gemmaMenu.add(createImproveAction(workspace));\n",
    "        gemmaMenu.add(createDocumentAction(workspace));\n",
    "        gemmaMenu.addSeparator();\n",
    "        gemmaMenu.add(createExplainAction(workspace));\n",
    "        gemmaMenu.add(createSchemaAction(workspace));\n",
    "        \n",
    "        // Add to menu bar\n",
    "        menuBar.add(gemmaMenu);\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createSummarizeAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Summarize with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            // Trigger AI Positron summarize action\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.summarize\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createImproveAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Improve Writing with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.improve\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createDocumentAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Generate Documentation with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.document\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createExplainAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Explain XML with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.explain.xml\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createSchemaAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Generate Schema with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.schema\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "}\n",
    "\n",
    "// File: plugin.xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<plugin\n",
    " id=\"oxygen.gemma.ai.plugin\"\n",
    " name=\"Oxygen Gemma AI Plugin\"\n",
    " description=\"Integration plugin for local Ollama Gemma3:1b model with Oxygen AI Positron 5.0\"\n",
    " version=\"1.0.0\"\n",
    " vendor=\"Custom Development\"\n",
    " class=\"com.example.oxygen.gemma.OxygenGemmaPlugin\">\n",
    " \n",
    " <runtime>\n",
    "  <library name=\"lib/oxygen-gemma-plugin.jar\"/>\n",
    " </runtime>\n",
    " \n",
    " <extension point=\"WorkspaceAccess\">\n",
    "  <workspace class=\"com.example.oxygen.gemma.OxygenGemmaPlugin\"/>\n",
    " </extension>\n",
    " \n",
    "</plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9106ebe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. **Comprehensive Integration Test**\n",
    "\n",
    "Create a comprehensive test to verify everything works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077aeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def comprehensive_oxygen_gemma_test():\n",
    "    \"\"\"Comprehensive test of Oxygen AI Positron 5.0 + Gemma3:1b integration\"\"\"\n",
    "    \n",
    "    base_url = \"http://localhost:5000\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Health Check Test\",\n",
    "            \"endpoint\": \"/api/health\",\n",
    "            \"method\": \"GET\",\n",
    "            \"expected_fields\": [\"status\", \"service\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Models List Test\",\n",
    "            \"endpoint\": \"/api/models\",\n",
    "            \"method\": \"GET\",\n",
    "            \"expected_fields\": [\"data\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Basic Text Generation Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"What is XML and why is it important in modern web development?\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 500\n",
    "            },\n",
    "            \"expected_fields\": [\"text\", \"response\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Chat Completions Test (OpenAI-compatible)\",\n",
    "            \"endpoint\": \"/api/chat/completions\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"Explain XML namespaces with examples\"}\n",
    "                ],\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 600\n",
    "            },\n",
    "            \"expected_fields\": [\"choices\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"XML Schema Generation Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"Generate an XML Schema for a book catalog with the following structure: book (id attribute), title, author, publication year, genre\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 1000\n",
    "            },\n",
    "            \"expected_fields\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"XML Explanation Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"Explain this XML structure: <library><book id='1'><title>XML Guide</title><author>John Doe</author><year>2024</year></book></library>\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 800\n",
    "            },\n",
    "            \"expected_fields\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Writing Improvement Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"Improve this text: XML is a markup language that is used for storing and transporting data and it is very useful for web development\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 400\n",
    "            },\n",
    "            \"expected_fields\": [\"text\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 Running Comprehensive Oxygen AI Positron 5.0 + Gemma3:1b Integration Test\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test each case\n",
    "    passed_tests = 0\n",
    "    total_tests = len(test_cases)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}/{total_tests}: {test['name']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if test['method'] == 'GET':\n",
    "                response = requests.get(f\"{base_url}{test['endpoint']}\", timeout=30)\n",
    "            else:\n",
    "                response = requests.post(f\"{base_url}{test['endpoint']}\", \n",
    "                                       json=test['payload'], timeout=60)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # Check for expected fields\n",
    "                found_fields = []\n",
    "                missing_fields = []\n",
    "                \n",
    "                for field in test['expected_fields']:\n",
    "                    if field in result:\n",
    "                        found_fields.append(field)\n",
    "                    else:\n",
    "                        missing_fields.append(field)\n",
    "                \n",
    "                if not missing_fields:\n",
    "                    print(f\"✅ PASS - Response time: {processing_time:.2f}s\")\n",
    "                    \n",
    "                    # Extract and display response content\n",
    "                    if 'choices' in result:\n",
    "                        content = result['choices'][0]['message']['content']\n",
    "                    elif 'data' in result:\n",
    "                        content = f\"Found {len(result['data'])} models\"\n",
    "                    else:\n",
    "                        content = result.get('text', result.get('response', result.get('status', '')))\n",
    "                    \n",
    "                    if isinstance(content, str) and len(content) > 100:\n",
    "                        print(f\"   Generated {len(content)} characters\")\n",
    "                        print(f\"   Preview: {content[:150]}...\")\n",
    "                    else:\n",
    "                        print(f\"   Result: {content}\")\n",
    "                    \n",
    "                    passed_tests += 1\n",
    "                else:\n",
    "                    print(f\"⚠️ PARTIAL - Missing fields: {missing_fields}\")\n",
    "                    print(f\"   Available fields: {list(result.keys())}\")\n",
    "            else:\n",
    "                print(f\"❌ FAIL - HTTP {response.status_code}\")\n",
    "                print(f\"   Error: {response.text}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"❌ TIMEOUT - Request took too long\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"✅ Integration test completed!\")\n",
    "    print(f\"📊 Results: {passed_tests}/{total_tests} tests passed ({(passed_tests/total_tests)*100:.1f}%)\")\n",
    "    \n",
    "    if passed_tests == total_tests:\n",
    "        print(\"🎉 All tests passed! Your Oxygen AI Positron + Gemma3:1b integration is ready!\")\n",
    "    else:\n",
    "        print(\"⚠️ Some tests failed. Please check the configuration and try again.\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_oxygen_gemma_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d027e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. **Usage Instructions**\n",
    "\n",
    "### Step 8.1: Basic Usage in Oxygen\n",
    "1. Open an XML document in Oxygen XML Editor\n",
    "2. Select some text\n",
    "3. Use one of these methods:\n",
    "   - **Menu**: Go to **Gemma AI** menu and select an action\n",
    "   - **Keyboard shortcuts**: Use `Ctrl+Alt+S` (summarize), `Ctrl+Alt+I` (improve), etc.\n",
    "   - **AI Positron panel**: Use the AI Positron side panel\n",
    "\n",
    "### Step 8.2: Advanced Usage\n",
    "1. Configure custom prompts in the AI Positron settings\n",
    "2. Adjust model parameters (temperature, max tokens) per action\n",
    "3. Create additional custom actions for specific use cases\n",
    "\n",
    "## 9. **Troubleshooting**\n",
    "\n",
    "### Issue 1: Connection Problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b13c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_oxygen_connection():\n",
    "    \"\"\"Diagnose connection issues with Oxygen AI Positron\"\"\"\n",
    "    \n",
    "    print(\"🔍 Diagnosing Oxygen AI Positron connection...\")\n",
    "    \n",
    "    # Check if all services are running\n",
    "    checks = [\n",
    "        (\"Ollama Server\", \"http://localhost:11434/api/tags\"),\n",
    "        (\"Flask API\", \"http://localhost:5000/api/health\"),\n",
    "        (\"Models List\", \"http://localhost:5000/api/models\"),\n",
    "        (\"Generate Endpoint\", \"http://localhost:5000/api/generate\")\n",
    "    ]\n",
    "    \n",
    "    for name, url in checks:\n",
    "        try:\n",
    "            if \"generate\" in url:\n",
    "                response = requests.post(url, json={\"input\": \"test\", \"model\": \"gemma3:1b\"}, timeout=10)\n",
    "            else:\n",
    "                response = requests.get(url, timeout=10)\n",
    "            \n",
    "            print(f\"✅ {name}: Status {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {name}: {e}\")\n",
    "            \n",
    "    # Check Gemma3:1b model\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, shell=True)\n",
    "        if 'gemma3:1b' in result.stdout:\n",
    "            print(\"✅ Gemma3:1b model available\")\n",
    "        else:\n",
    "            print(\"❌ Gemma3:1b model not found - run 'ollama pull gemma3:1b'\")\n",
    "    except:\n",
    "        print(\"❌ Cannot check Ollama models\")\n",
    "\n",
    "diagnose_oxygen_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a651",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This comprehensive setup provides full integration between your local Ollama Gemma3:1b service and Oxygen XML Editor using AI Positron 5.0, following the official documentation and sample plugin structure for maximum compatibility and functionality.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be2147",
   "metadata": {},
   "source": [
    "Looking at your Ollama RESTful service implementation, I can see that it currently doesn't include API key authentication. Ollama itself doesn't natively support API keys, but you can add API key authentication to your Flask wrapper for security. Here's how to implement it:\n",
    "\n",
    "## 1. **Add API Key Authentication to Your Flask API**\n",
    "\n",
    "Update your `OxygenOllamaAPI` class to include API key support:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "import time\n",
    "import secrets\n",
    "import os\n",
    "from functools import wraps\n",
    "\n",
    "class OxygenOllamaAPI:\n",
    "    def __init__(self, api_key=None):\n",
    "        self.app = Flask(__name__)\n",
    "        # Enhanced CORS configuration for Oxygen XML Editor\n",
    "        CORS(self.app, \n",
    "             origins=['*'],\n",
    "             methods=['GET', 'POST', 'OPTIONS', 'PUT', 'DELETE'],\n",
    "             allow_headers=['Content-Type', 'Authorization', 'X-Requested-With', 'Accept', 'X-API-Key'],\n",
    "             expose_headers=['Content-Type', 'Authorization'],\n",
    "             supports_credentials=False)\n",
    "        \n",
    "        # Set API key\n",
    "        # self.api_key = api_key or os.getenv('OLLAMA_API_KEY') or self.generate_api_key()\n",
    "        # self.api_key = os.getenv('OLLAMA_API_KEY')\n",
    "        self.api_key = self.generate_api_key()\n",
    "        print(f\"🔑 API Key: {self.api_key}\")\n",
    "        \n",
    "        self.setup_routes()\n",
    "    \n",
    "    def generate_api_key(self):\n",
    "        \"\"\"Generate a secure API key\"\"\"\n",
    "        return f\"ollama-{secrets.token_urlsafe(32)}\"\n",
    "    \n",
    "    def require_api_key(self, f):\n",
    "        \"\"\"Decorator to require API key authentication\"\"\"\n",
    "        @wraps(f)\n",
    "        def decorated_function(*args, **kwargs):\n",
    "            # Skip API key check for OPTIONS requests (CORS preflight)\n",
    "            if request.method == 'OPTIONS':\n",
    "                return f(*args, **kwargs)\n",
    "            \n",
    "            # Check for API key in headers\n",
    "            provided_key = (request.headers.get('X-API-Key') or \n",
    "                          request.headers.get('Authorization', '').replace('Bearer ', '') or\n",
    "                          request.args.get('api_key'))\n",
    "            \n",
    "            if not provided_key:\n",
    "                return jsonify({\n",
    "                    'error': 'API key required',\n",
    "                    'message': 'Please provide API key in X-API-Key header, Authorization header, or api_key parameter'\n",
    "                }), 401\n",
    "            \n",
    "            if provided_key != self.api_key:\n",
    "                return jsonify({\n",
    "                    'error': 'Invalid API key',\n",
    "                    'message': 'The provided API key is not valid'\n",
    "                }), 403\n",
    "            \n",
    "            return f(*args, **kwargs)\n",
    "        return decorated_function\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST', 'OPTIONS'])\n",
    "        @self.require_api_key\n",
    "        def generate():\n",
    "            # Handle CORS preflight requests\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-Requested-With,X-API-Key')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "            \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Handle multiple input field formats for Oxygen compatibility\n",
    "                prompt = (data.get('prompt') or \n",
    "                         data.get('input') or \n",
    "                         data.get('text') or \n",
    "                         data.get('message', ''))\n",
    "                \n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                max_tokens = data.get('max_tokens', 1000)\n",
    "                temperature = data.get('temperature', 0.7)\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"🤖 Processing authenticated request: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API with proper options\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': max_tokens,\n",
    "                        'temperature': temperature,\n",
    "                        'top_k': 40,\n",
    "                        'top_p': 0.9\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # Format response for Oxygen AI Positron compatibility\n",
    "                    oxygen_response = {\n",
    "                        'text': generated_text,  # Primary response field for Oxygen\n",
    "                        'response': generated_text,  # Alternative field\n",
    "                        'content': generated_text,  # Another alternative\n",
    "                        'choices': [{  # OpenAI-compatible format\n",
    "                            'text': generated_text,\n",
    "                            'message': {\n",
    "                                'content': generated_text,\n",
    "                                'role': 'assistant'\n",
    "                            }\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'success': True,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'metadata': {\n",
    "                            'tokens_generated': len(generated_text.split()),\n",
    "                            'model_used': model,\n",
    "                            'processing_time': result.get('total_duration', 0)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"✅ Generated {len(generated_text)} characters for authenticated user\")\n",
    "                    return jsonify(oxygen_response)\n",
    "                else:\n",
    "                    error_msg = f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    print(f\"❌ {error_msg}\")\n",
    "                    return jsonify({'error': error_msg}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"❌ Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'success': False\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/chat/completions', methods=['POST', 'OPTIONS'])\n",
    "        @self.require_api_key\n",
    "        def chat_completions():\n",
    "            \"\"\"OpenAI-compatible endpoint for Oxygen AI Positron\"\"\"\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-API-Key')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "                \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                messages = data.get('messages', [])\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                \n",
    "                # Extract prompt from messages\n",
    "                if messages:\n",
    "                    prompt = messages[-1].get('content', '')\n",
    "                else:\n",
    "                    prompt = data.get('prompt', '')\n",
    "                \n",
    "                # Use the same generation logic\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': data.get('max_tokens', 1000),\n",
    "                        'temperature': data.get('temperature', 0.7)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # OpenAI-compatible response format for Oxygen\n",
    "                    openai_response = {\n",
    "                        'choices': [{\n",
    "                            'message': {\n",
    "                                'role': 'assistant',\n",
    "                                'content': generated_text\n",
    "                            },\n",
    "                            'finish_reason': 'stop',\n",
    "                            'index': 0\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split()),\n",
    "                            'prompt_tokens': len(prompt.split()),\n",
    "                            'completion_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'object': 'chat.completion'\n",
    "                    }\n",
    "                    \n",
    "                    return jsonify(openai_response)\n",
    "                else:\n",
    "                    return jsonify({'error': 'Generation failed'}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint (no authentication required)\"\"\"\n",
    "            try:\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                models = ollama_response.json().get('models', []) if ollama_response.status_code == 200 else []\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'healthy',\n",
    "                    'service': 'ollama-gemma-api',\n",
    "                    'version': '1.0.0',\n",
    "                    'authentication': 'enabled',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'available_models': [m.get('name', '') for m in models],\n",
    "                    'flask_api': 'running',\n",
    "                    'endpoints': [\n",
    "                        '/api/generate (requires API key)',\n",
    "                        '/api/chat/completions (requires API key)',\n",
    "                        '/api/health (public)',\n",
    "                        '/api/models (requires API key)',\n",
    "                        '/api/key (public)'\n",
    "                    ]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        @self.require_api_key\n",
    "        def list_models():\n",
    "            \"\"\"List available models in OpenAI-compatible format\"\"\"\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    models_data = response.json()\n",
    "                    models = []\n",
    "                    for model in models_data.get('models', []):\n",
    "                        models.append({\n",
    "                            'id': model.get('name', ''),\n",
    "                            'object': 'model',\n",
    "                            'created': 0,\n",
    "                            'owned_by': 'ollama',\n",
    "                            'name': model.get('name', ''),\n",
    "                            'size': model.get('size', 0),\n",
    "                            'modified': model.get('modified_at', '')\n",
    "                        })\n",
    "                    return jsonify({'data': models, 'object': 'list'})\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/api/key', methods=['GET'])\n",
    "        def get_api_key():\n",
    "            \"\"\"Get the current API key (for initial setup)\"\"\"\n",
    "            return jsonify({\n",
    "                'api_key': self.api_key,\n",
    "                'usage': {\n",
    "                    'header': f'X-API-Key: {self.api_key}',\n",
    "                    'authorization': f'Authorization: Bearer {self.api_key}',\n",
    "                    'parameter': f'?api_key={self.api_key}'\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False, threaded=True)\n",
    "\n",
    "# Create and start the enhanced API server with API key\n",
    "# You can set a custom API key or let it generate one\n",
    "custom_api_key = \"lic-da-16888\"  # Optional: set your own key\n",
    "oxygen_api = OxygenOllamaAPI(api_key=custom_api_key)\n",
    "\n",
    "def start_oxygen_api_server():\n",
    "    oxygen_api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_oxygen_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"🚀 Oxygen AI Positron 5.0 compatible API server with API key authentication started on http://localhost:5000\")\n",
    "print(\"📋 Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text (requires API key)\")\n",
    "print(\"- POST /api/chat/completions - OpenAI-compatible endpoint (requires API key)\")\n",
    "print(\"- GET /api/health - Health check (public)\")\n",
    "print(\"- GET /api/models - List available models (requires API key)\")\n",
    "print(\"- GET /api/key - Get API key information (public)\")\n",
    "\n",
    "# Test the API with authentication\n",
    "time.sleep(3)\n",
    "try:\n",
    "    # First, get the API key\n",
    "    key_response = requests.get('http://localhost:5000/api/key')\n",
    "    if key_response.status_code == 200:\n",
    "        api_key = key_response.json()['api_key']\n",
    "        print(f\"\\n🔑 Using API Key: {api_key}\")\n",
    "        \n",
    "        # Test authenticated request\n",
    "        test_data = {\n",
    "            'input': 'Write a brief summary about XML editing.',\n",
    "            'model': 'gemma3:1b'\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'X-API-Key': api_key\n",
    "        }\n",
    "        \n",
    "        response = requests.post('http://localhost:5000/api/generate', \n",
    "                               json=test_data, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"\\n✅ Authenticated API Test successful!\")\n",
    "            print(f\"Response: {result.get('text', result.get('response', ''))[:200]}...\")\n",
    "        else:\n",
    "            print(f\"❌ API Test failed: {response.status_code}\")\n",
    "            print(response.text)\n",
    "    else:\n",
    "        print(\"❌ Failed to get API key\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ API Test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58cd85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Update Oxygen AI Positron Custom Connector Configuration**\n",
    "\n",
    "Update your custom connector configuration to include the API key:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"local-ollama-gemma3-1b\",\n",
    "  \"name\": \"Local Ollama Gemma3:1b (with API Key)\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model for text generation (API key protected)\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"type\": \"custom\",\n",
    "  \"provider\": \"ollama\",\n",
    "  \"baseUrl\": \"http://localhost:5000\",\n",
    "  \"authentication\": {\n",
    "    \"type\": \"api_key\",\n",
    "    \"required\": true,\n",
    "    \"apiKey\": \"your-api-key-here\"\n",
    "  },\n",
    "  \"capabilities\": {\n",
    "    \"textGeneration\": true,\n",
    "    \"chatCompletion\": true,\n",
    "    \"streaming\": false\n",
    "  },\n",
    "  \"endpoints\": {\n",
    "    \"textGeneration\": {\n",
    "      \"path\": \"/api/generate\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-API-Key\": \"${apiKey}\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"input\": \"${prompt}\",\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.text\",\n",
    "        \"alternativeTextPaths\": [\n",
    "          \"$.response\", \n",
    "          \"$.content\", \n",
    "          \"$.choices[0].text\",\n",
    "          \"$.choices[0].message.content\"\n",
    "        ],\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"chatCompletion\": {\n",
    "      \"path\": \"/api/chat/completions\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer ${apiKey}\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"${prompt}\"\n",
    "          }\n",
    "        ],\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.choices[0].message.content\",\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"models\": {\n",
    "      \"path\": \"/api/models\",\n",
    "      \"method\": \"GET\",\n",
    "      \"headers\": {\n",
    "        \"X-API-Key\": \"${apiKey}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"modelsPath\": \"$.data\",\n",
    "        \"modelIdPath\": \"$.id\",\n",
    "        \"modelNamePath\": \"$.name\"\n",
    "      }\n",
    "    },\n",
    "    \"health\": {\n",
    "      \"path\": \"/api/health\",\n",
    "      \"method\": \"GET\"\n",
    "    }\n",
    "  },\n",
    "  \"defaultParameters\": {\n",
    "    \"model\": \"gemma3:1b\",\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0.7\n",
    "  },\n",
    "  \"timeout\": 120000,\n",
    "  \"retryAttempts\": 2,\n",
    "  \"retryDelay\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5cbf6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Configure API Key in Oxygen XML Editor**\n",
    "\n",
    "### Step 3.1: Get Your API Key\n",
    "Run this function to get your current API key:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8574568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_key_info():\n",
    "    \"\"\"Get API key information from your local service\"\"\"\n",
    "    try:\n",
    "        response = requests.get('http://localhost:5000/api/key', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            key_info = response.json()\n",
    "            print(\"🔑 API Key Information:\")\n",
    "            print(f\"API Key: {key_info['api_key']}\")\n",
    "            print(\"\\n📋 Usage Examples:\")\n",
    "            print(f\"Header: {key_info['usage']['header']}\")\n",
    "            print(f\"Authorization: {key_info['usage']['authorization']}\")\n",
    "            print(f\"Parameter: {key_info['usage']['parameter']}\")\n",
    "            return key_info['api_key']\n",
    "        else:\n",
    "            print(f\"❌ Failed to get API key: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error getting API key: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get your API key\n",
    "api_key = get_api_key_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff6f9b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3.2: Configure in Oxygen AI Positron\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Click **Add Custom Connector** or edit your existing connector\n",
    "3. Set the following:\n",
    "   - **Name**: `Local Ollama Gemma3:1b (Authenticated)`\n",
    "   - **Base URL**: `http://localhost:5000`\n",
    "   - **API Key**: `[paste your API key here]`\n",
    "   - **Authentication Type**: API Key or Bearer Token\n",
    "\n",
    "## 4. **Test Authenticated API**\n",
    "\n",
    "Create a test function to verify API key authentication:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_authenticated_api():\n",
    "    \"\"\"Test API with different authentication methods\"\"\"\n",
    "    \n",
    "    # Get API key\n",
    "    key_response = requests.get('http://localhost:5000/api/key')\n",
    "    if key_response.status_code != 200:\n",
    "        print(\"❌ Failed to get API key\")\n",
    "        return\n",
    "    \n",
    "    api_key = key_response.json()['api_key']\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"X-API-Key Header\",\n",
    "            \"headers\": {\"X-API-Key\": api_key}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Authorization Bearer\",\n",
    "            \"headers\": {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"URL Parameter\",\n",
    "            \"headers\": {},\n",
    "            \"params\": {\"api_key\": api_key}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"No Authentication (should fail)\",\n",
    "            \"headers\": {},\n",
    "            \"should_fail\": True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    test_data = {\n",
    "        'input': 'Test authentication with a simple request.',\n",
    "        'model': 'gemma3:1b'\n",
    "    }\n",
    "    \n",
    "    print(\"🧪 Testing API Key Authentication\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test['name']}\")\n",
    "        \n",
    "        try:\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            headers.update(test.get('headers', {}))\n",
    "            \n",
    "            params = test.get('params', {})\n",
    "            \n",
    "            response = requests.post('http://localhost:5000/api/generate', \n",
    "                                   json=test_data, \n",
    "                                   headers=headers,\n",
    "                                   params=params,\n",
    "                                   timeout=30)\n",
    "            \n",
    "            if test.get('should_fail', False):\n",
    "                if response.status_code in [401, 403]:\n",
    "                    print(f\"✅ PASS - Authentication correctly rejected (HTTP {response.status_code})\")\n",
    "                else:\n",
    "                    print(f\"❌ FAIL - Expected authentication failure but got HTTP {response.status_code}\")\n",
    "            else:\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('text', result.get('response', ''))\n",
    "                    print(f\"✅ PASS - Generated {len(generated_text)} characters\")\n",
    "                    print(f\"   Preview: {generated_text[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"❌ FAIL - HTTP {response.status_code}: {response.text}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"✅ Authentication test completed!\")\n",
    "\n",
    "# Run authentication test\n",
    "test_authenticated_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93dc02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Environment Variable Configuration**\n",
    "\n",
    "For better security, you can also set the API key via environment variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set API key via environment variable (run once)\n",
    "def set_api_key_env(api_key):\n",
    "    \"\"\"Set API key as environment variable\"\"\"\n",
    "    os.environ['OLLAMA_API_KEY'] = api_key\n",
    "    print(f\"✅ API key set in environment: OLLAMA_API_KEY={api_key}\")\n",
    "\n",
    "# Use a custom API key\n",
    "custom_key = \"ollama-secure-key-2024\"\n",
    "set_api_key_env(custom_key)\n",
    "\n",
    "# The API will automatically use this environment variable\n",
    "# when creating the OxygenOllamaAPI instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51efce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OLLAMA_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fb0e5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. **Security Best Practices**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "\n",
    "class SecureOllamaAPI(OxygenOllamaAPI):\n",
    "    \"\"\"Enhanced API with additional security features\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        super().__init__(api_key)\n",
    "        self.request_log = {}\n",
    "        self.rate_limit = 60  # requests per minute\n",
    "    \n",
    "    def check_rate_limit(self, client_ip):\n",
    "        \"\"\"Basic rate limiting\"\"\"\n",
    "        now = time.time()\n",
    "        minute = int(now // 60)\n",
    "        \n",
    "        if client_ip not in self.request_log:\n",
    "            self.request_log[client_ip] = {}\n",
    "        \n",
    "        if minute not in self.request_log[client_ip]:\n",
    "            self.request_log[client_ip][minute] = 0\n",
    "        \n",
    "        self.request_log[client_ip][minute] += 1\n",
    "        \n",
    "        # Clean old entries\n",
    "        for old_minute in list(self.request_log[client_ip].keys()):\n",
    "            if old_minute < minute - 1:\n",
    "                del self.request_log[client_ip][old_minute]\n",
    "        \n",
    "        return self.request_log[client_ip][minute] <= self.rate_limit\n",
    "    \n",
    "    def hash_api_key(self, key):\n",
    "        \"\"\"Hash API key for logging\"\"\"\n",
    "        return hashlib.sha256(key.encode()).hexdigest()[:16]\n",
    "\n",
    "# Use the secure version\n",
    "# secure_api = SecureOllamaAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5cef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This implementation provides:\n",
    "\n",
    "1. **Multiple Authentication Methods**: X-API-Key header, Authorization Bearer token, or URL parameter\n",
    "2. **Automatic Key Generation**: Creates secure API keys if none provided\n",
    "3. **CORS Support**: Properly handles preflight requests with authentication headers\n",
    "4. **Error Handling**: Clear error messages for authentication failures\n",
    "5. **Health Endpoint**: Public endpoint to check service status\n",
    "6. **Key Retrieval**: Endpoint to get current API key for initial setup\n",
    "7. **Security Features**: Rate limiting and key hashing options\n",
    "\n",
    "The API key will be required for all protected endpoints (`/api/generate`, `/api/chat/completions`, `/api/models`) while keeping health check and key retrieval endpoints public for initial setup.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

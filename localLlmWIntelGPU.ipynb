{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "895924e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\project\\ocr\\.venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: flask in c:\\project\\ocr\\.venv\\lib\\site-packages (3.1.1)\n",
      "Collecting flask_cors\n",
      "  Downloading flask_cors-6.0.0-py3-none-any.whl.metadata (961 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\project\\ocr\\.venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\project\\ocr\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\project\\ocr\\.venv\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\project\\ocr\\.venv\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: blinker>=1.9.0 in c:\\project\\ocr\\.venv\\lib\\site-packages (from flask) (1.9.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\project\\ocr\\.venv\\lib\\site-packages (from flask) (8.2.1)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in c:\\project\\ocr\\.venv\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\project\\ocr\\.venv\\lib\\site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\project\\ocr\\.venv\\lib\\site-packages (from flask) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in c:\\project\\ocr\\.venv\\lib\\site-packages (from flask) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\project\\ocr\\.venv\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Downloading flask_cors-6.0.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: flask_cors\n",
      "Successfully installed flask_cors-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests flask flask_cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb02a74",
   "metadata": {},
   "source": [
    "Yes, it's possible to install and run Ollama with the Gemma model from a Jupyter notebook and provide a RESTful API. Here's how to set it up:\n",
    "\n",
    "## 1. Install Ollama\n",
    "\n",
    "First, install Ollama on your Windows machine. You can download it from the official website or use the notebook to install it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6f2d038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please download Ollama from: https://ollama.ai/download\n",
      "Or run: winget install Ollama.Ollama\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Download and install Ollama (run this once)\n",
    "def install_ollama():\n",
    "    \"\"\"Download and install Ollama on Windows\"\"\"\n",
    "    print(\"Please download Ollama from: https://ollama.ai/download\")\n",
    "    print(\"Or run: winget install Ollama.Ollama\")\n",
    "    \n",
    "# Uncomment to install\n",
    "install_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0764f290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Ollama using winget...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Download and install Ollama (run this once)\n",
    "def install_ollama():\n",
    "    \"\"\"Download and install Ollama on Windows using winget\"\"\"\n",
    "    try:\n",
    "        print(\"Installing Ollama using winget...\")\n",
    "        result = subprocess.run(['winget', 'install', 'Ollama.Ollama'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"Ollama installed successfully!\")\n",
    "            print(\"You may need to restart your terminal or add Ollama to PATH\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Installation failed: {result.stderr}\")\n",
    "            print(\"Alternative: Download from https://ollama.ai/download\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error installing Ollama: {e}\")\n",
    "        print(\"Manual installation required:\")\n",
    "        print(\"1. Download from: https://ollama.ai/download\")\n",
    "        print(\"2. Or run in PowerShell/CMD: winget install Ollama.Ollama\")\n",
    "        return False\n",
    "\n",
    "# Install Ollama\n",
    "install_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0894ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Ollama status...\n",
      "âŒ Ollama not found\n",
      "\n",
      "Proceeding with installation...\n",
      "Starting Ollama installation with winget...\n",
      "This may take several minutes...\n",
      "\n",
      "\n",
      "-\n",
      "\n",
      "\n",
      "-\n",
      "\\\n",
      "\n",
      "Found Ollama [Ollama.Ollama] Version 0.8.0\n",
      "This application is licensed to you by its owner.\n",
      "Microsoft is not responsible for, nor does it grant any licenses to, third-party packages.\n",
      "Downloading https://github.com/ollama/ollama/releases/download/v0.8.0/OllamaSetup.exe\n",
      "\n",
      "-\n",
      "\\\n",
      "|\n",
      "/\n",
      "-\n",
      "\\\n",
      "|\n",
      "/\n",
      "-\n",
      "\\\n",
      "âŒ Error during installation: 'cp950' codec can't decode byte 0xe2 in position 125: illegal multibyte sequence\n",
      "\n",
      "Rechecking status after installation...\n",
      "Checking Ollama status...\n",
      "âŒ Ollama not found\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def install_ollama_with_monitoring():\n",
    "    \"\"\"Install Ollama with detailed monitoring\"\"\"\n",
    "    try:\n",
    "        print(\"Starting Ollama installation with winget...\")\n",
    "        print(\"This may take several minutes...\\n\")\n",
    "        \n",
    "        # Run installation with real-time output\n",
    "        process = subprocess.Popen(\n",
    "            ['winget', 'install', 'Ollama.Ollama', '--accept-package-agreements', '--accept-source-agreements'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            universal_newlines=True,\n",
    "            shell=True\n",
    "        )\n",
    "        \n",
    "        # Monitor output in real-time\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "        \n",
    "        return_code = process.poll()\n",
    "        \n",
    "        if return_code == 0:\n",
    "            print(\"\\nâœ… Installation completed successfully!\")\n",
    "            \n",
    "            # Verify installation\n",
    "            print(\"Verifying installation...\")\n",
    "            verify_result = subprocess.run(['ollama', '--version'], \n",
    "                                         capture_output=True, text=True, shell=True)\n",
    "            if verify_result.returncode == 0:\n",
    "                print(f\"âœ… Verification successful: {verify_result.stdout.strip()}\")\n",
    "            else:\n",
    "                print(\"âš ï¸ Installation completed but Ollama not found in PATH\")\n",
    "                print(\"You may need to restart your terminal or VS Code\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\\nâŒ Installation failed with return code: {return_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during installation: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama is installed and running\"\"\"\n",
    "    print(\"Checking Ollama status...\")\n",
    "    \n",
    "    # Check if Ollama is installed\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', '--version'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Ollama installed: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(\"âŒ Ollama not found\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking Ollama: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if Ollama service is running\n",
    "    try:\n",
    "        service_check = subprocess.run(['sc', 'query', 'Ollama'], \n",
    "                                     capture_output=True, text=True, shell=True)\n",
    "        if 'RUNNING' in service_check.stdout:\n",
    "            print(\"âœ… Ollama service is running\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Ollama service is not running\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not check service status: {e}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Check current status first\n",
    "if not check_ollama_status():\n",
    "    print(\"\\nProceeding with installation...\")\n",
    "    install_ollama_with_monitoring()\n",
    "    print(\"\\nRechecking status after installation...\")\n",
    "    check_ollama_status()\n",
    "else:\n",
    "    print(\"Ollama is already installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b83fae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Pull and Run Gemma Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fc3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama server started on http://localhost:11434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Pull the Gemma model (this will take some time)\n",
    "def setup_gemma():\n",
    "    \"\"\"Pull the Gemma model using Ollama\"\"\"\n",
    "    try:\n",
    "        # Pull gemma model\n",
    "        result = subprocess.run(['ollama', 'pull', 'gemma3:4b'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        # result = subprocess.run(['ollama', 'pull', 'gemma3:1b'], \n",
    "        #                       capture_output=True, text=True, shell=True)\n",
    "        print(\"Gemma model pulled successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run Ollama server in background\n",
    "def start_ollama_server():\n",
    "    \"\"\"Start Ollama server\"\"\"\n",
    "    try:\n",
    "        subprocess.Popen(['ollama', 'serve'], shell=True)\n",
    "        time.sleep(5)  # Wait for server to start\n",
    "        print(\"Ollama server started on http://localhost:11434\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting server: {e}\")\n",
    "        return False\n",
    "\n",
    "# Setup the environment\n",
    "setup_gemma()\n",
    "start_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627b8c87",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Create RESTful API Wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47d2f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API server started on http://localhost:5000\n",
      "Available endpoints:\n",
      "- POST /api/generate - Generate text\n",
      "- GET /api/models - List available models\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "\n",
    "class OllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)  # Enable CORS for Oxygen XML Editor\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST'])\n",
    "        def generate():\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                prompt = data.get('prompt', '')\n",
    "                # model = data.get('model', 'gemma3:4b')\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                \n",
    "                # Call Ollama API\n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json={\n",
    "                                           'model': model,\n",
    "                                           'prompt': prompt,\n",
    "                                           'stream': False\n",
    "                                       })\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return jsonify({\n",
    "                        'success': True,\n",
    "                        'response': result.get('response', ''),\n",
    "                        'model': model\n",
    "                    })\n",
    "                else:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': 'Failed to generate response'\n",
    "                    }), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags')\n",
    "                if response.status_code == 200:\n",
    "                    return jsonify(response.json())\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    def run(self, host='localhost', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False)\n",
    "\n",
    "# Create and start the API server\n",
    "api = OllamaAPI()\n",
    "\n",
    "# Run in a separate thread to avoid blocking the notebook\n",
    "def start_api_server():\n",
    "    api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "server_thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"API server started on http://localhost:5000\")\n",
    "print(\"Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text\")\n",
    "print(\"- GET /api/models - List available models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbce15a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Test the API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fbfc1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Test Successful!\n",
      "Response: Okay, let's break down machine learning in simple terms. Think of it like this:\n",
      "\n",
      "**Instead of giving a computer *exactly* what to do, we teach it to learn from data.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1. **Data is Key:** Machine learning starts with lots of information â€“ data. This could be anything â€“ pictures, text, numbers, sounds, or even website clicks.\n",
      "\n",
      "2. **Pattern Recognition:**  We feed the computer this data. The computer then looks for patterns, relationships, and trends within the data.  It tries to figure out *what* is happening.\n",
      "\n",
      "3. **Learning from Examples:**  The computer doesnâ€™t know the answer initially. It just looks at examples.  For example, if we show it thousands of pictures of cats and dogs, it learns to identify the features that distinguish a cat from a dog.\n",
      "\n",
      "4. **Making Predictions:**  Once the computer has learned the patterns, it can use that knowledge to make predictions about *new* data it hasnâ€™t seen before.  \n",
      "\n",
      "   * **Example:** After learning from the pictures, it can now look at a new picture and say \"That's likely a cat!\"\n",
      "\n",
      "\n",
      "**There are different types of machine learning, but here are a few common types to understand:**\n",
      "\n",
      "* **Supervised Learning:**  We give the computer labeled data â€“ examples with the \"correct answer.\"  (Like teaching it to identify cats and dogs by showing it labeled pictures.)\n",
      "* **Unsupervised Learning:** We give the computer unlabeled data and let it find patterns on its own. (Like grouping customers into different segments based on their purchasing habits.)\n",
      "* **Reinforcement Learning:**  The computer learns by trial and error, getting rewards for good actions and penalties for bad ones. (Think of a robot learning to walk â€“ it gets feedback and adjusts its movements.)\n",
      "\n",
      "\n",
      "**In short, machine learning is about teaching computers to learn from data so they can make better predictions and decisions.**\n",
      "\n",
      "**Think of it like teaching a puppy a trick.** You donâ€™t tell it *exactly* how to do it â€“ you show it, reward it when it gets close, and correct it when it messes up.  Eventually, it learns the trick on its own.\n",
      "\n",
      "\n",
      "**Do you want me to give you a more specific example? For instance, could you tell me:**\n",
      "\n",
      "*   What kind of data would you use for machine learning? (e.g., images, text, numbers?)\n",
      "*   What kind of applications of machine learning are you most interested in?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test the API\n",
    "def test_api():\n",
    "    \"\"\"Test the local API\"\"\"\n",
    "    test_data = {\n",
    "        'prompt': 'Explain what machine learning is in simple terms.',\n",
    "        # 'model': 'gemma3:4b'\n",
    "        'model': 'gemma3:1b'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post('http://127.0.0.1:5000/api/generate', \n",
    "                               json=test_data)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"API Test Successful!\")\n",
    "            print(f\"Response: {result['response']}\")\n",
    "        else:\n",
    "            print(f\"API Test Failed: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"API Test Error: {e}\")\n",
    "\n",
    "# Wait a moment for server to start, then test\n",
    "time.sleep(3)\n",
    "test_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7742356",
   "metadata": {},
   "source": [
    "When you get \"API Test Failed: 500\", it indicates a server-side error. Here are the most likely causes and how to fix them:\n",
    "\n",
    "## Common Issues Causing HTTP 500:\n",
    "\n",
    "### 1. **Ollama Server Not Running**\n",
    "The most common cause - your Flask API is running but can't connect to Ollama:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c287e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Checking if Ollama server is running...\n",
      "âœ… Ollama server is running\n",
      "Available models: []\n",
      "\n",
      "2. Checking Flask API...\n",
      "Flask API status: 200\n",
      "\n",
      "3. Testing direct Ollama API...\n",
      "Direct Ollama test: 404\n",
      "Error: {\"error\":\"model 'gemma3:4b' not found\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def diagnose_api_error():\n",
    "    \"\"\"Diagnose API 500 errors\"\"\"\n",
    "    \n",
    "    # Check if Ollama server is running\n",
    "    print(\"1. Checking if Ollama server is running...\")\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Ollama server is running\")\n",
    "            models = response.json().get('models', [])\n",
    "            print(f\"Available models: {[m['name'] for m in models]}\")\n",
    "        else:\n",
    "            print(\"âŒ Ollama server returned error:\", response.status_code)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama server on localhost:11434\")\n",
    "        print(\"Starting Ollama server...\")\n",
    "        subprocess.Popen(['ollama', 'serve'], shell=True)\n",
    "        time.sleep(10)\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking Ollama: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Check if Flask API is running\n",
    "    print(\"\\n2. Checking Flask API...\")\n",
    "    try:\n",
    "        response = requests.get('http://localhost:5000/api/models', timeout=5)\n",
    "        print(f\"Flask API status: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Flask API error: {e}\")\n",
    "    \n",
    "    # Test with a simple model request\n",
    "    print(\"\\n3. Testing direct Ollama API...\")\n",
    "    try:\n",
    "        test_request = {\n",
    "            'model': 'gemma3:4b',\n",
    "            'prompt': 'Hello',\n",
    "            'stream': False\n",
    "        }\n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json=test_request, timeout=30)\n",
    "        print(f\"Direct Ollama test: {response.status_code}\")\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Direct Ollama test failed: {e}\")\n",
    "\n",
    "# Run diagnosis\n",
    "diagnose_api_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0757ee",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. **Model Not Downloaded**\n",
    "Gemma3:4b model might not be pulled yet:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecb13deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if gemma3:4b model is available...\n",
      "âŒ gemma3:4b model not found\n",
      "Available models:\n",
      "\n",
      "Pulling gemma3:4b model (this may take several minutes)...\n",
      "âŒ Failed to pull model\n",
      "'ollama' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensure_model_available():\n",
    "    \"\"\"Ensure the Gemma model is available\"\"\"\n",
    "    print(\"Checking if gemma3:4b model is available...\")\n",
    "    \n",
    "    try:\n",
    "        # List available models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if 'gemma3:4b' in result.stdout:\n",
    "            print(\"âœ… gemma3:4b model is available\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ gemma3:4b model not found\")\n",
    "            print(\"Available models:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            print(\"Pulling gemma3:4b model (this may take several minutes)...\")\n",
    "            pull_result = subprocess.run(['ollama', 'pull', 'gemma3:4b'], \n",
    "                                       capture_output=True, text=True, shell=True)\n",
    "            \n",
    "            if pull_result.returncode == 0:\n",
    "                print(\"âœ… Model pulled successfully\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ Failed to pull model\")\n",
    "                print(pull_result.stderr)\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking models: {e}\")\n",
    "        return False\n",
    "\n",
    "ensure_model_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac66150",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. **Enhanced Error Handling in Flask API**\n",
    "Add better error handling to see exactly what's failing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f65e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "class OllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST'])\n",
    "        def generate():\n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'success': False, 'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                prompt = data.get('prompt', '')\n",
    "                model = data.get('model', 'gemma3:4b')\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'success': False, 'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"Generating response for prompt: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection first\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'success': False, \n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'success': False, \n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API\n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json={\n",
    "                                           'model': model,\n",
    "                                           'prompt': prompt,\n",
    "                                           'stream': False\n",
    "                                       }, timeout=60)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return jsonify({\n",
    "                        'success': True,\n",
    "                        'response': result.get('response', ''),\n",
    "                        'model': model\n",
    "                    })\n",
    "                else:\n",
    "                    return jsonify({\n",
    "                        'success': False,\n",
    "                        'error': f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    }), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'success': False,\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'traceback': error_traceback\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint\"\"\"\n",
    "            try:\n",
    "                # Check Ollama connection\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                return jsonify({\n",
    "                    'flask_api': 'running',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'ollama_status_code': ollama_response.status_code\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'flask_api': 'running',\n",
    "                    'ollama_server': 'not_reachable',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    def run(self, host='localhost', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=True)\n",
    "\n",
    "# Create and start the enhanced API server\n",
    "api = OllamaAPI()\n",
    "\n",
    "def start_api_server():\n",
    "    api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "server_thread = threading.Thread(target=start_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"Enhanced API server started on http://localhost:5000\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Test health endpoint\n",
    "try:\n",
    "    health_response = requests.get('http://localhost:5000/api/health')\n",
    "    print(f\"Health check: {health_response.json()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Health check failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc109e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. **Updated Test Function**\n",
    "Use a more robust test function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a737a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_api_enhanced():\n",
    "    \"\"\"Enhanced API test with better error reporting\"\"\"\n",
    "    print(\"Testing API with enhanced error reporting...\")\n",
    "    \n",
    "    test_data = {\n",
    "        'prompt': 'What is 2+2?',\n",
    "        'model': 'gemma3:4b'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Sending request to Flask API...\")\n",
    "        response = requests.post('http://127.0.0.1:5000/api/generate', \n",
    "                               json=test_data, timeout=60)\n",
    "        \n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"âœ… API Test Successful!\")\n",
    "            print(f\"Response: {result.get('response', 'No response')}\")\n",
    "        else:\n",
    "            print(f\"âŒ API Test Failed: {response.status_code}\")\n",
    "            try:\n",
    "                error_detail = response.json()\n",
    "                print(f\"Error details: {error_detail}\")\n",
    "            except:\n",
    "                print(f\"Raw response: {response.text}\")\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"âŒ Request timed out - model might be loading\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API Test Error: {e}\")\n",
    "\n",
    "# Wait for server to start, then test\n",
    "time.sleep(5)\n",
    "test_api_enhanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5782c22",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run these diagnostic functions to identify the exact cause of the 500 error. The most common fix is ensuring Ollama server is running and the model is downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c793751",
   "metadata": {},
   "source": [
    "Collecting workspace informationYou can check if the Gemma model is successfully pulled from Ollama using several methods. Here are the most effective ways:\n",
    "\n",
    "## 1. **Check Available Models with `ollama list`**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7787e7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully retrieved model list:\n",
      "NAME         ID              SIZE      MODIFIED    \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    5 hours ago    \n",
      "gemma3:1b    8648f39daa8f    815 MB    5 hours ago    \n",
      "\n",
      "âœ… gemma3:4b model is available!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def check_pulled_models():\n",
    "    \"\"\"Check all models pulled by Ollama\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Successfully retrieved model list:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Check specifically for gemma3:4b\n",
    "            if 'gemma3:4b' in result.stdout:\n",
    "                print(\"âœ… gemma3:4b model is available!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ gemma3:4b model not found in the list\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"âŒ Error running ollama list: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking models: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check models\n",
    "check_pulled_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb5b7b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Use Ollama API to List Models**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "effa57cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Available models via API:\n",
      "  - gemma3:4b (Size: 3,338,801,804 bytes, Modified: 2025-05-30T10:55:01.5170616+08:00)\n",
      "  - gemma3:1b (Size: 815,319,791 bytes, Modified: 2025-05-30T10:45:20.8759414+08:00)\n",
      "âœ… gemma3:4b model found!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_models_via_api():\n",
    "    \"\"\"Check models using Ollama's REST API\"\"\"\n",
    "    try:\n",
    "        response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            models_data = response.json()\n",
    "            models = models_data.get('models', [])\n",
    "            \n",
    "            print(\"âœ… Available models via API:\")\n",
    "            for model in models:\n",
    "                name = model.get('name', 'Unknown')\n",
    "                size = model.get('size', 0)\n",
    "                modified = model.get('modified_at', 'Unknown')\n",
    "                print(f\"  - {name} (Size: {size:,} bytes, Modified: {modified})\")\n",
    "            \n",
    "            # Check for gemma3:4b specifically\n",
    "            gemma_models = [m for m in models if 'gemma3:4b' in m.get('name', '')]\n",
    "            if gemma_models:\n",
    "                print(\"âœ… gemma3:4b model found!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"âŒ gemma3:4b model not found\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"âŒ API request failed: {response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama server. Make sure it's running.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking models via API: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check via API\n",
    "check_models_via_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d64e0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Test Model with Simple Generation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a18cd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model generation...\n",
      "âœ… Model generation test successful!\n",
      "Prompt: Hello, say hi back to me.\n",
      "Response: Hello to you too! ðŸ˜Š How's your day going so far?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model_generation():\n",
    "    \"\"\"Test if the model can actually generate text\"\"\"\n",
    "    try:\n",
    "        test_request = {\n",
    "            # 'model': 'gemma3:4b',\n",
    "            'model': 'gemma3:1b',\n",
    "            'prompt': 'Hello, say hi back to me.',\n",
    "            'stream': False\n",
    "        }\n",
    "        \n",
    "        print(\"Testing model generation...\")\n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json=test_request, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            generated_text = result.get('response', '')\n",
    "            print(\"âœ… Model generation test successful!\")\n",
    "            print(f\"Prompt: {test_request['prompt']}\")\n",
    "            print(f\"Response: {generated_text}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Model generation failed: {response.status_code}\")\n",
    "            print(f\"Error: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error testing model generation: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test generation\n",
    "test_model_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213053d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Complete Model Status Check Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f62034c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "COMPLETE GEMMA3:4B MODEL STATUS CHECK\n",
      "==================================================\n",
      "\n",
      "1. Checking Ollama server...\n",
      "âœ… Ollama server is running\n",
      "\n",
      "2. Checking models via command line...\n",
      "âœ… Successfully retrieved model list:\n",
      "NAME         ID              SIZE      MODIFIED       \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    32 minutes ago    \n",
      "gemma3:1b    8648f39daa8f    815 MB    41 minutes ago    \n",
      "\n",
      "âœ… gemma3:4b model is available!\n",
      "\n",
      "3. Checking models via API...\n",
      "âœ… Available models via API:\n",
      "  - gemma3:4b (Size: 3,338,801,804 bytes, Modified: 2025-05-30T10:55:01.5170616+08:00)\n",
      "  - gemma3:1b (Size: 815,319,791 bytes, Modified: 2025-05-30T10:45:20.8759414+08:00)\n",
      "âœ… gemma3:4b model found!\n",
      "\n",
      "4. Testing model generation...\n",
      "Testing model generation...\n",
      "âœ… Model generation test successful!\n",
      "Prompt: Hello, say hi back to me.\n",
      "Response: Hi there! ðŸ‘‹ How can I help you today?\n",
      "\n",
      "==================================================\n",
      "SUMMARY:\n",
      "Command line check: âœ… PASS\n",
      "API check: âœ… PASS\n",
      "Generation test: âœ… PASS\n",
      "ðŸŽ‰ gemma3:4b model is fully functional!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complete_model_check():\n",
    "    \"\"\"Complete check for gemma3:4b model availability and functionality\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"COMPLETE GEMMA3:4B MODEL STATUS CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Check if Ollama is running\n",
    "    print(\"\\n1. Checking Ollama server...\")\n",
    "    try:\n",
    "        health = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        if health.status_code == 200:\n",
    "            print(\"âœ… Ollama server is running\")\n",
    "        else:\n",
    "            print(\"âŒ Ollama server error\")\n",
    "            return False\n",
    "    except:\n",
    "        print(\"âŒ Ollama server not reachable\")\n",
    "        return False\n",
    "    \n",
    "    # Step 2: Check model list via command\n",
    "    print(\"\\n2. Checking models via command line...\")\n",
    "    cmd_result = check_pulled_models()\n",
    "    \n",
    "    # Step 3: Check model list via API\n",
    "    print(\"\\n3. Checking models via API...\")\n",
    "    api_result = check_models_via_api()\n",
    "    \n",
    "    # Step 4: Test actual generation\n",
    "    print(\"\\n4. Testing model generation...\")\n",
    "    gen_result = test_model_generation()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"Command line check: {'âœ… PASS' if cmd_result else 'âŒ FAIL'}\")\n",
    "    print(f\"API check: {'âœ… PASS' if api_result else 'âŒ FAIL'}\")\n",
    "    print(f\"Generation test: {'âœ… PASS' if gen_result else 'âŒ FAIL'}\")\n",
    "    \n",
    "    if all([cmd_result, api_result, gen_result]):\n",
    "        print(\"ðŸŽ‰ gemma3:4b model is fully functional!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"âš ï¸ There are issues with the gemma3:4b model\")\n",
    "        return False\n",
    "\n",
    "# Run complete check\n",
    "complete_model_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9270a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **If Model is Not Found, Pull It**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_gemma_model():\n",
    "    \"\"\"Ensure gemma3:4b model is available, pull if necessary\"\"\"\n",
    "    if not complete_model_check():\n",
    "        print(\"\\nðŸ”„ Model not found or not working. Attempting to pull...\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Pulling gemma3:4b model (this may take several minutes)...\")\n",
    "            pull_process = subprocess.Popen(\n",
    "                ['ollama', 'pull', 'gemma3:4b'],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                shell=True\n",
    "            )\n",
    "            \n",
    "            # Show progress\n",
    "            while True:\n",
    "                output = pull_process.stdout.readline()\n",
    "                if output == '' and pull_process.poll() is not None:\n",
    "                    break\n",
    "                if output:\n",
    "                    print(output.strip())\n",
    "            \n",
    "            if pull_process.returncode == 0:\n",
    "                print(\"âœ… Model pulled successfully!\")\n",
    "                # Check again\n",
    "                return complete_model_check()\n",
    "            else:\n",
    "                print(\"âŒ Failed to pull model\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error pulling model: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"âœ… Model is already available and working!\")\n",
    "        return True\n",
    "\n",
    "# Ensure model is available\n",
    "ensure_gemma_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bf15ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Run these functions in your localLlmWIntelGPU.ipynb notebook to verify that the Gemma model is successfully pulled and working. The complete check will tell you exactly what's working and what needs to be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ddf5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Oxygen XML Editor Integration\n",
    "\n",
    "For Oxygen XML Editor, you can create a connector using the REST API:\n",
    "\n",
    "**Connector Configuration:**\n",
    "- **URL**: `http://localhost:5000/api/generate`\n",
    "- **Method**: POST\n",
    "- **Content-Type**: application/json\n",
    "- **Request Body**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa264b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"prompt\": \"${prompt}\",\n",
    "    \"model\": \"gemma:2b\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84544a1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Usage in Oxygen XML Editor:**\n",
    "1. Go to **Options** > **Preferences** > **External Tools**\n",
    "2. Create a new external tool with the API endpoint\n",
    "3. Configure parameters to pass selected text as prompt\n",
    "4. Set up response handling to insert generated content\n",
    "\n",
    "## 6. Enhanced API with Intel GPU Support\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26941f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Intel Extension for PyTorch...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m setup_intel_gpu()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Enhanced API endpoint with performance monitoring\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;129m@api\u001b[39m\u001b[43m.\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate/enhanced\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_enhanced\u001b[39m():\n\u001b[32m     18\u001b[39m     start_time = time.time()\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\scaffold.py:46\u001b[39m, in \u001b[36msetupmethod.<locals>.wrapper_func\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_func\u001b[39m(\u001b[38;5;28mself\u001b[39m: Scaffold, *args: t.Any, **kwargs: t.Any) -> t.Any:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_setup_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\app.py:415\u001b[39m, in \u001b[36mApp._check_setup_finished\u001b[39m\u001b[34m(self, f_name)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_setup_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m, f_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._got_first_request:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    416\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe setup method \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m can no longer be called\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m on the application. It has already handled its first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m request, any changes will not be applied\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m consistently.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    420\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure all imports, decorators, functions, etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m needed to set up the application are done before\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m running it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         )\n",
      "\u001b[31mAssertionError\u001b[39m: The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it."
     ]
    }
   ],
   "source": [
    "# For Intel GPU optimization, ensure you have Intel Extension for PyTorch\n",
    "def setup_intel_gpu():\n",
    "    \"\"\"Setup Intel GPU acceleration for better performance\"\"\"\n",
    "    try:\n",
    "        import intel_extension_for_pytorch as ipex\n",
    "        print(\"Intel Extension for PyTorch available\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Installing Intel Extension for PyTorch...\")\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', 'intel_extension_for_pytorch'])\n",
    "        return False\n",
    "\n",
    "setup_intel_gpu()\n",
    "\n",
    "# Enhanced API endpoint with performance monitoring\n",
    "@api.app.route('/api/generate/enhanced', methods=['POST'])\n",
    "def generate_enhanced():\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        prompt = data.get('prompt', '')\n",
    "        model = data.get('model', 'gemma:2b')\n",
    "        max_tokens = data.get('max_tokens', 100)\n",
    "        \n",
    "        response = requests.post('http://localhost:11434/api/generate', \n",
    "                               json={\n",
    "                                   'model': model,\n",
    "                                   'prompt': prompt,\n",
    "                                   'stream': False,\n",
    "                                   'options': {\n",
    "                                       'num_predict': max_tokens\n",
    "                                   }\n",
    "                               })\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return jsonify({\n",
    "                'success': True,\n",
    "                'response': result.get('response', ''),\n",
    "                'model': model,\n",
    "                'processing_time': processing_time,\n",
    "                'tokens_generated': len(result.get('response', '').split())\n",
    "            })\n",
    "        else:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'error': 'Failed to generate response',\n",
    "                'processing_time': processing_time\n",
    "            }), 500\n",
    "            \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'processing_time': time.time() - start_time\n",
    "        }), 500\n",
    "\n",
    "print(\"Enhanced API endpoint added: POST /api/generate/enhanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaadf8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This setup provides a complete solution for running Ollama with Gemma model locally and exposing it via REST API for Oxygen XML Editor integration. The API runs on port 5000 and provides endpoints for text generation and model management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680eb107",
   "metadata": {},
   "source": [
    "You can remove models from Ollama using several methods. Here are the most effective ways:\n",
    "\n",
    "## 1. **Using Ollama Command Line**\n",
    "\n",
    "The simplest method is using the `ollama rm` command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6722ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing model: gemma3:latest\n",
      "âŒ Failed to remove model gemma3:latest\n",
      "Error: \u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hError: model 'gemma3:latest' not found\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def remove_ollama_model(model_name):\n",
    "    \"\"\"Remove a specific model from Ollama\"\"\"\n",
    "    try:\n",
    "        print(f\"Removing model: {model_name}\")\n",
    "        result = subprocess.run(['ollama', 'rm', model_name], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Model {model_name} removed successfully!\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Failed to remove model {model_name}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error removing model: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example: Remove gemma3:4b model\n",
    "remove_ollama_model('gemma3:latest')\n",
    "\n",
    "# # Example: Remove gemma3:1b model\n",
    "# remove_ollama_model('gemma3:1b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03073e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **List and Remove Models Interactively**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d69059b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "NAME         ID              SIZE      MODIFIED      \n",
      "gemma3:1b    8648f39daa8f    815 MB    9 minutes ago    \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    10 hours ago     \n",
      "\n",
      "\n",
      "Models available for removal:\n",
      "1. gemma3:1b\n",
      "2. gemma3:4b\n",
      "\n",
      "Found Gemma models: ['gemma3:1b', 'gemma3:4b']\n",
      "Removing model: gemma3:1b\n",
      "âœ… Model gemma3:1b removed successfully!\n",
      "deleted 'gemma3:1b'\n",
      "\n",
      "Removing model: gemma3:4b\n",
      "âœ… Model gemma3:4b removed successfully!\n",
      "deleted 'gemma3:4b'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def list_and_remove_models():\n",
    "    \"\"\"List available models and allow interactive removal\"\"\"\n",
    "    try:\n",
    "        # First, list all available models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"Available models:\")\n",
    "            print(result.stdout)\n",
    "            \n",
    "            # Parse model names from output\n",
    "            lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n",
    "            models = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    model_name = line.split()[0]\n",
    "                    models.append(model_name)\n",
    "            \n",
    "            if not models:\n",
    "                print(\"No models found to remove.\")\n",
    "                return\n",
    "            \n",
    "            print(\"\\nModels available for removal:\")\n",
    "            for i, model in enumerate(models, 1):\n",
    "                print(f\"{i}. {model}\")\n",
    "            \n",
    "            # You can modify this to remove specific models\n",
    "            # For example, remove all gemma models:\n",
    "            gemma_models = [m for m in models if 'gemma' in m.lower()]\n",
    "            \n",
    "            if gemma_models:\n",
    "                print(f\"\\nFound Gemma models: {gemma_models}\")\n",
    "                for model in gemma_models:\n",
    "                    remove_ollama_model(model)\n",
    "            else:\n",
    "                print(\"No Gemma models found.\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"âŒ Error listing models: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Run the interactive removal\n",
    "list_and_remove_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44687b8e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Remove All Models (Clean Slate)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb8b9ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_all_models():\n",
    "    \"\"\"Remove all models from Ollama\"\"\"\n",
    "    try:\n",
    "        # Get list of all models\n",
    "        result = subprocess.run(['ollama', 'list'], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n",
    "            models = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    model_name = line.split()[0]\n",
    "                    models.append(model_name)\n",
    "            \n",
    "            if not models:\n",
    "                print(\"No models found to remove.\")\n",
    "                return\n",
    "            \n",
    "            print(f\"Found {len(models)} models to remove:\")\n",
    "            for model in models:\n",
    "                print(f\"  - {model}\")\n",
    "            \n",
    "            # Remove each model\n",
    "            for model in models:\n",
    "                print(f\"\\nRemoving {model}...\")\n",
    "                remove_result = subprocess.run(['ollama', 'rm', model], \n",
    "                                             capture_output=True, text=True, shell=True)\n",
    "                \n",
    "                if remove_result.returncode == 0:\n",
    "                    print(f\"âœ… {model} removed successfully\")\n",
    "                else:\n",
    "                    print(f\"âŒ Failed to remove {model}: {remove_result.stderr}\")\n",
    "            \n",
    "            print(\"\\nâœ… All models removal process completed!\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ Error listing models: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "# Uncomment to remove all models (use with caution!)\n",
    "# remove_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6a6ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Check Storage Space After Removal**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67090f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Ollama storage usage:\n",
      "ðŸ“ C:\\Users\\jeffw/.ollama: 3.87 GB\n",
      "ðŸ“ C:\\Users\\jeffw/AppData/Local/ollama: 0.96 GB\n",
      "ðŸ“ C:\\Users\\jeffw/AppData/Roaming/ollama: Not found\n",
      "ðŸ“ C:\\Users\\jeffw\\AppData\\Local\\ollama: 0.96 GB\n"
     ]
    }
   ],
   "source": [
    "def check_ollama_storage():\n",
    "    \"\"\"Check storage used by Ollama after removal\"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    \n",
    "    # Common Ollama data directories\n",
    "    possible_paths = [\n",
    "        os.path.expanduser(\"~/.ollama\"),\n",
    "        os.path.expanduser(\"~/AppData/Local/ollama\"),\n",
    "        os.path.expanduser(\"~/AppData/Roaming/ollama\"),\n",
    "        \"C:\\\\Users\\\\{}\\\\AppData\\\\Local\\\\ollama\".format(os.getenv('USERNAME'))\n",
    "    ]\n",
    "    \n",
    "    print(\"Checking Ollama storage usage:\")\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                total_size = 0\n",
    "                for dirpath, dirnames, filenames in os.walk(path):\n",
    "                    for filename in filenames:\n",
    "                        filepath = os.path.join(dirpath, filename)\n",
    "                        total_size += os.path.getsize(filepath)\n",
    "                \n",
    "                size_gb = total_size / (1024**3)\n",
    "                print(f\"ðŸ“ {path}: {size_gb:.2f} GB\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error checking {path}: {e}\")\n",
    "        else:\n",
    "            print(f\"ðŸ“ {path}: Not found\")\n",
    "\n",
    "# Check storage usage\n",
    "check_ollama_storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc1ec8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Complete Cleanup Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_ollama_cleanup():\n",
    "    \"\"\"Complete cleanup of Ollama models and verification\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"COMPLETE OLLAMA CLEANUP\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: List current models\n",
    "    print(\"\\n1. Current models:\")\n",
    "    list_result = subprocess.run(['ollama', 'list'], \n",
    "                                capture_output=True, text=True, shell=True)\n",
    "    if list_result.returncode == 0:\n",
    "        print(list_result.stdout)\n",
    "    else:\n",
    "        print(\"No models or error listing models\")\n",
    "    \n",
    "    # Step 2: Remove all models\n",
    "    print(\"\\n2. Removing all models...\")\n",
    "    remove_all_models()\n",
    "    \n",
    "    # Step 3: Verify removal\n",
    "    print(\"\\n3. Verifying removal...\")\n",
    "    verify_result = subprocess.run(['ollama', 'list'], \n",
    "                                  capture_output=True, text=True, shell=True)\n",
    "    if verify_result.returncode == 0:\n",
    "        lines = verify_result.stdout.strip().split('\\n')[1:]\n",
    "        remaining_models = [line for line in lines if line.strip()]\n",
    "        \n",
    "        if not remaining_models:\n",
    "            print(\"âœ… All models successfully removed!\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ {len(remaining_models)} models still present:\")\n",
    "            print(verify_result.stdout)\n",
    "    \n",
    "    # Step 4: Check storage\n",
    "    print(\"\\n4. Storage check after cleanup:\")\n",
    "    check_ollama_storage()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"CLEANUP COMPLETE\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Run complete cleanup (uncomment to use)\n",
    "# complete_ollama_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62be7d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Quick Commands Summary:\n",
    "\n",
    "**Remove specific model:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e9575",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "ollama rm gemma3:4b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8557b31",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Remove multiple models:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f04424d",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "ollama rm gemma3:4b gemma3:1b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22923d3c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**List models before removal:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496ba4e",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aef46a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use these functions to safely remove models from Ollama. The `complete_ollama_cleanup()` function will remove all models if you want to start fresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853d0e1",
   "metadata": {},
   "source": [
    "Based on your Jupyter notebook and the Oxygen AI Positron documentation, here are the necessary steps to integrate your Ollama + Gemma3:1b RESTful service with Oxygen XML Editor:\n",
    "\n",
    "## 1. **Ensure Your Ollama RESTful Service is Running**\n",
    "\n",
    "First, make sure your Flask API is running from your notebook:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2474c62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama server: 200\n",
      "âœ… Flask API: 404\n",
      "âœ… Generation test successful!\n",
      "Response: Hello! Thanks for the test. How can I help you today?...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify your API is running\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def verify_ollama_service():\n",
    "    \"\"\"Verify both Ollama and Flask API are running\"\"\"\n",
    "    try:\n",
    "        # Check Ollama server\n",
    "        ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "        print(f\"âœ… Ollama server: {ollama_response.status_code}\")\n",
    "        \n",
    "        # Check Flask API\n",
    "        flask_response = requests.get('http://localhost:5000/api/health', timeout=5)\n",
    "        print(f\"âœ… Flask API: {flask_response.status_code}\")\n",
    "        \n",
    "        # Test generation with gemma3:1b\n",
    "        test_data = {\n",
    "            'prompt': 'Hello, this is a test.',\n",
    "            'model': 'gemma3:1b'\n",
    "        }\n",
    "        \n",
    "        gen_response = requests.post('http://localhost:5000/api/generate', \n",
    "                                   json=test_data, timeout=30)\n",
    "        \n",
    "        if gen_response.status_code == 200:\n",
    "            result = gen_response.json()\n",
    "            print(\"âœ… Generation test successful!\")\n",
    "            print(f\"Response: {result.get('response', '')[:]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Generation test failed: {gen_response.status_code}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Service verification failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run verification\n",
    "verify_ollama_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b561d949",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Create Oxygen AI Positron Custom Connector Configuration**\n",
    "\n",
    "Create a JSON configuration file for your custom connector:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6deb2e",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"name\": \"Local Ollama Gemma3:1b\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model for text generation\",\n",
    "  \"endpoint\": \"http://localhost:5000/api/generate\",\n",
    "  \"method\": \"POST\",\n",
    "  \"headers\": {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\"\n",
    "  },\n",
    "  \"requestBody\": {\n",
    "    \"prompt\": \"${input}\",\n",
    "    \"model\": \"gemma3:1b\",\n",
    "    \"max_tokens\": 1000\n",
    "  },\n",
    "  \"responseMapping\": {\n",
    "    \"textPath\": \"$.response\",\n",
    "    \"errorPath\": \"$.error\"\n",
    "  },\n",
    "  \"timeout\": 60000,\n",
    "  \"streaming\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834438fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Install and Configure Oxygen AI Positron Add-on**\n",
    "\n",
    "### Step 3.1: Install the Add-on\n",
    "1. Open Oxygen XML Editor\n",
    "2. Go to **Help** > **Install new add-ons**\n",
    "3. Add the update site: `https://www.oxygenxml.com/InstData/Addons/default/updateSite.xml`\n",
    "4. Search for \"AI Positron\" and install it\n",
    "5. Restart Oxygen XML Editor\n",
    "\n",
    "### Step 3.2: Configure the Custom Connector\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Click **Add Custom Connector**\n",
    "3. Provide the configuration details:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d407ac8",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"connectorName\": \"Local Ollama Gemma3:1b\",\n",
    "  \"baseURL\": \"http://localhost:5000\",\n",
    "  \"apiKey\": \"\",\n",
    "  \"requestConfig\": {\n",
    "    \"endpoint\": \"/api/generate\",\n",
    "    \"method\": \"POST\",\n",
    "    \"headers\": {\n",
    "      \"Content-Type\": \"application/json\"\n",
    "    },\n",
    "    \"bodyTemplate\": {\n",
    "      \"prompt\": \"${prompt}\",\n",
    "      \"model\": \"gemma3:1b\"\n",
    "    }\n",
    "  },\n",
    "  \"responseConfig\": {\n",
    "    \"textFieldPath\": \"response\",\n",
    "    \"errorFieldPath\": \"error\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181868d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Enhanced Flask API for Oxygen Integration**\n",
    "\n",
    "Update your Flask API to be more compatible with Oxygen AI Positron:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f036cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Previous server detected. Starting new server...\n",
      "ðŸš€ Oxygen-compatible API server started on http://localhost:5000\n",
      "Available endpoints:\n",
      "- POST /api/generate - Generate text (Oxygen AI Positron endpoint)\n",
      "- GET /api/health - Health check\n",
      "- GET /api/models - List available models\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test successful!\n",
      "Response: ...\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "class OxygenOllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        CORS(self.app)  # Enable CORS for Oxygen XML Editor\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST', 'OPTIONS'])\n",
    "        def generate():\n",
    "            # Handle preflight requests\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "            \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Handle both 'prompt' and 'input' fields for compatibility\n",
    "                prompt = data.get('prompt') or data.get('input', '')\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                max_tokens = data.get('max_tokens', 500)\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"ðŸ¤– Processing request: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': max_tokens,\n",
    "                        'temperature': 0.7\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # Format response for Oxygen AI Positron\n",
    "                    oxygen_response = {\n",
    "                        'response': generated_text,\n",
    "                        'model': model,\n",
    "                        'success': True,\n",
    "                        'metadata': {\n",
    "                            'tokens_generated': len(generated_text.split()),\n",
    "                            'model_used': model\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… Generated {len(generated_text)} characters\")\n",
    "                    return jsonify(oxygen_response)\n",
    "                else:\n",
    "                    error_msg = f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    print(f\"âŒ {error_msg}\")\n",
    "                    return jsonify({'error': error_msg}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"âŒ Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'success': False\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint for Oxygen\"\"\"\n",
    "            try:\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                models = ollama_response.json().get('models', []) if ollama_response.status_code == 200 else []\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'healthy',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'available_models': [m.get('name', '') for m in models],\n",
    "                    'flask_api': 'running'\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            \"\"\"List available models for Oxygen configuration\"\"\"\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    models_data = response.json()\n",
    "                    models = []\n",
    "                    for model in models_data.get('models', []):\n",
    "                        models.append({\n",
    "                            'name': model.get('name', ''),\n",
    "                            'size': model.get('size', 0),\n",
    "                            'modified': model.get('modified_at', '')\n",
    "                        })\n",
    "                    return jsonify({'models': models})\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False, threaded=True)\n",
    "\n",
    "# Create and start the enhanced API server for Oxygen\n",
    "oxygen_api = OxygenOllamaAPI()\n",
    "\n",
    "def start_oxygen_api_server():\n",
    "    oxygen_api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Stop previous server if running\n",
    "try:\n",
    "    requests.get('http://localhost:5000/api/health', timeout=1)\n",
    "    print(\"âš ï¸ Previous server detected. Starting new server...\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "server_thread = threading.Thread(target=start_oxygen_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"ðŸš€ Oxygen-compatible API server started on http://localhost:5000\")\n",
    "print(\"Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text (Oxygen AI Positron endpoint)\")\n",
    "print(\"- GET /api/health - Health check\")\n",
    "print(\"- GET /api/models - List available models\")\n",
    "\n",
    "# Test the enhanced API\n",
    "time.sleep(3)\n",
    "test_data = {\n",
    "    'input': 'Write a brief summary about XML editing.',\n",
    "    'model': 'gemma3:1b'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post('http://localhost:5000/api/generate', json=test_data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\nâœ… Test successful!\")\n",
    "        print(f\"Response: {result.get('response', '')[:200]}...\")\n",
    "    else:\n",
    "        print(f\"âŒ Test failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666f791",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Configure Oxygen AI Positron Settings**\n",
    "\n",
    "In Oxygen XML Editor:\n",
    "\n",
    "### Step 5.1: Basic Configuration\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Set the following:\n",
    "   - **Provider**: Custom\n",
    "   - **Base URL**: `http://localhost:5000`\n",
    "   - **API Key**: Leave empty (not needed for local server)\n",
    "\n",
    "### Step 5.2: Advanced Configuration\n",
    "Create a connector configuration file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775d1a7",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"local-ollama-gemma\",\n",
    "  \"name\": \"Local Ollama Gemma3:1b\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model\",\n",
    "  \"type\": \"custom\",\n",
    "  \"config\": {\n",
    "    \"baseUrl\": \"http://localhost:5000\",\n",
    "    \"endpoints\": {\n",
    "      \"generate\": {\n",
    "        \"path\": \"/api/generate\",\n",
    "        \"method\": \"POST\",\n",
    "        \"headers\": {\n",
    "          \"Content-Type\": \"application/json\"\n",
    "        },\n",
    "        \"requestBodyTemplate\": {\n",
    "          \"input\": \"${prompt}\",\n",
    "          \"model\": \"gemma3:1b\",\n",
    "          \"max_tokens\": 1000\n",
    "        },\n",
    "        \"responseMapping\": {\n",
    "          \"textPath\": \"$.response\",\n",
    "          \"errorPath\": \"$.error\"\n",
    "        }\n",
    "      },\n",
    "      \"health\": {\n",
    "        \"path\": \"/api/health\",\n",
    "        \"method\": \"GET\"\n",
    "      }\n",
    "    },\n",
    "    \"timeout\": 60000,\n",
    "    \"retryAttempts\": 2\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec0bbc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. **Create Custom Actions in Oxygen**\n",
    "\n",
    "### Step 6.1: Document Actions\n",
    "1. Go to **Options** > **Menu Shortcut Keys**\n",
    "2. Create new actions for common AI tasks:\n",
    "\n",
    "#### Action 1: Summarize Selected Text\n",
    "- **Name**: \"AI Summarize\"\n",
    "- **Shortcut**: `Ctrl+Alt+S`\n",
    "- **Operation**: Call AI Positron with prompt: \"Summarize the following text: ${selection}\"\n",
    "\n",
    "#### Action 2: Improve Writing\n",
    "- **Name**: \"AI Improve Writing\"\n",
    "- **Shortcut**: `Ctrl+Alt+I`  \n",
    "- **Operation**: Call AI Positron with prompt: \"Improve the writing and clarity of: ${selection}\"\n",
    "\n",
    "#### Action 3: Generate Documentation\n",
    "- **Name**: \"AI Generate Docs\"\n",
    "- **Shortcut**: `Ctrl+Alt+D`\n",
    "- **Operation**: Call AI Positron with prompt: \"Generate documentation for: ${selection}\"\n",
    "\n",
    "### Step 6.2: Create Custom Framework\n",
    "Create a custom framework file `oxygen-ai-actions.framework`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dac8b",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<serialized version=\"24.1\" xml:space=\"preserve\">\n",
    "    <serializableOrderedMap>\n",
    "        <entry>\n",
    "            <String>document.types</String>\n",
    "            <documentTypeDescriptor-array>\n",
    "                <documentTypeDescriptor>\n",
    "                    <field name=\"name\">\n",
    "                        <String>AI Enhanced XML</String>\n",
    "                    </field>\n",
    "                    <field name=\"description\">\n",
    "                        <String>XML editing with AI assistance</String>\n",
    "                    </field>\n",
    "                    <field name=\"priority\">\n",
    "                        <Integer>3</Integer>\n",
    "                    </field>\n",
    "                    <field name=\"authorActions\">\n",
    "                        <authorAction-array>\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.summarize</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Summarize</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Summarize selected text using AI</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ai.positron.generate</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Summarize this text: ${selection}</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                        </authorAction-array>\n",
    "                    </field>\n",
    "                </documentTypeDescriptor>\n",
    "            </documentTypeDescriptor-array>\n",
    "        </entry>\n",
    "    </serializableOrderedMap>\n",
    "</serialized>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ee6124",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. **Test the Integration**\n",
    "\n",
    "### Step 7.1: Basic Test\n",
    "1. Open an XML document in Oxygen\n",
    "2. Select some text\n",
    "3. Use **AI Positron** > **Generate** or your custom action\n",
    "4. Verify the response is inserted correctly\n",
    "\n",
    "### Step 7.2: Advanced Test Script\n",
    "Create a test validation script:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67bafe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Oxygen AI Positron Integration\n",
      "==================================================\n",
      "\n",
      "Test 1: Simple generation\n",
      "âš ï¸ PARTIAL - Short response: 0 chars\n",
      "\n",
      "Test 2: Code explanation\n",
      "âš ï¸ PARTIAL - Short response: 0 chars\n",
      "\n",
      "Test 3: Documentation generation\n",
      "âš ï¸ PARTIAL - Short response: 0 chars\n",
      "\n",
      "==================================================\n",
      "âœ… Integration test completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_oxygen_integration():\n",
    "    \"\"\"Test the integration with various prompts\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple generation\",\n",
    "            \"prompt\": \"What is XML?\",\n",
    "            \"expected_length\": 50\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Code explanation\",\n",
    "            \"prompt\": \"Explain this XML: <book><title>Example</title></book>\",\n",
    "            \"expected_length\": 100\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Documentation generation\",\n",
    "            \"prompt\": \"Generate documentation for an XML schema\",\n",
    "            \"expected_length\": 200\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ§ª Testing Oxygen AI Positron Integration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test['name']}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.post('http://localhost:5000/api/generate', \n",
    "                                   json={\n",
    "                                       'input': test['prompt'],\n",
    "                                       'model': 'gemma3:1b'\n",
    "                                   }, \n",
    "                                   timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                generated_text = result.get('response', '')\n",
    "                \n",
    "                if len(generated_text) >= test['expected_length']:\n",
    "                    print(f\"âœ… PASS - Generated {len(generated_text)} characters\")\n",
    "                    print(f\"Preview: {generated_text[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸ PARTIAL - Short response: {len(generated_text)} chars\")\n",
    "            else:\n",
    "                print(f\"âŒ FAIL - HTTP {response.status_code}\")\n",
    "                print(response.text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… Integration test completed!\")\n",
    "\n",
    "# Run the test\n",
    "test_oxygen_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5afd37",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. **Troubleshooting Common Issues**\n",
    "\n",
    "### Issue 1: CORS Errors\n",
    "If you get CORS errors in Oxygen:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2fc15b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x1e230fd95b0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enhanced CORS configuration\n",
    "from flask_cors import CORS\n",
    "\n",
    "# Add this to your Flask app initialization\n",
    "CORS(oxygen_api.app, \n",
    "     origins=['*'],\n",
    "     methods=['GET', 'POST', 'OPTIONS'],\n",
    "     allow_headers=['Content-Type', 'Authorization', 'X-Requested-With'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fad26e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Issue 2: Timeout Issues\n",
    "Increase timeout values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cc2dc",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"timeout\": 120000,\n",
    "  \"retryAttempts\": 3,\n",
    "  \"retryDelay\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4581f45",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Issue 3: Model Not Found\n",
    "Verify your model is available:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "17dfecdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "NAME         ID              SIZE      MODIFIED    \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    5 hours ago    \n",
      "gemma3:1b    8648f39daa8f    815 MB    5 hours ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check available models\n",
    "def check_models():\n",
    "    result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, shell=True)\n",
    "    print(\"Available models:\")\n",
    "    print(result.stdout)\n",
    "\n",
    "check_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eef506",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This setup provides a complete integration between your local Ollama Gemma3:1b service and Oxygen XML Editor via the AI Positron add-on, allowing you to use AI assistance directly within your XML editing workflow.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbab512",
   "metadata": {},
   "source": [
    "Based on your Jupyter notebook and the Oxygen AI Positron 5.0 documentation with the sample plugin, here's a comprehensive guide to integrate your Ollama + Gemma3:1b RESTful service with Oxygen XML Editor:\n",
    "\n",
    "## 1. **Prepare Your Ollama RESTful Service for Oxygen Integration**\n",
    "\n",
    "First, enhance your existing Flask API to be fully compatible with Oxygen AI Positron 5.0:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90cf0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Oxygen AI Positron 5.0 compatible API server started on http://localhost:5000\n",
      "Available endpoints:\n",
      "- POST /api/generate - Generate text (primary endpoint)\n",
      "- POST /api/chat/completions - OpenAI-compatible endpoint\n",
      "- GET /api/health - Health check\n",
      "- GET /api/models - List available models\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… API Test successful!\n",
      "Response: ...\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "class OxygenOllamaAPI:\n",
    "    def __init__(self):\n",
    "        self.app = Flask(__name__)\n",
    "        # Enhanced CORS configuration for Oxygen XML Editor\n",
    "        CORS(self.app, \n",
    "             origins=['*'],\n",
    "             methods=['GET', 'POST', 'OPTIONS', 'PUT', 'DELETE'],\n",
    "             allow_headers=['Content-Type', 'Authorization', 'X-Requested-With', 'Accept'],\n",
    "             expose_headers=['Content-Type', 'Authorization'],\n",
    "             supports_credentials=False)\n",
    "        self.setup_routes()\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST', 'OPTIONS'])\n",
    "        def generate():\n",
    "            # Handle CORS preflight requests\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-Requested-With')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "            \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Handle multiple input field formats for Oxygen compatibility\n",
    "                prompt = (data.get('prompt') or \n",
    "                         data.get('input') or \n",
    "                         data.get('text') or \n",
    "                         data.get('message', ''))\n",
    "                \n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                max_tokens = data.get('max_tokens', 1000)\n",
    "                temperature = data.get('temperature', 0.7)\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"ðŸ¤– Processing Oxygen request: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API with proper options\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': max_tokens,\n",
    "                        'temperature': temperature,\n",
    "                        'top_k': 40,\n",
    "                        'top_p': 0.9\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # Format response for Oxygen AI Positron compatibility\n",
    "                    # Following the sample plugin structure\n",
    "                    oxygen_response = {\n",
    "                        'text': generated_text,  # Primary response field for Oxygen\n",
    "                        'response': generated_text,  # Alternative field\n",
    "                        'content': generated_text,  # Another alternative\n",
    "                        'choices': [{  # OpenAI-compatible format\n",
    "                            'text': generated_text,\n",
    "                            'message': {\n",
    "                                'content': generated_text,\n",
    "                                'role': 'assistant'\n",
    "                            }\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'success': True,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'metadata': {\n",
    "                            'tokens_generated': len(generated_text.split()),\n",
    "                            'model_used': model,\n",
    "                            'processing_time': result.get('total_duration', 0)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… Generated {len(generated_text)} characters for Oxygen\")\n",
    "                    return jsonify(oxygen_response)\n",
    "                else:\n",
    "                    error_msg = f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    print(f\"âŒ {error_msg}\")\n",
    "                    return jsonify({'error': error_msg}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"âŒ Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'success': False\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/chat/completions', methods=['POST', 'OPTIONS'])\n",
    "        def chat_completions():\n",
    "            \"\"\"OpenAI-compatible endpoint for Oxygen AI Positron\"\"\"\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "                \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                messages = data.get('messages', [])\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                \n",
    "                # Extract prompt from messages\n",
    "                if messages:\n",
    "                    prompt = messages[-1].get('content', '')\n",
    "                else:\n",
    "                    prompt = data.get('prompt', '')\n",
    "                \n",
    "                # Use the same generation logic\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': data.get('max_tokens', 1000),\n",
    "                        'temperature': data.get('temperature', 0.7)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # OpenAI-compatible response format for Oxygen\n",
    "                    openai_response = {\n",
    "                        'choices': [{\n",
    "                            'message': {\n",
    "                                'role': 'assistant',\n",
    "                                'content': generated_text\n",
    "                            },\n",
    "                            'finish_reason': 'stop',\n",
    "                            'index': 0\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split()),\n",
    "                            'prompt_tokens': len(prompt.split()),\n",
    "                            'completion_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'object': 'chat.completion'\n",
    "                    }\n",
    "                    \n",
    "                    return jsonify(openai_response)\n",
    "                else:\n",
    "                    return jsonify({'error': 'Generation failed'}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint for Oxygen monitoring\"\"\"\n",
    "            try:\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                models = ollama_response.json().get('models', []) if ollama_response.status_code == 200 else []\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'healthy',\n",
    "                    'service': 'ollama-gemma-api',\n",
    "                    'version': '1.0.0',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'available_models': [m.get('name', '') for m in models],\n",
    "                    'flask_api': 'running',\n",
    "                    'endpoints': [\n",
    "                        '/api/generate',\n",
    "                        '/api/chat/completions',\n",
    "                        '/api/health',\n",
    "                        '/api/models'\n",
    "                    ]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        def list_models():\n",
    "            \"\"\"List available models in OpenAI-compatible format\"\"\"\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    models_data = response.json()\n",
    "                    models = []\n",
    "                    for model in models_data.get('models', []):\n",
    "                        models.append({\n",
    "                            'id': model.get('name', ''),\n",
    "                            'object': 'model',\n",
    "                            'created': 0,\n",
    "                            'owned_by': 'ollama',\n",
    "                            'name': model.get('name', ''),\n",
    "                            'size': model.get('size', 0),\n",
    "                            'modified': model.get('modified_at', '')\n",
    "                        })\n",
    "                    return jsonify({'data': models, 'object': 'list'})\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False, threaded=True)\n",
    "\n",
    "# Create and start the enhanced API server\n",
    "oxygen_api = OxygenOllamaAPI()\n",
    "\n",
    "def start_oxygen_api_server():\n",
    "    oxygen_api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_oxygen_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"ðŸš€ Oxygen AI Positron 5.0 compatible API server started on http://localhost:5000\")\n",
    "print(\"Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text (primary endpoint)\")\n",
    "print(\"- POST /api/chat/completions - OpenAI-compatible endpoint\")\n",
    "print(\"- GET /api/health - Health check\")\n",
    "print(\"- GET /api/models - List available models\")\n",
    "\n",
    "# Test the API\n",
    "time.sleep(3)\n",
    "test_data = {\n",
    "    'input': 'Write a brief summary about XML editing.',\n",
    "    'model': 'gemma3:1b'\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post('http://localhost:5000/api/generate', json=test_data, timeout=30)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"\\nâœ… API Test successful!\")\n",
    "        print(f\"Response: {result.get('text', result.get('response', ''))[:200]}...\")\n",
    "    else:\n",
    "        print(f\"âŒ API Test failed: {response.status_code}\")\n",
    "        print(response.text)\n",
    "except Exception as e:\n",
    "    print(f\"âŒ API Test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982677d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Install Oxygen AI Positron 5.0 Add-on**\n",
    "\n",
    "### Step 2.1: Install the Add-on\n",
    "1. Open Oxygen XML Editor\n",
    "2. Go to **Help** > **Install new add-ons**\n",
    "3. Add the update site: `https://www.oxygenxml.com/InstData/Addons/default/updateSite.xml`\n",
    "4. Search for \"AI Positron\" and install version 5.0+\n",
    "5. Restart Oxygen XML Editor\n",
    "\n",
    "### Step 2.2: Verify Installation\n",
    "1. Check **Options** > **Preferences** > **Plugins** for \"AI Positron\"\n",
    "2. Verify the add-on is enabled and version 5.0+\n",
    "\n",
    "## 3. **Create Custom Connector Following Sample Plugin Structure**\n",
    "\n",
    "Based on the GitHub sample plugin, create a custom connector configuration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42630a",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"local-ollama-gemma3-1b\",\n",
    "  \"name\": \"Local Ollama Gemma3:1b\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model for text generation\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"type\": \"custom\",\n",
    "  \"provider\": \"ollama\",\n",
    "  \"baseUrl\": \"http://localhost:5000\",\n",
    "  \"authentication\": {\n",
    "    \"type\": \"none\",\n",
    "    \"required\": false\n",
    "  },\n",
    "  \"capabilities\": {\n",
    "    \"textGeneration\": true,\n",
    "    \"chatCompletion\": true,\n",
    "    \"streaming\": false\n",
    "  },\n",
    "  \"endpoints\": {\n",
    "    \"textGeneration\": {\n",
    "      \"path\": \"/api/generate\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"input\": \"${prompt}\",\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.text\",\n",
    "        \"alternativeTextPaths\": [\n",
    "          \"$.response\", \n",
    "          \"$.content\", \n",
    "          \"$.choices[0].text\",\n",
    "          \"$.choices[0].message.content\"\n",
    "        ],\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"chatCompletion\": {\n",
    "      \"path\": \"/api/chat/completions\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"${prompt}\"\n",
    "          }\n",
    "        ],\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.choices[0].message.content\",\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"models\": {\n",
    "      \"path\": \"/api/models\",\n",
    "      \"method\": \"GET\",\n",
    "      \"responseMapping\": {\n",
    "        \"modelsPath\": \"$.data\",\n",
    "        \"modelIdPath\": \"$.id\",\n",
    "        \"modelNamePath\": \"$.name\"\n",
    "      }\n",
    "    },\n",
    "    \"health\": {\n",
    "      \"path\": \"/api/health\",\n",
    "      \"method\": \"GET\"\n",
    "    }\n",
    "  },\n",
    "  \"defaultParameters\": {\n",
    "    \"model\": \"gemma3:1b\",\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0.7\n",
    "  },\n",
    "  \"timeout\": 120000,\n",
    "  \"retryAttempts\": 2,\n",
    "  \"retryDelay\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d68691",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Configure Oxygen AI Positron Settings**\n",
    "\n",
    "### Step 4.1: Basic Configuration\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Click **Add Custom Connector**\n",
    "3. Import the JSON configuration from Step 3\n",
    "4. Set the following:\n",
    "   - **Name**: `Local Ollama Gemma3:1b`\n",
    "   - **Base URL**: `http://localhost:5000`\n",
    "   - **API Key**: Leave empty\n",
    "   - **Default Model**: `gemma3:1b`\n",
    "\n",
    "### Step 4.2: Test Connection\n",
    "1. In the Custom Connector configuration dialog\n",
    "2. Click **Test Connection**\n",
    "3. Verify successful connection to your Ollama service\n",
    "\n",
    "## 5. **Create Custom Actions and Framework**\n",
    "\n",
    "### Step 5.1: Create Enhanced Framework File\n",
    "\n",
    "Create `oxygen-ai-gemma-framework.framework`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dba9d3",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<serialized version=\"25.1\" xml:space=\"preserve\">\n",
    "    <serializableOrderedMap>\n",
    "        <entry>\n",
    "            <String>document.types</String>\n",
    "            <documentTypeDescriptor-array>\n",
    "                <documentTypeDescriptor>\n",
    "                    <field name=\"name\">\n",
    "                        <String>AI Enhanced XML with Gemma3</String>\n",
    "                    </field>\n",
    "                    <field name=\"description\">\n",
    "                        <String>XML editing with AI assistance using local Gemma3:1b model via Oxygen AI Positron 5.0</String>\n",
    "                    </field>\n",
    "                    <field name=\"priority\">\n",
    "                        <Integer>3</Integer>\n",
    "                    </field>\n",
    "                    <field name=\"authorActions\">\n",
    "                        <authorAction-array>\n",
    "                            <!-- AI Summarize Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.summarize</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Summarize with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Summarize selected text using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please provide a concise and clear summary of the following text. Focus on the main points and key information:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>REPLACE</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>500</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt S</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Improve Writing Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.improve</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Improve Writing with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Improve writing style and clarity using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please improve the writing, clarity, and flow of the following text while maintaining its original meaning and intent. Make it more professional and readable:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>REPLACE</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>800</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt I</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Generate Documentation Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.document</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Generate Documentation with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Generate comprehensive documentation using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please generate comprehensive technical documentation for the following code or content. Include purpose, parameters, usage examples, and relevant notes:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>1200</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt D</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Explain XML Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.explain.xml</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Explain XML with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Explain XML structure and content using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please explain the following XML structure in detail. Describe what it represents, its elements, attributes, and how it should be used:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>800</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt E</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Generate XML Schema Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.schema</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Generate XML Schema with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Generate XML Schema (XSD) using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Based on the following XML structure, please generate a corresponding XML Schema (XSD) with appropriate element declarations, types, constraints, and documentation:\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>1500</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt X</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                            \n",
    "                            <!-- AI Translate Text Action -->\n",
    "                            <authorAction>\n",
    "                                <field name=\"id\">\n",
    "                                    <String>ai.gemma.translate</String>\n",
    "                                </field>\n",
    "                                <field name=\"name\">\n",
    "                                    <String>AI Translate with Gemma3</String>\n",
    "                                </field>\n",
    "                                <field name=\"description\">\n",
    "                                    <String>Translate selected text using local Gemma3:1b model</String>\n",
    "                                </field>\n",
    "                                <field name=\"operation\">\n",
    "                                    <operation>\n",
    "                                        <field name=\"id\">\n",
    "                                            <String>ro.sync.ecss.extensions.commons.operations.ai.PositronGenerateOperation</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-connector\">\n",
    "                                            <String>local-ollama-gemma3-1b</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-prompt\">\n",
    "                                            <String>Please translate the following text to English (if it's in another language) or provide a translation to Spanish (if it's in English):\n",
    "\n",
    "${selection}</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-insertPosition\">\n",
    "                                            <String>AFTER</String>\n",
    "                                        </field>\n",
    "                                        <field name=\"arg-maxTokens\">\n",
    "                                            <String>600</String>\n",
    "                                        </field>\n",
    "                                    </operation>\n",
    "                                </field>\n",
    "                                <field name=\"accelerator\">\n",
    "                                    <String>ctrl alt T</String>\n",
    "                                </field>\n",
    "                            </authorAction>\n",
    "                        </authorAction-array>\n",
    "                    </field>\n",
    "                </documentTypeDescriptor>\n",
    "            </documentTypeDescriptor-array>\n",
    "        </entry>\n",
    "    </serializableOrderedMap>\n",
    "</serialized>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed0d89",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 5.2: Install the Framework\n",
    "1. Save the framework file to your Oxygen frameworks directory\n",
    "2. Go to **Options** > **Preferences** > **Document Type Association**\n",
    "3. Import the framework or create a new document type based on it\n",
    "\n",
    "## 6. **Create Sample Plugin (Following GitHub Sample)**\n",
    "\n",
    "Based on the sample plugin structure, create a custom plugin:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8213cf",
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "// Create directory: c:\\Project\\OCR\\OxygenGemmaPlugin\\src\\main\\java\\com\\example\\oxygen\\gemma\\\n",
    "\n",
    "// File: OxygenGemmaPlugin.java\n",
    "package com.example.oxygen.gemma;\n",
    "\n",
    "import ro.sync.exml.plugin.Plugin;\n",
    "import ro.sync.exml.plugin.PluginDescriptor;\n",
    "import ro.sync.exml.workspace.api.standalone.StandalonePluginWorkspace;\n",
    "\n",
    "public class OxygenGemmaPlugin extends Plugin {\n",
    "    \n",
    "    public static final String PLUGIN_ID = \"oxygen.gemma.ai.plugin\";\n",
    "    \n",
    "    @Override\n",
    "    public void applicationStarted(StandalonePluginWorkspace workspace) {\n",
    "        // Add menu customizer for Gemma AI actions\n",
    "        workspace.addMenuBarCustomizer(new GemmaMenuCustomizer());\n",
    "        \n",
    "        // Add toolbar customizer\n",
    "        workspace.addToolbarComponentsCustomizer(new GemmaToolbarCustomizer());\n",
    "        \n",
    "        // Register custom AI connector\n",
    "        workspace.getUtilAccess().addExtension(new GemmaAIConnectorExtension());\n",
    "    }\n",
    "    \n",
    "    @Override\n",
    "    public PluginDescriptor getDescriptor() {\n",
    "        return new PluginDescriptor() {\n",
    "            @Override\n",
    "            public String getID() {\n",
    "                return PLUGIN_ID;\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getDescription() {\n",
    "                return \"Integration plugin for local Ollama Gemma3:1b model with Oxygen AI Positron 5.0\";\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getName() {\n",
    "                return \"Oxygen Gemma AI Plugin\";\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getVendor() {\n",
    "                return \"Custom Development\";\n",
    "            }\n",
    "            \n",
    "            @Override\n",
    "            public String getVersion() {\n",
    "                return \"1.0.0\";\n",
    "            }\n",
    "        };\n",
    "    }\n",
    "}\n",
    "\n",
    "// File: GemmaMenuCustomizer.java\n",
    "package com.example.oxygen.gemma;\n",
    "\n",
    "import ro.sync.exml.workspace.api.standalone.MenuBarCustomizer;\n",
    "import ro.sync.exml.workspace.api.standalone.StandalonePluginWorkspace;\n",
    "import javax.swing.*;\n",
    "\n",
    "public class GemmaMenuCustomizer implements MenuBarCustomizer {\n",
    "    \n",
    "    @Override\n",
    "    public void customizeMainMenu(JMenuBar menuBar, StandalonePluginWorkspace workspace) {\n",
    "        // Create Gemma AI menu\n",
    "        JMenu gemmaMenu = new JMenu(\"Gemma AI\");\n",
    "        \n",
    "        // Add actions\n",
    "        gemmaMenu.add(createSummarizeAction(workspace));\n",
    "        gemmaMenu.add(createImproveAction(workspace));\n",
    "        gemmaMenu.add(createDocumentAction(workspace));\n",
    "        gemmaMenu.addSeparator();\n",
    "        gemmaMenu.add(createExplainAction(workspace));\n",
    "        gemmaMenu.add(createSchemaAction(workspace));\n",
    "        \n",
    "        // Add to menu bar\n",
    "        menuBar.add(gemmaMenu);\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createSummarizeAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Summarize with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            // Trigger AI Positron summarize action\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.summarize\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createImproveAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Improve Writing with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.improve\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createDocumentAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Generate Documentation with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.document\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createExplainAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Explain XML with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.explain.xml\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "    \n",
    "    private JMenuItem createSchemaAction(StandalonePluginWorkspace workspace) {\n",
    "        JMenuItem item = new JMenuItem(\"Generate Schema with Gemma3\");\n",
    "        item.addActionListener(e -> {\n",
    "            workspace.getEditorAccess().invokeAction(\"ai.gemma.schema\");\n",
    "        });\n",
    "        return item;\n",
    "    }\n",
    "}\n",
    "\n",
    "// File: plugin.xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<plugin\n",
    " id=\"oxygen.gemma.ai.plugin\"\n",
    " name=\"Oxygen Gemma AI Plugin\"\n",
    " description=\"Integration plugin for local Ollama Gemma3:1b model with Oxygen AI Positron 5.0\"\n",
    " version=\"1.0.0\"\n",
    " vendor=\"Custom Development\"\n",
    " class=\"com.example.oxygen.gemma.OxygenGemmaPlugin\">\n",
    " \n",
    " <runtime>\n",
    "  <library name=\"lib/oxygen-gemma-plugin.jar\"/>\n",
    " </runtime>\n",
    " \n",
    " <extension point=\"WorkspaceAccess\">\n",
    "  <workspace class=\"com.example.oxygen.gemma.OxygenGemmaPlugin\"/>\n",
    " </extension>\n",
    " \n",
    "</plugin>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9106ebe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. **Comprehensive Integration Test**\n",
    "\n",
    "Create a comprehensive test to verify everything works:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2077aeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Running Comprehensive Oxygen AI Positron 5.0 + Gemma3:1b Integration Test\n",
      "================================================================================\n",
      "\n",
      "Test 1/7: Health Check Test\n",
      "------------------------------------------------------------\n",
      "âŒ FAIL - HTTP 404\n",
      "   Error: <!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "\n",
      "\n",
      "Test 2/7: Models List Test\n",
      "------------------------------------------------------------\n",
      "âš ï¸ PARTIAL - Missing fields: ['data']\n",
      "   Available fields: ['models']\n",
      "\n",
      "Test 3/7: Basic Text Generation Test\n",
      "------------------------------------------------------------\n",
      "âš ï¸ PARTIAL - Missing fields: ['text']\n",
      "   Available fields: ['model', 'response', 'success']\n",
      "\n",
      "Test 4/7: Chat Completions Test (OpenAI-compatible)\n",
      "------------------------------------------------------------\n",
      "âŒ FAIL - HTTP 404\n",
      "   Error: <!doctype html>\n",
      "<html lang=en>\n",
      "<title>404 Not Found</title>\n",
      "<h1>Not Found</h1>\n",
      "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n",
      "\n",
      "\n",
      "Test 5/7: XML Schema Generation Test\n",
      "------------------------------------------------------------\n",
      "âš ï¸ PARTIAL - Missing fields: ['text']\n",
      "   Available fields: ['model', 'response', 'success']\n",
      "\n",
      "Test 6/7: XML Explanation Test\n",
      "------------------------------------------------------------\n",
      "âš ï¸ PARTIAL - Missing fields: ['text']\n",
      "   Available fields: ['model', 'response', 'success']\n",
      "\n",
      "Test 7/7: Writing Improvement Test\n",
      "------------------------------------------------------------\n",
      "âš ï¸ PARTIAL - Missing fields: ['text']\n",
      "   Available fields: ['model', 'response', 'success']\n",
      "\n",
      "================================================================================\n",
      "âœ… Integration test completed!\n",
      "ðŸ“Š Results: 0/7 tests passed (0.0%)\n",
      "âš ï¸ Some tests failed. Please check the configuration and try again.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def comprehensive_oxygen_gemma_test():\n",
    "    \"\"\"Comprehensive test of Oxygen AI Positron 5.0 + Gemma3:1b integration\"\"\"\n",
    "    \n",
    "    base_url = \"http://localhost:5000\"\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Health Check Test\",\n",
    "            \"endpoint\": \"/api/health\",\n",
    "            \"method\": \"GET\",\n",
    "            \"expected_fields\": [\"status\", \"service\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Models List Test\",\n",
    "            \"endpoint\": \"/api/models\",\n",
    "            \"method\": \"GET\",\n",
    "            \"expected_fields\": [\"data\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Basic Text Generation Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"What is XML and why is it important in modern web development?\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 500\n",
    "            },\n",
    "            \"expected_fields\": [\"text\", \"response\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Chat Completions Test (OpenAI-compatible)\",\n",
    "            \"endpoint\": \"/api/chat/completions\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": \"Explain XML namespaces with examples\"}\n",
    "                ],\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 600\n",
    "            },\n",
    "            \"expected_fields\": [\"choices\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"XML Schema Generation Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"Generate an XML Schema for a book catalog with the following structure: book (id attribute), title, author, publication year, genre\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 1000\n",
    "            },\n",
    "            \"expected_fields\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"XML Explanation Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"Explain this XML structure: <library><book id='1'><title>XML Guide</title><author>John Doe</author><year>2024</year></book></library>\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 800\n",
    "            },\n",
    "            \"expected_fields\": [\"text\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Writing Improvement Test\",\n",
    "            \"endpoint\": \"/api/generate\",\n",
    "            \"method\": \"POST\",\n",
    "            \"payload\": {\n",
    "                \"input\": \"Improve this text: XML is a markup language that is used for storing and transporting data and it is very useful for web development\",\n",
    "                \"model\": \"gemma3:1b\",\n",
    "                \"max_tokens\": 400\n",
    "            },\n",
    "            \"expected_fields\": [\"text\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ§ª Running Comprehensive Oxygen AI Positron 5.0 + Gemma3:1b Integration Test\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test each case\n",
    "    passed_tests = 0\n",
    "    total_tests = len(test_cases)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}/{total_tests}: {test['name']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if test['method'] == 'GET':\n",
    "                response = requests.get(f\"{base_url}{test['endpoint']}\", timeout=30)\n",
    "            else:\n",
    "                response = requests.post(f\"{base_url}{test['endpoint']}\", \n",
    "                                       json=test['payload'], timeout=60)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                \n",
    "                # Check for expected fields\n",
    "                found_fields = []\n",
    "                missing_fields = []\n",
    "                \n",
    "                for field in test['expected_fields']:\n",
    "                    if field in result:\n",
    "                        found_fields.append(field)\n",
    "                    else:\n",
    "                        missing_fields.append(field)\n",
    "                \n",
    "                if not missing_fields:\n",
    "                    print(f\"âœ… PASS - Response time: {processing_time:.2f}s\")\n",
    "                    \n",
    "                    # Extract and display response content\n",
    "                    if 'choices' in result:\n",
    "                        content = result['choices'][0]['message']['content']\n",
    "                    elif 'data' in result:\n",
    "                        content = f\"Found {len(result['data'])} models\"\n",
    "                    else:\n",
    "                        content = result.get('text', result.get('response', result.get('status', '')))\n",
    "                    \n",
    "                    if isinstance(content, str) and len(content) > 100:\n",
    "                        print(f\"   Generated {len(content)} characters\")\n",
    "                        print(f\"   Preview: {content[:150]}...\")\n",
    "                    else:\n",
    "                        print(f\"   Result: {content}\")\n",
    "                    \n",
    "                    passed_tests += 1\n",
    "                else:\n",
    "                    print(f\"âš ï¸ PARTIAL - Missing fields: {missing_fields}\")\n",
    "                    print(f\"   Available fields: {list(result.keys())}\")\n",
    "            else:\n",
    "                print(f\"âŒ FAIL - HTTP {response.status_code}\")\n",
    "                print(f\"   Error: {response.text}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"âŒ TIMEOUT - Request took too long\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"âœ… Integration test completed!\")\n",
    "    print(f\"ðŸ“Š Results: {passed_tests}/{total_tests} tests passed ({(passed_tests/total_tests)*100:.1f}%)\")\n",
    "    \n",
    "    if passed_tests == total_tests:\n",
    "        print(\"ðŸŽ‰ All tests passed! Your Oxygen AI Positron + Gemma3:1b integration is ready!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Some tests failed. Please check the configuration and try again.\")\n",
    "\n",
    "# Run the comprehensive test\n",
    "if __name__ == \"__main__\":\n",
    "    comprehensive_oxygen_gemma_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d027e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 8. **Usage Instructions**\n",
    "\n",
    "### Step 8.1: Basic Usage in Oxygen\n",
    "1. Open an XML document in Oxygen XML Editor\n",
    "2. Select some text\n",
    "3. Use one of these methods:\n",
    "   - **Menu**: Go to **Gemma AI** menu and select an action\n",
    "   - **Keyboard shortcuts**: Use `Ctrl+Alt+S` (summarize), `Ctrl+Alt+I` (improve), etc.\n",
    "   - **AI Positron panel**: Use the AI Positron side panel\n",
    "\n",
    "### Step 8.2: Advanced Usage\n",
    "1. Configure custom prompts in the AI Positron settings\n",
    "2. Adjust model parameters (temperature, max tokens) per action\n",
    "3. Create additional custom actions for specific use cases\n",
    "\n",
    "## 9. **Troubleshooting**\n",
    "\n",
    "### Issue 1: Connection Problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "68b13c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Diagnosing Oxygen AI Positron connection...\n",
      "âœ… Ollama Server: Status 200\n",
      "âœ… Flask API: Status 404\n",
      "âœ… Models List: Status 200\n",
      "âœ… Generate Endpoint: Status 200\n",
      "âœ… Gemma3:1b model available\n"
     ]
    }
   ],
   "source": [
    "def diagnose_oxygen_connection():\n",
    "    \"\"\"Diagnose connection issues with Oxygen AI Positron\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” Diagnosing Oxygen AI Positron connection...\")\n",
    "    \n",
    "    # Check if all services are running\n",
    "    checks = [\n",
    "        (\"Ollama Server\", \"http://localhost:11434/api/tags\"),\n",
    "        (\"Flask API\", \"http://localhost:5000/api/health\"),\n",
    "        (\"Models List\", \"http://localhost:5000/api/models\"),\n",
    "        (\"Generate Endpoint\", \"http://localhost:5000/api/generate\")\n",
    "    ]\n",
    "    \n",
    "    for name, url in checks:\n",
    "        try:\n",
    "            if \"generate\" in url:\n",
    "                response = requests.post(url, json={\"input\": \"test\", \"model\": \"gemma3:1b\"}, timeout=10)\n",
    "            else:\n",
    "                response = requests.get(url, timeout=10)\n",
    "            \n",
    "            print(f\"âœ… {name}: Status {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {name}: {e}\")\n",
    "            \n",
    "    # Check Gemma3:1b model\n",
    "    try:\n",
    "        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, shell=True)\n",
    "        if 'gemma3:1b' in result.stdout:\n",
    "            print(\"âœ… Gemma3:1b model available\")\n",
    "        else:\n",
    "            print(\"âŒ Gemma3:1b model not found - run 'ollama pull gemma3:1b'\")\n",
    "    except:\n",
    "        print(\"âŒ Cannot check Ollama models\")\n",
    "\n",
    "diagnose_oxygen_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936a651",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This comprehensive setup provides full integration between your local Ollama Gemma3:1b service and Oxygen XML Editor using AI Positron 5.0, following the official documentation and sample plugin structure for maximum compatibility and functionality.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be2147",
   "metadata": {},
   "source": [
    "Looking at your Ollama RESTful service implementation, I can see that it currently doesn't include API key authentication. Ollama itself doesn't natively support API keys, but you can add API key authentication to your Flask wrapper for security. Here's how to implement it:\n",
    "\n",
    "## 1. **Add API Key Authentication to Your Flask API**\n",
    "\n",
    "Update your `OxygenOllamaAPI` class to include API key support:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6535bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”‘ API Key: ollama-Oyz5XTUtmR0z49loubZzwoYeUGcgZhL3xgewX97lIfs\n",
      "ðŸš€ Oxygen AI Positron 5.0 compatible API server with API key authentication started on http://localhost:5000\n",
      "ðŸ“‹ Available endpoints:\n",
      "- POST /api/generate - Generate text (requires API key)\n",
      "- POST /api/chat/completions - OpenAI-compatible endpoint (requires API key)\n",
      "- GET /api/health - Health check (public)\n",
      "- GET /api/models - List available models (requires API key)\n",
      "- GET /api/key - Get API key information (public)\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed to get API key\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import threading\n",
    "import requests\n",
    "import traceback\n",
    "import time\n",
    "import secrets\n",
    "import os\n",
    "from functools import wraps\n",
    "\n",
    "class OxygenOllamaAPI:\n",
    "    def __init__(self, api_key=None):\n",
    "        self.app = Flask(__name__)\n",
    "        # Enhanced CORS configuration for Oxygen XML Editor\n",
    "        CORS(self.app, \n",
    "             origins=['*'],\n",
    "             methods=['GET', 'POST', 'OPTIONS', 'PUT', 'DELETE'],\n",
    "             allow_headers=['Content-Type', 'Authorization', 'X-Requested-With', 'Accept', 'X-API-Key'],\n",
    "             expose_headers=['Content-Type', 'Authorization'],\n",
    "             supports_credentials=False)\n",
    "        \n",
    "        # Set API key\n",
    "        # self.api_key = api_key or os.getenv('OLLAMA_API_KEY') or self.generate_api_key()\n",
    "        # self.api_key = os.getenv('OLLAMA_API_KEY')\n",
    "        self.api_key = self.generate_api_key()\n",
    "        print(f\"ðŸ”‘ API Key: {self.api_key}\")\n",
    "        \n",
    "        self.setup_routes()\n",
    "    \n",
    "    def generate_api_key(self):\n",
    "        \"\"\"Generate a secure API key\"\"\"\n",
    "        return f\"ollama-{secrets.token_urlsafe(32)}\"\n",
    "    \n",
    "    def require_api_key(self, f):\n",
    "        \"\"\"Decorator to require API key authentication\"\"\"\n",
    "        @wraps(f)\n",
    "        def decorated_function(*args, **kwargs):\n",
    "            # Skip API key check for OPTIONS requests (CORS preflight)\n",
    "            if request.method == 'OPTIONS':\n",
    "                return f(*args, **kwargs)\n",
    "            \n",
    "            # Check for API key in headers\n",
    "            provided_key = (request.headers.get('X-API-Key') or \n",
    "                          request.headers.get('Authorization', '').replace('Bearer ', '') or\n",
    "                          request.args.get('api_key'))\n",
    "            \n",
    "            if not provided_key:\n",
    "                return jsonify({\n",
    "                    'error': 'API key required',\n",
    "                    'message': 'Please provide API key in X-API-Key header, Authorization header, or api_key parameter'\n",
    "                }), 401\n",
    "            \n",
    "            if provided_key != self.api_key:\n",
    "                return jsonify({\n",
    "                    'error': 'Invalid API key',\n",
    "                    'message': 'The provided API key is not valid'\n",
    "                }), 403\n",
    "            \n",
    "            return f(*args, **kwargs)\n",
    "        return decorated_function\n",
    "        \n",
    "    def setup_routes(self):\n",
    "        @self.app.route('/api/generate', methods=['POST', 'OPTIONS'])\n",
    "        @self.require_api_key\n",
    "        def generate():\n",
    "            # Handle CORS preflight requests\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-Requested-With,X-API-Key')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "            \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                if not data:\n",
    "                    return jsonify({'error': 'No JSON data provided'}), 400\n",
    "                \n",
    "                # Handle multiple input field formats for Oxygen compatibility\n",
    "                prompt = (data.get('prompt') or \n",
    "                         data.get('input') or \n",
    "                         data.get('text') or \n",
    "                         data.get('message', ''))\n",
    "                \n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                max_tokens = data.get('max_tokens', 1000)\n",
    "                temperature = data.get('temperature', 0.7)\n",
    "                \n",
    "                if not prompt:\n",
    "                    return jsonify({'error': 'No prompt provided'}), 400\n",
    "                \n",
    "                print(f\"ðŸ¤– Processing authenticated request: {prompt[:50]}...\")\n",
    "                \n",
    "                # Test Ollama connection\n",
    "                try:\n",
    "                    health_check = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                    if health_check.status_code != 200:\n",
    "                        return jsonify({\n",
    "                            'error': f'Ollama server not responding: {health_check.status_code}'\n",
    "                        }), 500\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    return jsonify({\n",
    "                        'error': 'Cannot connect to Ollama server on localhost:11434'\n",
    "                    }), 500\n",
    "                \n",
    "                # Call Ollama API with proper options\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': max_tokens,\n",
    "                        'temperature': temperature,\n",
    "                        'top_k': 40,\n",
    "                        'top_p': 0.9\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # Format response for Oxygen AI Positron compatibility\n",
    "                    oxygen_response = {\n",
    "                        'text': generated_text,  # Primary response field for Oxygen\n",
    "                        'response': generated_text,  # Alternative field\n",
    "                        'content': generated_text,  # Another alternative\n",
    "                        'choices': [{  # OpenAI-compatible format\n",
    "                            'text': generated_text,\n",
    "                            'message': {\n",
    "                                'content': generated_text,\n",
    "                                'role': 'assistant'\n",
    "                            }\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'success': True,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'metadata': {\n",
    "                            'tokens_generated': len(generated_text.split()),\n",
    "                            'model_used': model,\n",
    "                            'processing_time': result.get('total_duration', 0)\n",
    "                        }\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… Generated {len(generated_text)} characters for authenticated user\")\n",
    "                    return jsonify(oxygen_response)\n",
    "                else:\n",
    "                    error_msg = f'Ollama API error: {response.status_code} - {response.text}'\n",
    "                    print(f\"âŒ {error_msg}\")\n",
    "                    return jsonify({'error': error_msg}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_traceback = traceback.format_exc()\n",
    "                print(f\"âŒ Exception in generate endpoint: {error_traceback}\")\n",
    "                return jsonify({\n",
    "                    'error': f'Server error: {str(e)}',\n",
    "                    'success': False\n",
    "                }), 500\n",
    "        \n",
    "        @self.app.route('/api/chat/completions', methods=['POST', 'OPTIONS'])\n",
    "        @self.require_api_key\n",
    "        def chat_completions():\n",
    "            \"\"\"OpenAI-compatible endpoint for Oxygen AI Positron\"\"\"\n",
    "            if request.method == 'OPTIONS':\n",
    "                response = jsonify({'status': 'ok'})\n",
    "                response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "                response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization,X-API-Key')\n",
    "                response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')\n",
    "                return response\n",
    "                \n",
    "            try:\n",
    "                data = request.get_json()\n",
    "                messages = data.get('messages', [])\n",
    "                model = data.get('model', 'gemma3:1b')\n",
    "                \n",
    "                # Extract prompt from messages\n",
    "                if messages:\n",
    "                    prompt = messages[-1].get('content', '')\n",
    "                else:\n",
    "                    prompt = data.get('prompt', '')\n",
    "                \n",
    "                # Use the same generation logic\n",
    "                ollama_request = {\n",
    "                    'model': model,\n",
    "                    'prompt': prompt,\n",
    "                    'stream': False,\n",
    "                    'options': {\n",
    "                        'num_predict': data.get('max_tokens', 1000),\n",
    "                        'temperature': data.get('temperature', 0.7)\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json=ollama_request, timeout=120)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('response', '')\n",
    "                    \n",
    "                    # OpenAI-compatible response format for Oxygen\n",
    "                    openai_response = {\n",
    "                        'choices': [{\n",
    "                            'message': {\n",
    "                                'role': 'assistant',\n",
    "                                'content': generated_text\n",
    "                            },\n",
    "                            'finish_reason': 'stop',\n",
    "                            'index': 0\n",
    "                        }],\n",
    "                        'model': model,\n",
    "                        'usage': {\n",
    "                            'total_tokens': len(generated_text.split()),\n",
    "                            'prompt_tokens': len(prompt.split()),\n",
    "                            'completion_tokens': len(generated_text.split())\n",
    "                        },\n",
    "                        'object': 'chat.completion'\n",
    "                    }\n",
    "                    \n",
    "                    return jsonify(openai_response)\n",
    "                else:\n",
    "                    return jsonify({'error': 'Generation failed'}), 500\n",
    "                    \n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/api/health', methods=['GET'])\n",
    "        def health():\n",
    "            \"\"\"Health check endpoint (no authentication required)\"\"\"\n",
    "            try:\n",
    "                ollama_response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "                models = ollama_response.json().get('models', []) if ollama_response.status_code == 200 else []\n",
    "                \n",
    "                return jsonify({\n",
    "                    'status': 'healthy',\n",
    "                    'service': 'ollama-gemma-api',\n",
    "                    'version': '1.0.0',\n",
    "                    'authentication': 'enabled',\n",
    "                    'ollama_server': 'running' if ollama_response.status_code == 200 else 'error',\n",
    "                    'available_models': [m.get('name', '') for m in models],\n",
    "                    'flask_api': 'running',\n",
    "                    'endpoints': [\n",
    "                        '/api/generate (requires API key)',\n",
    "                        '/api/chat/completions (requires API key)',\n",
    "                        '/api/health (public)',\n",
    "                        '/api/models (requires API key)',\n",
    "                        '/api/key (public)'\n",
    "                    ]\n",
    "                })\n",
    "            except Exception as e:\n",
    "                return jsonify({\n",
    "                    'status': 'unhealthy',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        @self.app.route('/api/models', methods=['GET'])\n",
    "        @self.require_api_key\n",
    "        def list_models():\n",
    "            \"\"\"List available models in OpenAI-compatible format\"\"\"\n",
    "            try:\n",
    "                response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    models_data = response.json()\n",
    "                    models = []\n",
    "                    for model in models_data.get('models', []):\n",
    "                        models.append({\n",
    "                            'id': model.get('name', ''),\n",
    "                            'object': 'model',\n",
    "                            'created': 0,\n",
    "                            'owned_by': 'ollama',\n",
    "                            'name': model.get('name', ''),\n",
    "                            'size': model.get('size', 0),\n",
    "                            'modified': model.get('modified_at', '')\n",
    "                        })\n",
    "                    return jsonify({'data': models, 'object': 'list'})\n",
    "                else:\n",
    "                    return jsonify({'error': 'Failed to fetch models'}), 500\n",
    "            except Exception as e:\n",
    "                return jsonify({'error': str(e)}), 500\n",
    "        \n",
    "        @self.app.route('/api/key', methods=['GET'])\n",
    "        def get_api_key():\n",
    "            \"\"\"Get the current API key (for initial setup)\"\"\"\n",
    "            return jsonify({\n",
    "                'api_key': self.api_key,\n",
    "                'usage': {\n",
    "                    'header': f'X-API-Key: {self.api_key}',\n",
    "                    'authorization': f'Authorization: Bearer {self.api_key}',\n",
    "                    'parameter': f'?api_key={self.api_key}'\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    def run(self, host='0.0.0.0', port=5000):\n",
    "        self.app.run(host=host, port=port, debug=False, threaded=True)\n",
    "\n",
    "# Create and start the enhanced API server with API key\n",
    "# You can set a custom API key or let it generate one\n",
    "custom_api_key = \"lic-da-16888\"  # Optional: set your own key\n",
    "oxygen_api = OxygenOllamaAPI(api_key=custom_api_key)\n",
    "\n",
    "def start_oxygen_api_server():\n",
    "    oxygen_api.run(host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start server in background thread\n",
    "server_thread = threading.Thread(target=start_oxygen_api_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"ðŸš€ Oxygen AI Positron 5.0 compatible API server with API key authentication started on http://localhost:5000\")\n",
    "print(\"ðŸ“‹ Available endpoints:\")\n",
    "print(\"- POST /api/generate - Generate text (requires API key)\")\n",
    "print(\"- POST /api/chat/completions - OpenAI-compatible endpoint (requires API key)\")\n",
    "print(\"- GET /api/health - Health check (public)\")\n",
    "print(\"- GET /api/models - List available models (requires API key)\")\n",
    "print(\"- GET /api/key - Get API key information (public)\")\n",
    "\n",
    "# Test the API with authentication\n",
    "time.sleep(3)\n",
    "try:\n",
    "    # First, get the API key\n",
    "    key_response = requests.get('http://localhost:5000/api/key')\n",
    "    if key_response.status_code == 200:\n",
    "        api_key = key_response.json()['api_key']\n",
    "        print(f\"\\nðŸ”‘ Using API Key: {api_key}\")\n",
    "        \n",
    "        # Test authenticated request\n",
    "        test_data = {\n",
    "            'input': 'Write a brief summary about XML editing.',\n",
    "            'model': 'gemma3:1b'\n",
    "        }\n",
    "        \n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'X-API-Key': api_key\n",
    "        }\n",
    "        \n",
    "        response = requests.post('http://localhost:5000/api/generate', \n",
    "                               json=test_data, headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(f\"\\nâœ… Authenticated API Test successful!\")\n",
    "            print(f\"Response: {result.get('text', result.get('response', ''))[:200]}...\")\n",
    "        else:\n",
    "            print(f\"âŒ API Test failed: {response.status_code}\")\n",
    "            print(response.text)\n",
    "    else:\n",
    "        print(\"âŒ Failed to get API key\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ API Test error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d58cd85",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Update Oxygen AI Positron Custom Connector Configuration**\n",
    "\n",
    "Update your custom connector configuration to include the API key:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59fc9c",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"id\": \"local-ollama-gemma3-1b\",\n",
    "  \"name\": \"Local Ollama Gemma3:1b (with API Key)\",\n",
    "  \"description\": \"Local Ollama server with Gemma3:1b model for text generation (API key protected)\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"type\": \"custom\",\n",
    "  \"provider\": \"ollama\",\n",
    "  \"baseUrl\": \"http://localhost:5000\",\n",
    "  \"authentication\": {\n",
    "    \"type\": \"api_key\",\n",
    "    \"required\": true,\n",
    "    \"apiKey\": \"your-api-key-here\"\n",
    "  },\n",
    "  \"capabilities\": {\n",
    "    \"textGeneration\": true,\n",
    "    \"chatCompletion\": true,\n",
    "    \"streaming\": false\n",
    "  },\n",
    "  \"endpoints\": {\n",
    "    \"textGeneration\": {\n",
    "      \"path\": \"/api/generate\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"X-API-Key\": \"${apiKey}\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"input\": \"${prompt}\",\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.text\",\n",
    "        \"alternativeTextPaths\": [\n",
    "          \"$.response\", \n",
    "          \"$.content\", \n",
    "          \"$.choices[0].text\",\n",
    "          \"$.choices[0].message.content\"\n",
    "        ],\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"chatCompletion\": {\n",
    "      \"path\": \"/api/chat/completions\",\n",
    "      \"method\": \"POST\",\n",
    "      \"headers\": {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer ${apiKey}\"\n",
    "      },\n",
    "      \"requestBodyTemplate\": {\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"${prompt}\"\n",
    "          }\n",
    "        ],\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"max_tokens\": \"${maxTokens:1000}\",\n",
    "        \"temperature\": \"${temperature:0.7}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"textPath\": \"$.choices[0].message.content\",\n",
    "        \"errorPath\": \"$.error\"\n",
    "      }\n",
    "    },\n",
    "    \"models\": {\n",
    "      \"path\": \"/api/models\",\n",
    "      \"method\": \"GET\",\n",
    "      \"headers\": {\n",
    "        \"X-API-Key\": \"${apiKey}\"\n",
    "      },\n",
    "      \"responseMapping\": {\n",
    "        \"modelsPath\": \"$.data\",\n",
    "        \"modelIdPath\": \"$.id\",\n",
    "        \"modelNamePath\": \"$.name\"\n",
    "      }\n",
    "    },\n",
    "    \"health\": {\n",
    "      \"path\": \"/api/health\",\n",
    "      \"method\": \"GET\"\n",
    "    }\n",
    "  },\n",
    "  \"defaultParameters\": {\n",
    "    \"model\": \"gemma3:1b\",\n",
    "    \"maxTokens\": 1000,\n",
    "    \"temperature\": 0.7\n",
    "  },\n",
    "  \"timeout\": 120000,\n",
    "  \"retryAttempts\": 2,\n",
    "  \"retryDelay\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5cbf6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Configure API Key in Oxygen XML Editor**\n",
    "\n",
    "### Step 3.1: Get Your API Key\n",
    "Run this function to get your current API key:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8574568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed to get API key: 404\n"
     ]
    }
   ],
   "source": [
    "def get_api_key_info():\n",
    "    \"\"\"Get API key information from your local service\"\"\"\n",
    "    try:\n",
    "        response = requests.get('http://localhost:5000/api/key', timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            key_info = response.json()\n",
    "            print(\"ðŸ”‘ API Key Information:\")\n",
    "            print(f\"API Key: {key_info['api_key']}\")\n",
    "            print(\"\\nðŸ“‹ Usage Examples:\")\n",
    "            print(f\"Header: {key_info['usage']['header']}\")\n",
    "            print(f\"Authorization: {key_info['usage']['authorization']}\")\n",
    "            print(f\"Parameter: {key_info['usage']['parameter']}\")\n",
    "            return key_info['api_key']\n",
    "        else:\n",
    "            print(f\"âŒ Failed to get API key: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting API key: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get your API key\n",
    "api_key = get_api_key_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff6f9b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3.2: Configure in Oxygen AI Positron\n",
    "1. Go to **Options** > **Preferences** > **Plugins** > **AI Positron**\n",
    "2. Click **Add Custom Connector** or edit your existing connector\n",
    "3. Set the following:\n",
    "   - **Name**: `Local Ollama Gemma3:1b (Authenticated)`\n",
    "   - **Base URL**: `http://localhost:5000`\n",
    "   - **API Key**: `[paste your API key here]`\n",
    "   - **Authentication Type**: API Key or Bearer Token\n",
    "\n",
    "## 4. **Test Authenticated API**\n",
    "\n",
    "Create a test function to verify API key authentication:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "32ce77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed to get API key\n"
     ]
    }
   ],
   "source": [
    "def test_authenticated_api():\n",
    "    \"\"\"Test API with different authentication methods\"\"\"\n",
    "    \n",
    "    # Get API key\n",
    "    key_response = requests.get('http://localhost:5000/api/key')\n",
    "    if key_response.status_code != 200:\n",
    "        print(\"âŒ Failed to get API key\")\n",
    "        return\n",
    "    \n",
    "    api_key = key_response.json()['api_key']\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"X-API-Key Header\",\n",
    "            \"headers\": {\"X-API-Key\": api_key}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Authorization Bearer\",\n",
    "            \"headers\": {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"URL Parameter\",\n",
    "            \"headers\": {},\n",
    "            \"params\": {\"api_key\": api_key}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"No Authentication (should fail)\",\n",
    "            \"headers\": {},\n",
    "            \"should_fail\": True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    test_data = {\n",
    "        'input': 'Test authentication with a simple request.',\n",
    "        'model': 'gemma3:1b'\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ§ª Testing API Key Authentication\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test['name']}\")\n",
    "        \n",
    "        try:\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            headers.update(test.get('headers', {}))\n",
    "            \n",
    "            params = test.get('params', {})\n",
    "            \n",
    "            response = requests.post('http://localhost:5000/api/generate', \n",
    "                                   json=test_data, \n",
    "                                   headers=headers,\n",
    "                                   params=params,\n",
    "                                   timeout=30)\n",
    "            \n",
    "            if test.get('should_fail', False):\n",
    "                if response.status_code in [401, 403]:\n",
    "                    print(f\"âœ… PASS - Authentication correctly rejected (HTTP {response.status_code})\")\n",
    "                else:\n",
    "                    print(f\"âŒ FAIL - Expected authentication failure but got HTTP {response.status_code}\")\n",
    "            else:\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result.get('text', result.get('response', ''))\n",
    "                    print(f\"âœ… PASS - Generated {len(generated_text)} characters\")\n",
    "                    print(f\"   Preview: {generated_text[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"âŒ FAIL - HTTP {response.status_code}: {response.text}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR - {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"âœ… Authentication test completed!\")\n",
    "\n",
    "# Run authentication test\n",
    "test_authenticated_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93dc02",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Environment Variable Configuration**\n",
    "\n",
    "For better security, you can also set the API key via environment variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4d91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API key set in environment: OLLAMA_API_KEY=ollama-secure-key-2024\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set API key via environment variable (run once)\n",
    "def set_api_key_env(api_key):\n",
    "    \"\"\"Set API key as environment variable\"\"\"\n",
    "    os.environ['OLLAMA_API_KEY'] = api_key\n",
    "    print(f\"âœ… API key set in environment: OLLAMA_API_KEY={api_key}\")\n",
    "\n",
    "# Use a custom API key\n",
    "custom_key = \"ollama-secure-key-2024\"\n",
    "set_api_key_env(custom_key)\n",
    "\n",
    "# The API will automatically use this environment variable\n",
    "# when creating the OxygenOllamaAPI instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a51efce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ollama-secure-key-2024'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OLLAMA_API_KEY']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fb0e5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. **Security Best Practices**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d9077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import time\n",
    "\n",
    "class SecureOllamaAPI(OxygenOllamaAPI):\n",
    "    \"\"\"Enhanced API with additional security features\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None):\n",
    "        super().__init__(api_key)\n",
    "        self.request_log = {}\n",
    "        self.rate_limit = 60  # requests per minute\n",
    "    \n",
    "    def check_rate_limit(self, client_ip):\n",
    "        \"\"\"Basic rate limiting\"\"\"\n",
    "        now = time.time()\n",
    "        minute = int(now // 60)\n",
    "        \n",
    "        if client_ip not in self.request_log:\n",
    "            self.request_log[client_ip] = {}\n",
    "        \n",
    "        if minute not in self.request_log[client_ip]:\n",
    "            self.request_log[client_ip][minute] = 0\n",
    "        \n",
    "        self.request_log[client_ip][minute] += 1\n",
    "        \n",
    "        # Clean old entries\n",
    "        for old_minute in list(self.request_log[client_ip].keys()):\n",
    "            if old_minute < minute - 1:\n",
    "                del self.request_log[client_ip][old_minute]\n",
    "        \n",
    "        return self.request_log[client_ip][minute] <= self.rate_limit\n",
    "    \n",
    "    def hash_api_key(self, key):\n",
    "        \"\"\"Hash API key for logging\"\"\"\n",
    "        return hashlib.sha256(key.encode()).hexdigest()[:16]\n",
    "\n",
    "# Use the secure version\n",
    "# secure_api = SecureOllamaAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba5cef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This implementation provides:\n",
    "\n",
    "1. **Multiple Authentication Methods**: X-API-Key header, Authorization Bearer token, or URL parameter\n",
    "2. **Automatic Key Generation**: Creates secure API keys if none provided\n",
    "3. **CORS Support**: Properly handles preflight requests with authentication headers\n",
    "4. **Error Handling**: Clear error messages for authentication failures\n",
    "5. **Health Endpoint**: Public endpoint to check service status\n",
    "6. **Key Retrieval**: Endpoint to get current API key for initial setup\n",
    "7. **Security Features**: Rate limiting and key hashing options\n",
    "\n",
    "The API key will be required for all protected endpoints (`/api/generate`, `/api/chat/completions`, `/api/models`) while keeping health check and key retrieval endpoints public for initial setup.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
